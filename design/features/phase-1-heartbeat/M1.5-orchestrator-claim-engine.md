# **Feature design: M1.5 - Orchestrator Claim Engine**

**Purpose**: Basic orchestrator that watches artefacts and creates claims
**Scope**: Core event-driven orchestration infrastructure
**Estimated tokens**: ~6,500 tokens

Associated phase: **Heartbeat (Phase 1)**
Status: **Draft**

***Template purpose:*** *This document is a blueprint for a single, implementable milestone. Its purpose is to provide an unambiguous specification for a developer (human or AI) to build a feature that is consistent with Holt's architecture and guiding principles.*

---

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Implement a lightweight, event-driven orchestrator that watches for new artefacts on the blackboard and creates corresponding claims, proving the core Pub/Sub architecture works end-to-end.

### **1.2. User story**

As a Holt operator, I want the orchestrator to automatically detect when new artefacts appear on the blackboard and create corresponding claims so that the system's event-driven coordination infrastructure is proven to work, enabling agent integration in Phase 2.

### **1.3. Success criteria**

* When the CLI creates a GoalDefined artefact via `holt forage`, the orchestrator detects it within 100ms and creates a corresponding claim in `pending_review` status
* The orchestrator correctly identifies Terminal artefacts and does not create claims for them
* All created claims are published to the `claim_events` Pub/Sub channel, proving the agent notification pathway works
* The orchestrator's health check endpoint (`GET /healthz`) returns 200 OK when Redis is accessible and 503 when Redis is down
* The orchestrator gracefully shuts down on SIGTERM, cleanly closing Redis connections
* If the same artefact event is published twice, only one claim is created (idempotent operation)
* The orchestrator ignores artefacts that existed before it started (event-driven only, not historical scan)

**Validation questions:**
* ✅ Can each success criterion be automated as a test? Yes - all are integration-testable
* ✅ Does each criterion represent user-visible value? Yes - enables M1.6 validation and Phase 2 agent work
* ✅ Are the criteria specific enough to avoid ambiguity? Yes - specific behaviors and timing defined

### **1.4. Non-goals**

* **NOT in scope**: Agent bidding logic (Phase 2 - no agents exist yet)
* **NOT in scope**: Claim phase transitions (pending_review → pending_parallel → pending_exclusive)
* **NOT in scope**: Granted agent tracking (no agents to grant to)
* **NOT in scope**: Agent registry loading from `holt.yml` (YAGNI - no bidding in Phase 1)
* **NOT in scope**: Failure artefact creation (no agent execution = no failures)
* **NOT in scope**: Question/Answer orchestration logic (Phase 4)
* **NOT in scope**: Claim timeout or expiration (Phase 2+)
* **NOT in scope**: Integration with `holt up` command (that's M1.6)
* **NOT in scope**: Container image building in this milestone (Dockerfile delivered, but integration deferred)

---

## **2. The 'what': component impact analysis**

**Critical validation questions for this entire section:**
* ✅ Have I explicitly considered EVERY component (Blackboard, Orchestrator, Pup, CLI)?
* ✅ For components marked "No changes" - am I absolutely certain this feature doesn't affect them?
* ✅ Do my changes maintain the contracts and interfaces defined in the design documents?
* ✅ Will this feature work correctly with both single-instance and scaled agents (controller-worker pattern)?

### **2.1. Blackboard changes**

* **New/modified data structures:** No changes to schemas
* **New Pub/Sub channels:** No changes - uses existing `artefact_events` (subscribe) and `claim_events` (publish) channels
* **Usage patterns:**
  * Subscribes to `holt:{instance_name}:artefact_events` for real-time artefact notifications
  * Publishes to `holt:{instance_name}:claim_events` when new claims are created
  * Reads artefacts using `Client.GetArtefact()` to retrieve full artefact data
  * Writes claims using `Client.CreateClaim()` (which handles publishing internally per M1.2)
  * Checks for existing claims using `Client.GetClaimByArtefactID()` for idempotency

**Note:** The blackboard client (M1.2) already handles publishing claim events in `CreateClaim()`, so the orchestrator just needs to call that method.

### **2.2. Orchestrator changes**

* **New/modified logic:** This IS the orchestrator implementation - creating it from scratch

**Core responsibilities:**
1. **Event Loop**: Subscribe to `artefact_events` and process each message
2. **Claim Creation**: For each new artefact (except Terminal), create a claim in `pending_review` status
3. **Idempotency**: Check if a claim already exists for an artefact before creating
4. **Health Checks**: HTTP server on port 8080 exposing `/healthz` endpoint
5. **Graceful Shutdown**: Handle SIGTERM/SIGINT for clean Redis connection closure

**Process flow:**
```
Startup → Connect to Redis → Subscribe to artefact_events → Start HTTP server
  ↓
Event arrives → Parse artefact JSON → Get full artefact → Check structural_type
  ↓
If Terminal → Ignore
If not Terminal → Check if claim exists → Create claim if missing → Publish claim_events
  ↓
SIGTERM → Unsubscribe → Close Redis → Shutdown HTTP server → Exit
```

* **New/modified configurations (holt.yml):** No configuration file loading required in M1.5

### **2.3. Agent pup changes**

* **New/modified logic:** No changes
* **Changes to tool execution contract:** No changes

**Justification:** No agents exist in Phase 1. The orchestrator publishes to `claim_events`, but no pups are listening yet. This proves the infrastructure works for Phase 2 integration.

### **2.4. CLI changes**

* **New/modified commands:** No changes
* **Changes to user output:** No changes

**Justification:** The orchestrator is a background service. CLI commands (`holt forage` in M1.6) will trigger it indirectly, but M1.5 doesn't modify the CLI itself.

**Note:** M1.6 will integrate the orchestrator container into `holt up`, but that's outside M1.5's scope.

---

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

**Decision 1: Event-Driven Only (No Historical Scan)**
* **Rationale:** The orchestrator should only process artefacts created after it starts, not scan for pre-existing artefacts
* **Implementation:** Subscribe to Pub/Sub channel and process messages as they arrive
* **Risk Mitigation:** If the orchestrator restarts, it will only process new artefacts going forward. This is acceptable for Phase 1.
* **Why This Approach:** Aligns with Holt's event-driven architecture and keeps orchestrator stateless

**Decision 2: Idempotency via Claim Lookup**
* **Rationale:** Prevents duplicate claims if the same artefact event is published twice (network retry, developer error, etc.)
* **Implementation:** Before creating a claim, call `Client.GetClaimByArtefactID()` to check if one exists
* **Risk:** Adds a Redis read operation per artefact event (acceptable overhead for correctness)
* **Alternative Considered:** Redis SET NX pattern - rejected as more complex than needed for V1

**Decision 3: Fail-Fast on Redis Errors**
* **Rationale:** If Redis is unavailable, the orchestrator cannot function and should exit cleanly
* **Implementation:** No retry logic beyond go-redis defaults. Let container runtime restart the orchestrator.
* **Risk Mitigation:** Docker/Kubernetes will restart failed containers automatically
* **Why This Approach:** Simplicity and reliability - avoids complex retry state machines

**Decision 4: No Configuration File Loading**
* **Rationale:** Agent registry not needed in Phase 1 (no bidding), so no need to parse `holt.yml`
* **Implementation:** Only accept environment variables for instance name and Redis URL
* **Risk:** Phase 2 will need to add configuration loading - acceptable technical debt
* **Why This Approach:** YAGNI - avoids unnecessary complexity

**Decision 5: Separate HTTP Server for Health Checks**
* **Rationale:** Health checks are critical for Docker/Kubernetes integration
* **Implementation:** Goroutine running HTTP server on port 8080, independent of event loop
* **Risk Mitigation:** Timeout on health check (2s) prevents hanging requests
* **Why This Approach:** Standard pattern for containerized services

**Biggest Risks:**
1. **Pub/Sub message loss**: If orchestrator is down when an artefact is created, it won't create a claim
   * **Mitigation for Phase 2:** Add persistent claim queue or startup scan logic
2. **Redis connection failures during operation**: Orchestrator will exit and restart
   * **Acceptable for Phase 1:** Container restart is the recovery mechanism
3. **Race condition on startup**: Artefacts created during orchestrator startup might be missed
   * **Acceptable for Phase 1:** Integration tests will validate the happy path

### **3.2. Implementation steps**

**Phase 1: Core Orchestrator Skeleton (Day 1)**
- [ ] Create `cmd/orchestrator/main.go` with basic structure
- [ ] Create `internal/orchestrator/engine.go` for core logic
- [ ] Implement environment variable loading (`HOLT_INSTANCE_NAME`, `REDIS_URL`)
- [ ] Create blackboard client connection
- [ ] Implement graceful shutdown (SIGTERM/SIGINT handling)

**Phase 2: Event Subscription & Processing (Day 2)**
- [ ] Subscribe to `artefact_events` Pub/Sub channel in `engine.go`
- [ ] Implement event parsing (JSON unmarshal to artefact object)
- [ ] Implement `GetArtefact()` call to retrieve full artefact data
- [ ] Add logging for all received events

**Phase 3: Claim Creation Logic (Day 3)**
- [ ] Implement Terminal artefact detection (skip claim creation)
- [ ] Implement idempotency check (`GetClaimByArtefactID()`)
- [ ] Implement claim creation with `pending_review` status
- [ ] Generate claim UUID using `github.com/google/uuid`
- [ ] Call `Client.CreateClaim()` to write and publish
- [ ] Add structured logging for all claim operations

**Phase 4: Health Check HTTP Server (Day 4)**
- [ ] Create `internal/orchestrator/health.go`
- [ ] Implement `/healthz` endpoint (GET request)
- [ ] Check Redis connectivity with `Ping()` call
- [ ] Return 200 OK if Redis reachable, 503 otherwise
- [ ] Add graceful HTTP server shutdown on SIGTERM
- [ ] Add timeout to health checks (2 seconds)

**Phase 5: Containerization (Day 5)**
- [ ] Create `Dockerfile` for orchestrator (multi-stage build)
- [ ] Use `golang:1.23-alpine` as build stage
- [ ] Use `alpine:latest` as runtime stage
- [ ] Copy binary to `/usr/local/bin/holt-orchestrator`
- [ ] Set `ENTRYPOINT ["/usr/local/bin/holt-orchestrator"]`
- [ ] Document required environment variables in Dockerfile

**Phase 6: Unit & Integration Tests (Days 6-7)**
- [ ] Unit tests for Terminal artefact detection
- [ ] Unit tests for claim struct creation
- [ ] Integration test: orchestrator creates claim for GoalDefined artefact
- [ ] Integration test: orchestrator skips Terminal artefacts
- [ ] Integration test: idempotency (duplicate events)
- [ ] Integration test: health check endpoints
- [ ] Integration test: graceful shutdown

**Phase 7: Documentation & Polish (Day 8)**
- [ ] Add comprehensive logging (structured JSON logs)
- [ ] Document startup sequence and expected behavior
- [ ] Create README for orchestrator component
- [ ] Final code review and refactoring
- [ ] Update this design document with implementation notes

### **3.3. Performance & resource considerations**

* **Resource usage:**
  * **CPU:** Minimal - event-driven, blocks on Pub/Sub
  * **Memory:** < 50 MB baseline (Go runtime + Redis client)
  * **Network:** One persistent Redis connection for Pub/Sub + occasional CRUD operations
  * **Storage:** None (stateless - all state in Redis)

* **Scalability limits:**
  * **V1 Constraint:** Single orchestrator instance only (full consensus model requires deterministic processing)
  * **Artefact throughput:** Designed for < 100 artefacts/minute (well within Redis Pub/Sub capacity)
  * **Claim creation latency:** Target < 100ms from artefact event to claim published

* **Performance requirements:**
  * **Event processing latency:** < 100ms from receiving `artefact_events` message to publishing `claim_events`
  * **Health check response time:** < 2 seconds (includes Redis ping)
  * **Startup time:** < 5 seconds (connect to Redis, subscribe to channel, start HTTP server)
  * **Graceful shutdown time:** < 10 seconds (unsubscribe, close connections)

### **3.4. Testing strategy**

* **Unit tests:**
  * `internal/orchestrator/engine_test.go`:
    * `TestIsTerminalArtefact()` - Validates detection of Terminal structural type
    * `TestCreateClaimForArtefact()` - Validates claim struct creation with correct fields
    * `TestShouldSkipClaimCreation()` - Validates Terminal artefact logic
  * `internal/orchestrator/health_test.go`:
    * `TestHealthCheckEndpoint_RedisUp()` - Mock Redis connection, verify 200 response
    * `TestHealthCheckEndpoint_RedisDown()` - Mock Redis failure, verify 503 response

* **Integration tests:**
  * `cmd/orchestrator/orchestrator_integration_test.go` (uses testcontainers-go):
    * `TestOrchestrator_CreatesClaimForGoalDefined`:
      1. Start Redis container
      2. Start orchestrator process
      3. Publish GoalDefined artefact to `artefact_events`
      4. Verify claim created in Redis with `pending_review` status
      5. Verify claim published to `claim_events`
    * `TestOrchestrator_SkipsTerminalArtefacts`:
      1. Start Redis container
      2. Start orchestrator process
      3. Publish Terminal artefact to `artefact_events`
      4. Verify NO claim created in Redis
      5. Verify NO message on `claim_events`
    * `TestOrchestrator_IdempotentClaimCreation`:
      1. Start Redis container
      2. Start orchestrator process
      3. Publish same artefact event twice
      4. Verify only ONE claim exists in Redis
    * `TestOrchestrator_HealthCheckEndpoint`:
      1. Start Redis container
      2. Start orchestrator process
      3. Call `GET http://localhost:8080/healthz`
      4. Verify 200 OK response
      5. Stop Redis container
      6. Call health check again
      7. Verify 503 response
    * `TestOrchestrator_GracefulShutdown`:
      1. Start Redis container
      2. Start orchestrator process
      3. Send SIGTERM signal
      4. Verify orchestrator exits with code 0
      5. Verify Redis subscription cleanly closed

* **Performance tests:**
  * `TestOrchestrator_EventProcessingLatency`:
    1. Start Redis and orchestrator
    2. Publish artefact event
    3. Measure time until claim appears in Redis
    4. Assert < 100ms latency

* **E2E tests (holt tests):**
  * Deferred to **M1.6** - requires full system integration with `holt up` and `holt forage`

**Test coverage target:** ≥ 85% for all orchestrator packages

---

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

* **No new third-party dependencies** beyond existing project dependencies:
  * `github.com/redis/go-redis/v9` - Already used in M1.2
  * `github.com/google/uuid` - Already used in M1.1
  * Standard library only for HTTP server, signal handling, JSON parsing

* **Justification for existing dependencies:**
  * Redis client: Required for blackboard interaction
  * UUID library: Required for claim ID generation

* **What we're NOT building:**
  * No custom Pub/Sub implementation (using Redis)
  * No custom health check framework (using standard library HTTP server)
  * No configuration file parser (using environment variables only)

### **4.2. Auditability**

* **New artefacts created:** None - orchestrator only creates Claims, not Artefacts
* **Immutable records:** All Claims are written to Redis and never modified in Phase 1
* **Audit trail preservation:**
  * Every claim creation is logged with timestamp and artefact ID
  * Claims remain in Redis indefinitely (no TTL or deletion)
  * Claim events published to `claim_events` create event stream record

* **Logging standards:**
  * Structured JSON logging for all significant events
  * Log fields: `timestamp`, `level`, `component`, `event_type`, `artefact_id`, `claim_id`, `instance_name`

### **4.3. Small, single-purpose components**

* **Orchestrator's single purpose:** Watch artefacts, create claims
* **Component boundaries:**
  * Orchestrator does NOT execute tools (that's Agent Pup's job)
  * Orchestrator does NOT parse `holt.yml` (that's CLI's job in M1.4, orchestrator's job in Phase 2)
  * Orchestrator does NOT create artefacts (only claims)

* **Tight coupling avoided:**
  * Orchestrator depends only on blackboard client interface
  * No direct dependencies on CLI or Agent Pup packages
  * Communication only via Redis Pub/Sub (loose coupling)

### **4.4. Security considerations**

* **Attack surfaces:**
  * **HTTP health check endpoint:** No authentication required (read-only status check)
  * **Redis connection:** Assumes trusted network (Docker bridge network in M1.4)
  * **Environment variables:** Potential secret exposure if Redis URL contains password

* **Data protection:**
  * No sensitive data in logs (artefact payloads not logged at INFO level)
  * Redis password in `REDIS_URL` env var (follows Docker Compose conventions)

* **Container isolation:**
  * Orchestrator runs as non-root user in container (Dockerfile uses `USER nobody`)
  * Read-only root filesystem (add `readOnlyRootFilesystem: true` in future)
  * No privileged capabilities required

* **Network security:**
  * Health check endpoint binds to all interfaces (0.0.0.0:8080) but protected by Docker network isolation
  * Redis connection uses internal Docker network (not exposed to host)

**Security improvements for Phase 2:**
* Add authentication to health check endpoint
* Use Redis ACLs for least-privilege access
* Implement secrets management (not environment variables)

### **4.5. Backward compatibility**

* **Does this change existing APIs?** No - this is a new component
* **Does this modify existing data structures?** No - uses M1.1 schemas unchanged
* **Are existing workflows preserved?** N/A - no existing workflows yet
* **Is this feature additive?** Yes - pure addition

**Compatibility guarantees:**
* Claim schema matches M1.1 specification exactly
* Pub/Sub channels use M1.1 naming conventions
* Future orchestrator enhancements (bidding, phase transitions) will extend this base, not replace it

### **4.6. Dependency impact**

* **Redis usage changes:** No new Redis features required (uses Pub/Sub and hashes from M1.2)
* **Docker requirements:** No change - orchestrator runs as standard container
* **Go version:** Requires Go 1.23+ (consistent with M1.1, M1.2)
* **Build dependencies:** No new tools required (uses existing Makefile patterns)

**CI/CD impact:**
* Integration tests require Docker daemon (same as M1.4)
* Use testcontainers-go for Redis (same as M1.2)

---

## **5. Definition of done**

*This checklist must be fully satisfied for the milestone to be considered complete.*

- [ ] All implementation steps from section 3.2 are complete
- [ ] All tests defined in section 3.4 are implemented and passing
- [ ] Performance requirements from section 3.3 are met and verified (< 100ms claim creation latency)
- [ ] Overall test coverage has not decreased (minimum 85% for orchestrator packages)
- [ ] The Makefile has been updated with orchestrator build, test, and run commands
- [ ] Dockerfile for `holt-orchestrator` builds successfully and produces working image
- [ ] Health check endpoint (`/healthz`) responds correctly to Redis status
- [ ] The orchestrator correctly identifies and skips Terminal artefacts
- [ ] Idempotency test passes (duplicate artefact events produce single claim)
- [ ] Graceful shutdown test passes (SIGTERM handled correctly)
- [ ] All failure modes identified in section 6.1 have been implemented and tested
- [ ] Concurrency considerations from section 6.2 have been addressed
- [ ] All open questions from section 7 have been resolved or documented as future work
- [ ] AI agent implementation guidance has been followed and integration checklist completed
- [ ] Security considerations from section 4.4 have been addressed and validated
- [ ] Backward compatibility requirements from section 4.5 are satisfied
- [ ] Dependency impact analysis from section 4.6 has been completed and approved
- [ ] Operational readiness checklist from section 9 is fully satisfied
- [ ] Structured logging is implemented for all significant events
- [ ] Component README documentation is complete
- [ ] No TODOs remain in production code

---

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Failure 1: Redis unavailable on startup**
* **Scenario:** Orchestrator starts but Redis is not reachable
* **Detection:** `Client.Ping()` fails during initialization
* **Response:** Log error, exit with code 1 (container runtime will restart)
* **Test:** `TestOrchestrator_RedisUnavailableOnStartup`

**Failure 2: Redis connection lost during operation**
* **Scenario:** Pub/Sub connection drops while orchestrator is running
* **Detection:** Redis client returns connection error
* **Response:** Log error, attempt reconnection per go-redis defaults, exit if reconnection fails
* **Test:** `TestOrchestrator_RedisConnectionLost`

**Failure 3: Malformed artefact event**
* **Scenario:** `artefact_events` message contains invalid JSON
* **Detection:** `json.Unmarshal()` returns error
* **Response:** Log warning with message content, skip event, continue processing
* **Test:** `TestOrchestrator_MalformedArtefactEvent`

**Failure 4: Artefact referenced in event doesn't exist**
* **Scenario:** Event contains artefact ID, but `GetArtefact()` returns `redis.Nil`
* **Detection:** `GetArtefact()` returns not-found error
* **Response:** Log warning, skip claim creation, continue processing
* **Rationale:** Possible race condition or manual Redis manipulation
* **Test:** `TestOrchestrator_ArtefactNotFound`

**Failure 5: Claim creation fails**
* **Scenario:** `CreateClaim()` returns error (Redis write failure)
* **Detection:** Error returned from blackboard client
* **Response:** Log error with artefact ID, skip event, continue processing
* **Rationale:** Transient failures shouldn't crash orchestrator
* **Test:** `TestOrchestrator_ClaimCreationFailure`

**Failure 6: Health check called during startup**
* **Scenario:** HTTP request arrives before Redis connection established
* **Detection:** Blackboard client not initialized
* **Response:** Return 503 Service Unavailable
* **Test:** `TestOrchestrator_HealthCheckDuringStartup`

### **6.2. Concurrency considerations**

**Consideration 1: Single orchestrator instance only**
* **Constraint:** V1 full consensus model requires exactly one orchestrator
* **Enforcement:** Not enforced programmatically in M1.5 (future: use Redis lock)
* **Risk:** If two orchestrators run, duplicate claims will be created
* **Mitigation for Phase 2:** Add instance lock check on startup

**Consideration 2: Concurrent artefact events**
* **Scenario:** Multiple artefact events arrive simultaneously
* **Handling:** Redis Pub/Sub delivers messages serially to single subscriber
* **Safety:** No race condition - events processed one at a time
* **Implementation:** Single goroutine processes Pub/Sub messages

**Consideration 3: Health check during claim creation**
* **Scenario:** HTTP health check request arrives while claim is being created
* **Handling:** Separate goroutine for HTTP server, shared Redis client is thread-safe
* **Safety:** go-redis client handles concurrent requests safely
* **Implementation:** No synchronization needed

**Consideration 4: Shutdown during claim creation**
* **Scenario:** SIGTERM received while processing artefact event
* **Handling:**
  1. Stop accepting new Pub/Sub messages
  2. Allow in-flight claim creation to complete
  3. Close Redis connection
  4. Shutdown HTTP server
* **Safety:** Use context cancellation to signal shutdown
* **Implementation:**
  ```go
  ctx, cancel := context.WithCancel(context.Background())
  defer cancel()

  // On SIGTERM:
  cancel() // Signals all goroutines to stop
  ```

**Consideration 5: Idempotency check race condition**
* **Scenario:** Same artefact event delivered twice, both before claim creation completes
* **Handling:** Sequential event processing prevents race (Pub/Sub is serial)
* **Safety:** First event creates claim, second event finds existing claim
* **Implementation:** No special handling needed beyond idempotency check

### **6.3. Edge case handling**

**Edge Case 1: Empty artefact payload**
* **Scenario:** Artefact has `payload: ""`
* **Handling:** Valid artefact - create claim normally
* **Rationale:** Payload validation is artefact creator's responsibility

**Edge Case 2: Very large artefact payload**
* **Scenario:** Artefact payload is several MB (e.g., large diff)
* **Handling:** Works normally - Redis supports values up to 512 MB
* **Rationale:** No size limits needed in Phase 1

**Edge Case 3: Rapid artefact creation (burst)**
* **Scenario:** 100 artefacts created in 1 second
* **Handling:** Process sequentially at < 100ms/artefact = ~10 seconds total
* **Rationale:** Acceptable for Phase 1 (< 100 artefacts/minute expected)

**Edge Case 4: Artefact with invalid structural_type**
* **Scenario:** Artefact has `structural_type: "InvalidType"`
* **Handling:** M1.1 validation should prevent this, but if it occurs, create claim
* **Rationale:** Defensive - don't crash on invalid data

**Edge Case 5: Claim already exists with different status**
* **Scenario:** Idempotency check finds existing claim in different status (shouldn't happen in Phase 1)
* **Handling:** Skip claim creation, log warning
* **Rationale:** Idempotent behavior - don't create duplicate

**Edge Case 6: Orchestrator restart during event processing**
* **Scenario:** Container killed (SIGKILL) while processing artefact
* **Handling:** Event lost - acceptable for Phase 1
* **Mitigation for Phase 2:** Persistent message queue or startup scan

---

## **7. Open questions & decisions**

### **Resolved Decisions**

✅ **Q1: Should orchestrator load `holt.yml` for agent registry?**
* **Decision:** No - skip agent registry entirely in M1.5 (YAGNI for Phase 1)
* **Rationale:** No bidding = no need for agent list

✅ **Q2: Should orchestrator publish to `claim_events`?**
* **Decision:** Yes - publish full claim JSON to prove Pub/Sub architecture
* **Rationale:** Validates end-to-end pathway for Phase 2 agents

✅ **Q3: Should M1.5 integrate orchestrator into `holt up`?**
* **Decision:** No - M1.5 delivers binary + Dockerfile, M1.6 handles integration
* **Rationale:** Separates orchestrator implementation from CLI integration

✅ **Q4: How to handle duplicate artefact events?**
* **Decision:** Idempotency check via `GetClaimByArtefactID()` before creation
* **Rationale:** Correctness over performance for Phase 1

✅ **Q5: Should orchestrator scan for historical artefacts on startup?**
* **Decision:** No - event-driven only, ignore pre-existing artefacts
* **Rationale:** Simpler implementation, acceptable for Phase 1

### **Open Questions for Phase 2**

⏭️ **Q6: How should orchestrator handle multiple instances running?**
* **Current State:** Not enforced - could create duplicate claims
* **Phase 2 Solution:** Add Redis-based instance lock on startup

⏭️ **Q7: Should orchestrator retry failed claim creations?**
* **Current State:** Log error and skip event
* **Phase 2 Solution:** Add retry queue with exponential backoff

⏭️ **Q8: Should orchestrator emit metrics (Prometheus)?**
* **Current State:** Structured logs only
* **Phase 4 Solution:** Add metrics for claims created, latency, errors

---

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**Start with the simplest path:**
1. **Day 1:** Get orchestrator connecting to Redis and logging startup
2. **Day 2:** Subscribe to `artefact_events` and log received messages
3. **Day 3:** Create claims for artefacts (no idempotency yet)
4. **Day 4:** Add idempotency check and Terminal artefact detection
5. **Day 5:** Add health check HTTP server
6. **Day 6-7:** Write comprehensive tests
7. **Day 8:** Polish, document, containerize

**Defensive programming checklist:**
- [ ] Validate all environment variables on startup
- [ ] Check Redis connectivity before subscribing
- [ ] Validate artefact structure before creating claim
- [ ] Use context timeouts for all Redis operations
- [ ] Log all errors with sufficient context for debugging
- [ ] Never panic - always return errors gracefully

**Error handling from the beginning:**
- Every Redis operation wrapped in error check
- All errors logged with structured context
- Graceful degradation where possible (skip event, continue processing)
- Fatal errors (Redis unavailable) exit cleanly with code 1

### **8.2. Common pitfalls to avoid**

**Pitfall 1: Forgetting to check for Terminal artefacts**
* **Problem:** Creates claims for Terminal artefacts, violating orchestrator contract
* **Solution:** Explicit `if artefact.StructuralType == StructuralTypeTerminal { skip }`
* **Test:** `TestOrchestrator_SkipsTerminalArtefacts`

**Pitfall 2: Not handling Redis Pub/Sub reconnection**
* **Problem:** Connection drops, events stop flowing, no error logged
* **Solution:** go-redis handles reconnection automatically, but log connection state changes
* **Test:** `TestOrchestrator_RedisConnectionLost`

**Pitfall 3: Blocking health check on Pub/Sub message processing**
* **Problem:** If event processing blocks, health checks time out
* **Solution:** Run HTTP server in separate goroutine, use separate Redis ping
* **Test:** `TestOrchestrator_HealthCheckDuringEventProcessing`

**Pitfall 4: Not closing Redis connection on shutdown**
* **Problem:** Container takes 30s to exit (waiting for connection timeout)
* **Solution:** Implement graceful shutdown with context cancellation
* **Test:** `TestOrchestrator_GracefulShutdown`

**Pitfall 5: Logging sensitive data**
* **Problem:** Artefact payloads might contain secrets (API keys, passwords)
* **Solution:** Log artefact IDs only, not full payloads (use DEBUG level for payloads)

**Pitfall 6: Not validating environment variables**
* **Problem:** Orchestrator starts with empty `HOLT_INSTANCE_NAME`, creates invalid Redis keys
* **Solution:** Validate all required env vars on startup, exit immediately if missing
* **Test:** `TestOrchestrator_MissingEnvironmentVariables`

### **8.3. Integration checklist**

**Pre-implementation verification:**
- [x] M1.1 (Blackboard Foundation) complete - types and schemas defined
- [x] M1.2 (Blackboard Client Operations) complete - `Client.CreateClaim()` available
- [ ] M1.4 (CLI Lifecycle) complete - orchestrator will be integrated in M1.6
- [x] All component interfaces remain stable - no breaking changes

**Implementation validation:**
- [ ] No breaking changes to Artefact or Claim schemas
- [ ] Pub/Sub channel names match M1.1 specification exactly
- [ ] Claims created with exactly the fields defined in M1.1
- [ ] Health check endpoint matches standard format (`/healthz`)

---

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Metrics to track:**
* **Claims created:** Counter of total claims created since startup
* **Claim creation latency:** Time from artefact event to claim published
* **Artefacts skipped:** Counter of Terminal artefacts (not claimed)
* **Redis errors:** Counter of Redis operation failures
* **Idempotency hits:** Counter of duplicate artefact events detected

**Structured logging events:**
```json
// Startup
{"level":"info","event":"orchestrator_startup","instance":"prod","redis_url":"redis://..."}

// Claim created
{"level":"info","event":"claim_created","artefact_id":"uuid","claim_id":"uuid","latency_ms":45}

// Terminal artefact skipped
{"level":"info","event":"terminal_skipped","artefact_id":"uuid","artefact_type":"FinalReport"}

// Idempotency hit
{"level":"warn","event":"duplicate_artefact","artefact_id":"uuid","existing_claim_id":"uuid"}

// Redis error
{"level":"error","event":"redis_error","operation":"CreateClaim","error":"connection refused"}
```

**Health check diagnostics:**
* `/healthz` returns JSON with Redis connectivity status
  ```json
  {
    "status": "healthy",
    "redis": "connected",
    "instance": "prod",
    "uptime_seconds": 3600
  }
  ```

**How operators detect issues:**
1. **Orchestrator down:** `GET /healthz` returns connection refused
2. **Redis down:** `GET /healthz` returns 503 with `"redis": "disconnected"`
3. **Claims not being created:** Check logs for "claim_created" events, verify `artefact_events` subscription

### **9.2. Rollback and disaster recovery**

**Can this feature be disabled via configuration?**
* No - orchestrator is core component and cannot be disabled
* However, in M1.4 integration (M1.6), orchestrator container can be stopped: `docker stop holt-orchestrator-{instance}`

**Rollback procedure:**
* **If orchestrator has bugs:** Stop container, revert to busybox placeholder from M1.4
* **Steps:**
  1. `holt down --name {instance}`
  2. Revert `holt up` command to use busybox orchestrator
  3. `holt up --name {instance}`

**Data migration for rollback:**
* **No migration needed:** Claims already created remain in Redis
* **Side effect:** Old claims won't be processed by reverted orchestrator (acceptable)

**Recovery time objective (RTO):**
* **Container restart:** < 5 seconds (orchestrator startup time)
* **Full rollback:** < 2 minutes (stop, rebuild, restart containers)

### **9.3. Documentation and training**

**New CLI commands:** None - orchestrator is a background service

**User guides to update:**
* `README.md`: Add section on orchestrator architecture
* Architecture diagram: Show orchestrator in system overview

**API documentation:**
* Orchestrator exposes HTTP health check endpoint only
* Document in `docs/orchestrator-api.md`:
  ```
  GET /healthz
  Returns: 200 OK (Redis connected) or 503 Service Unavailable (Redis down)
  Response body: JSON with status details
  ```

**Troubleshooting guides:**
* **Issue:** Claims not being created
  * **Check:** Verify orchestrator container is running (`docker ps`)
  * **Check:** Verify health check returns 200 (`curl http://localhost:8080/healthz`)
  * **Check:** Verify Redis is running and accessible
  * **Check:** Review orchestrator logs (`docker logs holt-orchestrator-{instance}`)

* **Issue:** Orchestrator exits immediately
  * **Check:** Verify environment variables are set (`HOLT_INSTANCE_NAME`, `REDIS_URL`)
  * **Check:** Verify Redis URL is correct and reachable
  * **Check:** Review container logs for startup errors

**Team training needs:**
* Developers need to understand:
  * Orchestrator's event-driven architecture (subscribe to `artefact_events`)
  * Claim creation contract (when claims are created, when they're skipped)
  * Health check integration with Docker/Kubernetes
  * Graceful shutdown behavior (SIGTERM handling)

---

## **10. Self-validation checklist**

### **Before starting implementation:**

- [x] I understand how this feature aligns with Phase 1 (proving infrastructure works)
- [x] All success criteria (section 1.3) are measurable and testable
- [x] I have considered every component in section 2 explicitly
- [x] All design decisions (section 3.1) are justified and documented
- [x] I understand the scope boundaries (what's NOT in M1.5)

### **During implementation:**

- [ ] I am implementing the simplest solution that meets success criteria
- [ ] All error scenarios (section 6) are being handled, not just happy path
- [ ] Tests are being written before or alongside code (TDD approach)
- [ ] I am validating that existing M1.1 and M1.2 contracts are not broken
- [ ] Structured logging is added for all significant events
- [ ] Graceful shutdown is implemented from the beginning

### **Before submission:**

- [ ] All items in Definition of Done (section 5) are complete
- [ ] Feature has been tested in a clean environment from scratch
- [ ] Integration tests run successfully with testcontainers-go
- [ ] Dockerfile builds and produces working container image
- [ ] Health check endpoint responds correctly to all scenarios
- [ ] Idempotency test passes (duplicate events handled correctly)
- [ ] Terminal artefact detection test passes
- [ ] Graceful shutdown test passes (SIGTERM handled cleanly)
- [ ] Performance requirement met (< 100ms claim creation latency)
- [ ] Documentation is updated and accurate
- [ ] I have considered the operational impact (section 9) of this feature
- [ ] No TODOs remain in production code
- [ ] Code review completed and feedback addressed

---

## **Appendix A: Example orchestrator main.go structure**

```go
package main

import (
    "context"
    "fmt"
    "os"
    "os/signal"
    "syscall"

    "github.com/yourusername/holt/internal/orchestrator"
    "github.com/yourusername/holt/pkg/blackboard"
    "github.com/redis/go-redis/v9"
)

func main() {
    // 1. Load environment variables
    instanceName := os.Getenv("HOLT_INSTANCE_NAME")
    redisURL := os.Getenv("REDIS_URL")

    if instanceName == "" || redisURL == "" {
        fmt.Fprintf(os.Stderr, "Error: HOLT_INSTANCE_NAME and REDIS_URL must be set\n")
        os.Exit(1)
    }

    // 2. Parse Redis URL
    redisOpts, err := redis.ParseURL(redisURL)
    if err != nil {
        fmt.Fprintf(os.Stderr, "Error: Invalid REDIS_URL: %v\n", err)
        os.Exit(1)
    }

    // 3. Create blackboard client
    client, err := blackboard.NewClient(redisOpts, instanceName)
    if err != nil {
        fmt.Fprintf(os.Stderr, "Error: Failed to create blackboard client: %v\n", err)
        os.Exit(1)
    }
    defer client.Close()

    // 4. Verify Redis connectivity
    ctx := context.Background()
    if err := client.Ping(ctx); err != nil {
        fmt.Fprintf(os.Stderr, "Error: Redis not accessible: %v\n", err)
        os.Exit(1)
    }

    // 5. Create orchestrator engine
    engine := orchestrator.NewEngine(client, instanceName)

    // 6. Setup graceful shutdown
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()

    sigCh := make(chan os.Signal, 1)
    signal.Notify(sigCh, syscall.SIGTERM, syscall.SIGINT)

    // 7. Start orchestrator in goroutine
    errCh := make(chan error, 1)
    go func() {
        errCh <- engine.Run(ctx)
    }()

    // 8. Wait for shutdown signal or error
    select {
    case <-sigCh:
        fmt.Println("Shutdown signal received, stopping gracefully...")
        cancel()
    case err := <-errCh:
        if err != nil {
            fmt.Fprintf(os.Stderr, "Orchestrator error: %v\n", err)
            os.Exit(1)
        }
    }

    fmt.Println("Orchestrator stopped")
}
```

---

## **Appendix B: Example Dockerfile**

```dockerfile
# Build stage
FROM golang:1.23-alpine AS builder

WORKDIR /build

# Copy go mod files
COPY go.mod go.sum ./
RUN go mod download

# Copy source code
COPY . .

# Build orchestrator binary
RUN CGO_ENABLED=0 GOOS=linux GOARCH=arm64 go build -o holt-orchestrator ./cmd/orchestrator

# Runtime stage
FROM alpine:latest

# Install ca-certificates for HTTPS
RUN apk --no-cache add ca-certificates

# Create non-root user
RUN adduser -D -u 1000 holt

# Copy binary from builder
COPY --from=builder /build/holt-orchestrator /usr/local/bin/holt-orchestrator

# Switch to non-root user
USER holt

# Expose health check port
EXPOSE 8080

# Set entrypoint
ENTRYPOINT ["/usr/local/bin/holt-orchestrator"]

# Document required environment variables
ENV HOLT_INSTANCE_NAME=""
ENV REDIS_URL=""
```

---

## **Appendix C: Integration with M1.6**

**M1.6 will update `holt up` to:**

1. Build orchestrator image: `docker build -t holt-orchestrator:latest -f Dockerfile .`
2. Replace busybox placeholder with real orchestrator:
   ```go
   // M1.6 changes to cmd/holt/commands/up.go:

   // Build orchestrator image
   buildCtx := docker.BuildContext{
       Dockerfile: "Dockerfile",
       Tags:       []string{"holt-orchestrator:latest"},
   }
   if err := buildImage(ctx, client, buildCtx); err != nil {
       return fmt.Errorf("failed to build orchestrator: %w", err)
   }

   // Create orchestrator container
   containerResp, err := client.ContainerCreate(ctx, &container.Config{
       Image: "holt-orchestrator:latest",  // Changed from busybox
       Env: []string{
           fmt.Sprintf("HOLT_INSTANCE_NAME=%s", instanceName),
           fmt.Sprintf("REDIS_URL=redis://holt-redis-%s:6379", instanceName),
       },
       Labels: map[string]string{
           "holt.project":         "true",
           "holt.instance.name":   instanceName,
           "holt.instance.run_id": runID,
           "holt.workspace.path":  workspacePath,
           "holt.component":       "orchestrator",
       },
   }, &container.HostConfig{
       NetworkMode: container.NetworkMode(networkName),
   }, nil, nil, containerName)
   ```

---

**End of Design Document**
