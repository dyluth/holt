# **Feature design: M1.6 - Workflow Initiation (forage)**

**Purpose**: Implement `holt forage` to initiate workflows and validate Phase 1 E2E pipeline
**Scope**: CLI command for creating GoalDefined artefacts and observing orchestrator response
**Estimated tokens**: ~7,000 tokens

Associated phase: **Heartbeat (Phase 1)**
Status: **Draft**

***Template purpose:*** *This document is a blueprint for a single, implementable milestone. Its purpose is to provide an unambiguous specification for a developer (human or AI) to build a feature that is consistent with Holt's architecture and guiding principles.*

---

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Implement the `holt forage` command to create GoalDefined artefacts and validate the complete Phase 1 event-driven pipeline from CLI through orchestrator to claim creation.

### **1.2. User story**

As a Holt user, I want to initiate a workflow by providing a high-level goal description so that the system can begin coordinating agents to accomplish that goal, and I want immediate feedback confirming the orchestrator received my request.

### **1.3. Success criteria**

* Running `holt forage --goal "hello world"` creates a GoalDefined artefact on the blackboard with correct structure
* The orchestrator detects the artefact within 200ms and creates a corresponding claim
* Running `holt forage --watch --goal "..."` validates the E2E pipeline by polling for claim creation and reporting success
* The command fails gracefully with helpful error messages when prerequisites aren't met (no Git repo, dirty workspace, no instances running)
* The command correctly infers the target instance from the current workspace without requiring explicit `--name` flag
* `holt watch` command structure exists as a stub with clear guidance for Phase 1 users
* All Phase 1 success criteria are validated: CLI → Artefact → Orchestrator → Claim pipeline proven

**Validation questions:**
* ✅ Can each success criterion be automated as a test? Yes - all are E2E testable
* ✅ Does each criterion represent user-visible value? Yes - completes Phase 1 validation
* ✅ Are the criteria specific enough to avoid ambiguity? Yes - specific behaviors defined

### **1.4. Non-goals**

* **NOT in scope**: Agent execution or tool invocation (Phase 2)
* **NOT in scope**: Real-time workflow monitoring beyond simple claim verification (Phase 2+ for `holt watch`)
* **NOT in scope**: Question/Answer handling (Phase 4)
* **NOT in scope**: Multiple goal submission or batch operations
* **NOT in scope**: Goal templates or structured goal formats
* **NOT in scope**: Workspace branch management or Git operations
* **NOT in scope**: Claim status tracking beyond initial creation (pending_review only in Phase 1)

---

## **2. The 'what': component impact analysis**

**Critical validation questions for this entire section:**
* ✅ Have I explicitly considered EVERY component (Blackboard, Orchestrator, Pup, CLI)?
* ✅ For components marked "No changes" - am I absolutely certain this feature doesn't affect them?
* ✅ Do my changes maintain the contracts and interfaces defined in the design documents?
* ✅ Will this feature work correctly with both single-instance and scaled agents (controller-worker pattern)?

### **2.1. Blackboard changes**

* **New/modified data structures:** No changes to schemas
* **New Pub/Sub channels:** No changes - uses existing `artefact_events` channel
* **Usage patterns:**
  * CLI creates GoalDefined artefacts using `Client.CreateArtefact()`
  * CLI polls for claims using `Client.GetClaimByArtefactID()`
  * All standard artefact validation and publishing happens automatically

### **2.2. Orchestrator changes**

* **New/modified logic:** No changes to orchestrator
* **New/modified configurations:** No changes

**Justification:** Orchestrator already handles all artefact types correctly. GoalDefined is a Standard structural type, so orchestrator creates a claim automatically.

### **2.3. Agent pup changes**

* **New/modified logic:** No changes
* **Changes to tool execution contract:** No changes

**Justification:** No agents in Phase 1. Pup integration is Phase 2+.

### **2.4. CLI changes**

* **New/modified commands:**

**1. `holt forage [--name <instance>] [--watch] --goal "description"`**
- Creates GoalDefined artefact on blackboard
- Validates prerequisites (Git repo, clean workspace)
- Infers or uses explicit instance targeting
- Optionally validates orchestrator claim creation (--watch flag)

**2. `holt watch [--name <instance>]`**
- Phase 1: Stub command with informational message
- Phase 2+: Will implement real-time workflow monitoring

**3. Modification to `holt up` (from M1.4)**
- Add Redis port mapping to host
- Store port in Docker label for discovery
- Find next available port starting from 6379

* **Changes to user output:**
  * New structured output for forage command
  * Progress indicators for --watch polling
  * Enhanced error messages following M1.4 patterns

---

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

**Decision 1: Redis Port Exposure (M1.4 Modification)**
* **Rationale:** CLI runs on host, Redis in isolated container network. Need connectivity for blackboard operations.
* **Implementation:** Map Redis container port 6379 to host port (6379+), store in label `holt.redis.port`
* **Risk:** Opens Redis to host network (mitigated: localhost only, Phase 1 simplification)
* **Why This Approach:** Simplest for Phase 1, documented as technical debt for Phase 2 security improvements

**Decision 2: Workspace-Based Instance Inference**
* **Rationale:** Better UX than requiring explicit --name for single-instance workflows
* **Implementation:** Query Docker for containers with matching `holt.workspace.path` label
* **Risk:** Ambiguity if multiple instances on same workspace (mitigated: require --name in that case)
* **Why This Approach:** Aligns with Git-centric workflow, minimizes friction

**Decision 3: Strict Git Validation**
* **Rationale:** System specification requires clean workspace for Git-centric workflow integrity
* **Implementation:** Check `git status --porcelain` is empty before creating artefact
* **Risk:** May frustrate users with experimental workflows (mitigated: clear error messages)
* **Why This Approach:** Enforces spec requirement, prevents bad initial state

**Decision 4: Simple Polling for Claim Verification (--watch)**
* **Rationale:** Phase 1 needs E2E validation, not production monitoring
* **Implementation:** Poll `GetClaimByArtefactID` every 200ms for 5 seconds
* **Risk:** Inefficient compared to Pub/Sub (acceptable for Phase 1)
* **Why This Approach:** Simplest validation, Phase 2 will use real-time subscriptions

**Decision 5: Stub `holt watch` Command**
* **Rationale:** Establish command structure for Phase 2, guide users to --watch flag
* **Implementation:** Print informational message, exit immediately
* **Risk:** User confusion about command availability (mitigated: clear guidance message)
* **Why This Approach:** Maintains consistent CLI interface, prepares for Phase 2

**Biggest Risks:**
1. **Port conflicts**: Multiple Holt instances or other services using 6379-638X
   * **Mitigation:** Port allocation algorithm checks both Docker labels and bind availability
2. **Race condition on startup**: Artefact created before orchestrator subscribes
   * **Acceptable for Phase 1:** Integration tests will start orchestrator first
3. **Git validation edge cases**: Submodules, worktrees, detached HEAD
   * **Mitigation:** Use standard `git status --porcelain`, document limitations

### **3.2. Implementation steps**

**Phase 1: M1.4 Modification - Redis Port Exposure (Days 1-2)**
- [ ] Add port allocation algorithm to `internal/instance/ports.go`
  - [ ] Query Docker for existing `holt.redis.port` labels
  - [ ] Find highest port number, increment
  - [ ] Verify port is bindable on host (not in use by other processes)
  - [ ] Return available port
- [ ] Modify `cmd/holt/commands/up.go`
  - [ ] Call port allocation before creating Redis container
  - [ ] Add port binding to Redis container creation: `6379/tcp → {allocatedPort}`
  - [ ] Add label `holt.redis.port={allocatedPort}` to Redis container
  - [ ] Update success message to show allocated port
- [ ] Update M1.4 integration tests to verify port allocation and labeling

**Phase 2: Git Validation Logic (Day 3)**
- [ ] Create `internal/git/validation.go`
  - [ ] Implement `IsGitRepository()` - check for .git directory
  - [ ] Implement `GetGitRoot()` - run `git rev-parse --show-toplevel`
  - [ ] Implement `IsWorkspaceClean()` - run `git status --porcelain`
  - [ ] Implement `GetDirtyFiles()` - parse porcelain output for error messages
- [ ] Write unit tests for Git validation functions
- [ ] Test edge cases: submodules, empty repos, detached HEAD

**Phase 3: Instance Discovery Logic (Day 4)**
- [ ] Create `internal/instance/discovery.go`
  - [ ] Implement `FindInstanceByWorkspace(workspacePath)` - query Docker labels
  - [ ] Implement `GetInstanceRedisPort(instanceName)` - read label from Redis container
  - [ ] Implement `VerifyInstanceRunning(instanceName)` - check container state
  - [ ] Handle canonicalization: resolve symlinks, absolute paths
- [ ] Write unit tests with mock Docker client
- [ ] Integration tests with real containers

**Phase 4: `holt forage` Command (Days 5-6)**
- [ ] Create `cmd/holt/commands/forage.go`
  - [ ] Define command structure with Cobra
  - [ ] Add `--name`, `--watch`, `--goal` flags
  - [ ] Implement prerequisite validation:
    - [ ] Check Git repository exists
    - [ ] Check Git workspace is clean
    - [ ] Resolve instance name (infer or explicit)
    - [ ] Verify instance is running
    - [ ] Discover Redis port
  - [ ] Implement GoalDefined artefact creation:
    - [ ] Generate new UUIDs for ID and LogicalID
    - [ ] Set version=1, structural_type=Standard, type="GoalDefined"
    - [ ] Set payload to goal string, produced_by_role="user"
    - [ ] Set source_artefacts=[]
  - [ ] Connect to Redis via discovered port
  - [ ] Call `client.CreateArtefact()`
  - [ ] Print artefact ID on success
  - [ ] If --watch: call `runWatch(artefactID)`
- [ ] Implement comprehensive error handling with M1.4-style messages

**Phase 5: `runWatch` Function (Day 7)**
- [ ] Create `internal/watch/watch.go`
  - [ ] Implement `runWatch(ctx, client, artefactID, timeout)`
  - [ ] Poll loop: every 200ms for 5 seconds total
  - [ ] Use `client.GetClaimByArtefactID(ctx, artefactID)`
  - [ ] Print progress indicator (spinner or "⏳ Waiting...")
  - [ ] On success: print "✓ Claim created: {claim_id} (status: {status})"
  - [ ] On timeout: print error with troubleshooting guidance
- [ ] Write unit tests with mock blackboard client
- [ ] Test timeout behavior, immediate success, and error conditions

**Phase 6: `holt watch` Stub Command (Day 8)**
- [ ] Create `cmd/holt/commands/watch.go`
  - [ ] Define command structure with Cobra
  - [ ] Add `--name` flag (for consistency)
  - [ ] Implement stub behavior:
    - [ ] Print informational message
    - [ ] Suggest using `holt forage --watch` for Phase 1
    - [ ] Exit with code 0
- [ ] Write minimal test verifying stub behavior

**Phase 7: Integration & E2E Tests (Days 9-10)**
- [ ] Write E2E test: `TestForage_CreatesArtefact`
  - [ ] Start Redis + orchestrator containers
  - [ ] Run `holt forage --goal "test"`
  - [ ] Verify artefact exists in Redis
  - [ ] Verify correct structure (GoalDefined, Standard, etc.)
- [ ] Write E2E test: `TestForage_WithWatch`
  - [ ] Start full system
  - [ ] Run `holt forage --watch --goal "test"`
  - [ ] Verify claim creation detected within timeout
- [ ] Write E2E test: `TestForage_GitValidation`
  - [ ] Create dirty Git repo
  - [ ] Verify forage fails with helpful error
- [ ] Write E2E test: `TestForage_InstanceInference`
  - [ ] Test 0, 1, 2+ instance scenarios
  - [ ] Verify correct behavior for each
- [ ] Write E2E test: `TestWatchStub`
  - [ ] Verify stub message printed
  - [ ] Verify exit code 0

**Phase 8: Documentation & Polish (Day 11)**
- [ ] Update `cmd/holt/commands/README.md` or create forage-specific docs
- [ ] Add examples of forage usage
- [ ] Document Git validation requirements
- [ ] Document instance inference logic
- [ ] Update M1.4 design document with Redis port modification
- [ ] Update Phase 1 README with complete E2E workflow
- [ ] Code review and refactoring

### **3.3. Performance & resource considerations**

* **Resource usage:**
  * **CPU:** Minimal - polling loop runs 25 iterations max (5s / 200ms)
  * **Memory:** < 20 MB (Go CLI binary, blackboard client)
  * **Network:** Single Redis connection, minimal traffic (artefact creation + claim polling)
  * **Disk:** None (stateless CLI command)

* **Scalability limits:**
  * **Port allocation:** 100 concurrent instances (6379-6478) before needing second port range
  * **Git validation:** Works with repos up to any size (delegates to git CLI)
  * **Polling efficiency:** Acceptable for Phase 1 validation (Phase 2 will use Pub/Sub)

* **Performance requirements:**
  * **Git validation time:** < 500ms for typical repo (depends on repo size)
  * **Artefact creation time:** < 100ms (Redis write + publish)
  * **Claim detection time:** < 200ms typical (orchestrator processes in ~50-100ms)
  * **Total forage --watch time:** < 1 second for success, 5 seconds for timeout

### **3.4. Testing strategy**

* **Unit tests:**
  * `internal/git/validation_test.go`:
    * `TestIsGitRepository()` - Validates .git detection
    * `TestGetGitRoot()` - Validates git rev-parse parsing
    * `TestIsWorkspaceClean()` - Validates porcelain output parsing
    * `TestGetDirtyFiles()` - Validates error message formatting
  * `internal/instance/discovery_test.go`:
    * `TestFindInstanceByWorkspace()` - Mock Docker queries
    * `TestGetInstanceRedisPort()` - Mock label reading
    * `TestVerifyInstanceRunning()` - Mock container state checks
  * `internal/instance/ports_test.go`:
    * `TestAllocatePort()` - Validates port allocation algorithm
    * `TestFindNextAvailablePort()` - Edge cases (all ports taken, etc.)
  * `internal/watch/watch_test.go`:
    * `TestRunWatch_Success()` - Mock immediate claim found
    * `TestRunWatch_Timeout()` - Mock no claim after 5s
    * `TestRunWatch_ProgressIndicator()` - Verify UI output

* **Integration tests:**
  * `cmd/holt/commands/forage_integration_test.go` (requires Docker):
    * `TestForage_CreatesGoalDefinedArtefact`:
      1. Start Redis container
      2. Run forage command (as subprocess)
      3. Query Redis directly to verify artefact structure
    * `TestForage_GitValidation_FailsOnDirty`:
      1. Create temp Git repo with uncommitted file
      2. Run forage, expect error
      3. Verify error message contains dirty file name
    * `TestForage_InstanceInference_SingleInstance`:
      1. Start one instance for workspace
      2. Run forage without --name
      3. Verify correct instance targeted
    * `TestForage_InstanceInference_MultipleInstances`:
      1. Start two instances on same workspace (using --force)
      2. Run forage without --name
      3. Verify error requiring explicit --name
    * `TestForage_RedisPortDiscovery`:
      1. Start instance with Redis on port 6380
      2. Verify forage discovers correct port from label
      3. Verify successful connection

* **E2E tests (full system):**
  * `cmd/holt/commands/e2e_test.go`:
    * `TestE2E_Phase1_Heartbeat`:
      1. Run `holt init` in temp directory
      2. Run `holt up`
      3. Run `holt forage --watch --goal "hello world"`
      4. Verify:
         - Artefact created
         - Orchestrator created claim
         - Claim detected within timeout
         - Success message printed
      5. Run `holt down`
      6. Verify clean teardown
    * `TestE2E_ForageWatch_OrchestatorDown`:
      1. Start Redis without orchestrator
      2. Run `holt forage --watch --goal "test"`
      3. Verify timeout error after 5s
      4. Verify helpful troubleshooting message

* **Performance tests:**
  * `TestForage_LatencyBenchmark`:
    * Measure time from forage invocation to artefact creation
    * Assert < 1 second for typical case
  * `TestForage_WatchLatencyBenchmark`:
    * Measure time from artefact creation to claim detection
    * Assert < 500ms for typical case

**Test coverage target:** ≥ 85% for all M1.6 packages

---

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

* **No new third-party dependencies** beyond existing:
  * `github.com/spf13/cobra` - Already used for CLI commands
  * `github.com/docker/docker` - Already used in M1.4
  * Standard library for Git subprocess calls

* **What we're NOT building:**
  * No goal parsing or validation (just pass through string)
  * No goal templates or structured formats
  * No batch goal submission
  * No persistent watch process (stub only)

### **4.2. Auditability**

* **New artefacts created:** GoalDefined artefacts with complete provenance
  * Every forage command creates immutable artefact with timestamp
  * `produced_by_role: "user"` clearly identifies CLI origin
  * Artefact ID printed to user for traceability

* **Audit trail preservation:**
  * Every forage invocation logged with goal content
  * Artefact creation time captured by Redis
  * Complete chain from CLI → Artefact → Claim → (future) Agent actions

* **No modification of existing data:** All operations create new records only

### **4.3. Small, single-purpose components**

* **forage command's single purpose:** Create GoalDefined artefacts from user goals
* **runWatch's single purpose:** Validate claim creation for given artefact
* **Git validation's single purpose:** Verify workspace prerequisites
* **Instance discovery's single purpose:** Resolve instance name to connection details

**Tight coupling avoided:**
* forage → blackboard client (interface, not implementation)
* forage → Git validation (pure functions, no state)
* forage → instance discovery (Docker API wrapper)
* runWatch → blackboard client (interface-based)

### **4.4. Security considerations**

* **Attack surfaces:**
  * **Redis exposure on localhost:** Only localhost binding, not exposed to network
  * **Goal string injection:** Validated as opaque string, no parsing/execution
  * **Git command injection:** Uses standard library exec, no shell interpolation
  * **Docker API access:** Read-only queries for instance discovery

* **Data protection:**
  * Goal strings may contain sensitive information (logged to blackboard, not CLI logs)
  * Redis connection string not logged
  * Container labels readable by Docker socket access (requires Docker permissions)

* **Privilege requirements:**
  * Docker socket access (same as M1.4 `holt up`)
  * Git repository read access (standard user permissions)
  * Network bind check (non-privileged port range 6379+)

**Security improvements for Phase 2:**
* Remove Redis localhost exposure (use container networking)
* Add authentication to Redis connections
* Implement secrets management for sensitive goals

### **4.5. Backward compatibility**

* **Does this change existing APIs?** No - adds new CLI commands only
* **Does this modify existing data structures?** No - uses M1.1 Artefact schema unchanged
* **Are existing workflows preserved?** Yes - all M1.1-M1.5 functionality unaffected
* **Is this feature additive?** Yes - pure addition

**Compatibility guarantees:**
* GoalDefined artefacts follow standard Artefact schema (no special fields)
* `holt forage` can be added to existing systems without migration
* Redis port exposure is additive to M1.4 (doesn't break existing functionality)

**Breaking change in M1.4 modification:**
* Existing `holt up` behavior changes slightly (adds port mapping)
* **Mitigation:** Port mapping is additive, doesn't break existing container logic
* **Documentation:** Update M1.4 docs with note about Redis port exposure

### **4.6. Dependency impact**

* **Redis usage changes:**
  * Adds localhost port binding (6379+)
  * No change to Redis configuration or persistence
  * No new Redis features required

* **Docker requirements:**
  * No change - uses existing SDK
  * Adds one label (`holt.redis.port`) to Redis containers
  * Adds port binding (standard Docker feature)

* **Go version:** Requires Go 1.23+ (consistent with M1.1-M1.5)

* **Git requirements:**
  * Git CLI must be available on PATH
  * Version 2.0+ (standard for `git status --porcelain`)

**CI/CD impact:**
* Integration tests require Git in test environment
* E2E tests require Git + Docker (same as M1.4)

---

## **5. Definition of done**

*This checklist must be fully satisfied for the milestone to be considered complete.*

- [ ] All implementation steps from section 3.2 are complete
- [ ] All tests defined in section 3.4 are implemented and passing
- [ ] Performance requirements from section 3.3 are met and verified
- [ ] Overall test coverage has not decreased (minimum 85% for M1.6 packages)
- [ ] The Makefile has been updated with any new test commands
- [ ] `holt forage --goal "..."` creates GoalDefined artefact correctly
- [ ] `holt forage --watch --goal "..."` detects claim creation within timeout
- [ ] `holt watch` stub prints informational message and exits
- [ ] Git validation fails on dirty workspace with helpful error
- [ ] Instance inference works for 0, 1, and 2+ instances
- [ ] Redis port allocation finds available ports correctly
- [ ] M1.4 modification (Redis port exposure) is implemented and tested
- [ ] All failure modes identified in section 6.1 have been implemented and tested
- [ ] Concurrency considerations from section 6.2 have been addressed
- [ ] All open questions from section 7 have been resolved
- [ ] Error messages follow M1.4 pattern (Title, Explanation, Suggested Actions)
- [ ] Security considerations from section 4.4 have been addressed
- [ ] Backward compatibility requirements from section 4.5 are satisfied
- [ ] M1.4 design document updated with Redis port modification note
- [ ] Phase 1 success criteria validated: CLI → Artefact → Orchestrator → Claim proven
- [ ] E2E test passes: `holt init && holt up && holt forage --watch --goal "hello world"`
- [ ] Documentation is complete (command help, examples, troubleshooting)

---

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Failure 1: Not in Git repository**
* **Scenario:** User runs `holt forage` outside Git repository
* **Detection:** `git rev-parse --show-toplevel` returns error
* **Response:** Fail with error message
* **Error Message:**
  ```
  Error: not a Git repository

  Holt requires a Git repository to manage workflows.

  Initialize Git first:
    git init
    holt init
    holt up
  ```
* **Test:** `TestForage_NotInGitRepo`

**Failure 2: Dirty Git workspace**
* **Scenario:** User has uncommitted changes or untracked files
* **Detection:** `git status --porcelain` returns non-empty output
* **Response:** Fail with detailed error showing dirty files
* **Error Message:**
  ```
  Error: Git workspace is not clean

  Uncommitted changes:
   M src/main.go
   M pkg/utils.go

  Untracked files:
  ?? temp.txt
  ?? debug.log

  Please commit or stash changes before running holt forage:
    git add .
    git commit -m "your message"

  Or to stash temporarily:
    git stash
  ```
* **Test:** `TestForage_DirtyWorkspace`

**Failure 3: No instances running**
* **Scenario:** User runs forage but no Holt instances exist
* **Detection:** Docker query returns empty list
* **Response:** Fail with startup instructions
* **Error Message:**
  ```
  Error: no Holt instances found

  No running instances found for workspace:
    /home/user/myproject

  Start an instance first:
    holt up

  Then retry:
    holt forage --goal "your goal"
  ```
* **Test:** `TestForage_NoInstances`

**Failure 4: Multiple instances on same workspace**
* **Scenario:** 2+ instances running on same workspace (via --force)
* **Detection:** Docker query returns multiple matches
* **Response:** Fail requiring explicit --name
* **Error Message:**
  ```
  Error: multiple instances found

  Found 2 running instances for this workspace:
    - dev (started 2h ago)
    - test (started 30m ago)

  Specify which instance to use:
    holt forage --name dev --goal "your goal"
    holt forage --name test --goal "your goal"
  ```
* **Test:** `TestForage_MultipleInstances`

**Failure 5: Instance exists but not running**
* **Scenario:** Container exists but is stopped
* **Detection:** Container state != "running"
* **Response:** Fail with restart instructions
* **Error Message:**
  ```
  Error: instance 'prod' is not running

  Container exists but is stopped.

  Start the instance:
    holt up --name prod

  Or if stuck, restart:
    holt down --name prod
    holt up --name prod
  ```
* **Test:** `TestForage_InstanceStopped`

**Failure 6: Redis port not discoverable**
* **Scenario:** Container missing `holt.redis.port` label
* **Detection:** Label query returns empty
* **Response:** Fail with troubleshooting guidance
* **Error Message:**
  ```
  Error: Redis port not found

  Instance 'prod' exists but Redis port label is missing.

  This may indicate:
    - Instance was created with older holt version
    - Manual container manipulation

  Restart the instance:
    holt down --name prod
    holt up --name prod
  ```
* **Test:** `TestForage_MissingPortLabel`

**Failure 7: Redis connection refused**
* **Scenario:** Port is bound but Redis not accepting connections
* **Detection:** Blackboard client connection fails
* **Response:** Fail with troubleshooting steps
* **Error Message:**
  ```
  Error: Redis connection failed

  Could not connect to Redis at localhost:6379

  Check Redis container status:
    docker logs holt-redis-prod

  Restart if needed:
    holt down --name prod
    holt up --name prod
  ```
* **Test:** `TestForage_RedisConnectionFailed`

**Failure 8: Missing --goal flag**
* **Scenario:** User runs `holt forage` without --goal
* **Detection:** Cobra flag validation
* **Response:** Fail with usage example
* **Error Message:**
  ```
  Error: required flag --goal not provided

  Usage:
    holt forage --goal "description of what you want to build"

  Example:
    holt forage --goal "Create a REST API for user management"

  For immediate validation:
    holt forage --watch --goal "your goal"
  ```
* **Test:** `TestForage_MissingGoalFlag`

**Failure 9: Empty goal string**
* **Scenario:** User provides `--goal ""`
* **Detection:** String length check after parsing
* **Response:** Fail with guidance
* **Error Message:**
  ```
  Error: goal cannot be empty

  Please provide a meaningful goal description.

  Example:
    holt forage --goal "Build a web scraper for news articles"
  ```
* **Test:** `TestForage_EmptyGoal`

**Failure 10: Orchestrator not creating claim (--watch timeout)**
* **Scenario:** 5 seconds elapsed, no claim created
* **Detection:** Polling timeout in runWatch
* **Response:** Print timeout error with troubleshooting
* **Error Message:**
  ```
  ✓ Goal artefact created: abc-123-def-456
  ⏳ Waiting for orchestrator to create claim...

  Error: timeout waiting for claim

  No claim created after 5 seconds.

  Possible causes:
    - Orchestrator container not running
    - Orchestrator not subscribed to artefact_events
    - Redis Pub/Sub issue

  Check orchestrator status:
    docker ps | grep orchestrator
    docker logs holt-orchestrator-prod

  Check artefact was created:
    holt hoard --name prod
  ```
* **Test:** `TestForage_WatchTimeout`

### **6.2. Concurrency considerations**

**Consideration 1: Multiple forage commands simultaneously**
* **Scenario:** User runs `holt forage` in multiple terminals
* **Handling:** Each creates independent artefact with unique ID
* **Safety:** No shared state, no race conditions
* **Implementation:** No synchronization needed

**Consideration 2: Port allocation race condition**
* **Scenario:** Two `holt up` commands run simultaneously
* **Handling:** Each queries Docker for highest port, could allocate same port
* **Safety:** Second bind will fail, up command rolls back
* **Implementation:** Acceptable for Phase 1 (rare edge case)
* **Phase 2 Fix:** Use atomic Redis counter or file-based lock

**Consideration 3: Instance inference during forage**
* **Scenario:** Instance stops between discovery and artefact creation
* **Handling:** Redis connection will fail with clear error
* **Safety:** No data corruption, just failed operation
* **Implementation:** Acceptable error handling

**Consideration 4: Git validation race condition**
* **Scenario:** Files modified between validation and artefact creation
* **Handling:** Validation is best-effort snapshot in time
* **Safety:** Acceptable - user should not modify workspace during operation
* **Implementation:** No locking needed (Git handles concurrent access)

### **6.3. Edge case handling**

**Edge Case 1: Very long goal strings**
* **Scenario:** Goal string is 10KB+ (unlikely but possible)
* **Handling:** Redis supports up to 512MB values - no practical limit
* **Implementation:** No size validation needed

**Edge Case 2: Goal with special characters**
* **Scenario:** Goal contains newlines, quotes, Unicode, etc.
* **Handling:** Store as opaque string, no parsing or escaping
* **Implementation:** Use raw string, no sanitization

**Edge Case 3: Symlinked workspace paths**
* **Scenario:** Current directory is symlink to real workspace
* **Handling:** Canonicalize paths using filepath.EvalSymlinks before comparison
* **Implementation:** Both forage and M1.4 must canonicalize consistently

**Edge Case 4: Git submodules**
* **Scenario:** User runs forage from within submodule
* **Handling:** `git rev-parse --show-toplevel` returns submodule root
* **Implementation:** Works correctly - treats submodule as workspace

**Edge Case 5: Detached HEAD state**
* **Scenario:** Git repo in detached HEAD (not on branch)
* **Handling:** `git status --porcelain` still works correctly
* **Implementation:** No special handling needed

**Edge Case 6: All ports exhausted (6379-6478)**
* **Scenario:** 100 instances running
* **Handling:** Port allocation fails, holt up returns error
* **Error Message:**
  ```
  Error: no available Redis ports

  All ports from 6379-6478 are in use.

  Stop unused instances:
    holt list
    holt down --name <unused-instance>
  ```
* **Implementation:** Check range exhaustion before bind attempt

**Edge Case 7: Rapid forage + down cycle**
* **Scenario:** User creates artefact then immediately runs `holt down`
* **Handling:** Artefact persists in Redis (no cleanup in M1.6)
* **Implementation:** Acceptable - artefacts are immutable, no harm

**Edge Case 8: runWatch polling exactly at claim creation**
* **Scenario:** Claim created between polling attempts
* **Handling:** Detected on next poll (within 200ms)
* **Implementation:** Acceptable latency for Phase 1

---

## **7. Open questions & decisions**

### **Resolved Decisions**

✅ **Q1: Redis connectivity approach**
* **Decision:** Expose Redis on localhost with port mapping (M1.4 modification)
* **Rationale:** Simplest for Phase 1, documented as technical debt

✅ **Q2: Instance name resolution**
* **Decision:** Infer from workspace path, require --name if ambiguous
* **Rationale:** Best UX for single-instance workflows

✅ **Q3: Git workspace validation**
* **Decision:** Strict enforcement (fail on any dirty files) with detailed error
* **Rationale:** Spec requirement, prevents bad state

✅ **Q4: Watch implementation**
* **Decision:** Simple polling (200ms, 5s timeout) for Phase 1
* **Rationale:** Sufficient for validation, Phase 2 will use Pub/Sub

✅ **Q5: Polling interval**
* **Decision:** 200ms (25 polls in 5 seconds)
* **Rationale:** Balances responsiveness and resource usage

✅ **Q6: Error message format**
* **Decision:** Follow M1.4 pattern (Title, Explanation, Suggested Actions)
* **Rationale:** Consistency across CLI

✅ **Q7: Instance state validation**
* **Decision:** Verify container is running, fail if stopped
* **Rationale:** Better error messages than connection failures

✅ **Q8: `holt watch` behavior**
* **Decision:** Stub command with informational message
* **Rationale:** Establishes structure for Phase 2, guides users correctly

### **Open Questions for Phase 2**

⏭️ **Q9: How to remove Redis port exposure?**
* **Current State:** Redis exposed on localhost for CLI connectivity
* **Phase 2 Solution:** Use sidecar container or HTTP API for blackboard access

⏭️ **Q10: How to implement real-time `holt watch`?**
* **Current State:** Stub command only
* **Phase 2 Solution:** Subscribe to claim_events channel, stream updates to terminal

⏭️ **Q11: Should forage support goal templates?**
* **Current State:** Free-form string only
* **Phase 3+ Solution:** Structured goal formats (JSON, YAML) for complex workflows

---

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**Start with the simplest path:**
1. **Day 1-2:** M1.4 modification (port allocation + labels)
2. **Day 3:** Git validation functions (pure, testable)
3. **Day 4:** Instance discovery (mock Docker for tests)
4. **Day 5-6:** Forage command (integrate all pieces)
5. **Day 7:** runWatch implementation (polling loop)
6. **Day 8:** Watch stub (trivial)
7. **Day 9-10:** E2E tests (full system validation)
8. **Day 11:** Documentation and polish

**Defensive programming checklist:**
- [ ] Validate all user inputs (flags, goal string)
- [ ] Canonicalize all file paths (symlinks, relative paths)
- [ ] Use context timeouts for all Redis operations
- [ ] Handle Docker API errors gracefully
- [ ] Parse Git output defensively (handle unexpected formats)
- [ ] Test error paths as thoroughly as happy paths

**Error handling from the beginning:**
- Every external dependency (Git, Docker, Redis) wrapped in error checks
- All errors logged with sufficient context for debugging
- User-facing errors follow M1.4 pattern consistently
- No panics - always return errors gracefully

### **8.2. Common pitfalls to avoid**

**Pitfall 1: Forgetting to canonicalize workspace paths**
* **Problem:** Symlinks cause workspace inference to fail
* **Solution:** Use `filepath.EvalSymlinks` + `filepath.Abs` consistently
* **Test:** `TestForage_SymlinkedWorkspace`

**Pitfall 2: Not handling empty Git porcelain output**
* **Problem:** Clean workspace returns empty string, not explicit "clean" message
* **Solution:** Check `len(output) == 0` for clean workspace
* **Test:** `TestGitValidation_CleanWorkspace`

**Pitfall 3: Assuming port 6379 is always available**
* **Problem:** First instance might need 6380 if 6379 in use by other process
* **Solution:** Always check bind availability, not just Docker labels
* **Test:** `TestPortAllocation_6379InUse`

**Pitfall 4: Not handling Redis connection timeout**
* **Problem:** Connection hangs if Redis is down
* **Solution:** Use context.WithTimeout for all Redis operations (2s default)
* **Test:** `TestForage_RedisTimeout`

**Pitfall 5: Forgetting to print artefact ID on success**
* **Problem:** User has no way to reference the created artefact
* **Solution:** Always print artefact ID, even without --watch
* **Test:** `TestForage_PrintsArtefactID`

**Pitfall 6: Polling without progress indicator**
* **Problem:** User thinks command is hung during 5s wait
* **Solution:** Print "⏳ Waiting for orchestrator..." with spinner or dots
* **Test:** `TestRunWatch_ProgressIndicator`

**Pitfall 7: Not testing with real orchestrator**
* **Problem:** Mocks hide integration issues
* **Solution:** E2E tests with full system (Redis + orchestrator)
* **Test:** `TestE2E_Phase1_Heartbeat`

### **8.3. Integration checklist**

**Pre-implementation verification:**
- [x] M1.1 (Blackboard Foundation) complete - Artefact types defined
- [x] M1.2 (Blackboard Client Operations) complete - CreateArtefact available
- [x] M1.3 (CLI Project Initialization) complete - holt init works
- [x] M1.4 (CLI Lifecycle Management) complete - holt up/down work
- [x] M1.5 (Orchestrator Claim Engine) complete - orchestrator creates claims
- [ ] M1.4 modification ready for Redis port exposure

**Implementation validation:**
- [ ] No breaking changes to Artefact schema
- [ ] forage uses standard CreateArtefact method
- [ ] GoalDefined follows Standard structural type conventions
- [ ] Redis connection uses discovered port from labels
- [ ] Error messages follow M1.4 patterns exactly

---

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Metrics to track:**
* **Forage invocations:** Counter of total forage commands executed
* **Forage success rate:** % of successful artefact creations
* **Forage latency:** Time from command invocation to artefact created
* **Watch success rate:** % of --watch runs that detected claim within timeout
* **Watch latency:** Time from artefact creation to claim detection
* **Git validation failures:** Counter of dirty workspace rejections
* **Instance inference failures:** Counter of ambiguous instance scenarios

**Logging events:**
```bash
# Successful forage
✓ Goal artefact created: abc-123-def-456

# Successful forage --watch
✓ Goal artefact created: abc-123-def-456
⏳ Waiting for orchestrator to create claim...
✓ Claim created: def-789-ghi-012 (status: pending_review)

Next steps:
  - Run 'holt watch' to monitor progress (Phase 2)
  - Run 'holt hoard' to view all artefacts

# Failed validation
Error: Git workspace is not clean

Uncommitted changes:
 M src/main.go

Please commit changes before running holt forage
```

**How operators detect issues:**
1. **Forage failing**: Check error message for specific failure mode
2. **Watch timing out**: Verify orchestrator is running (`docker ps`)
3. **Port conflicts**: Check port allocation errors in `holt up` logs

### **9.2. Rollback and disaster recovery**

**Can this feature be disabled via configuration?**
* No - forage is a core CLI command, cannot be disabled
* However, if Redis port exposure causes issues, can revert M1.4 modification

**Rollback procedure:**
* **If forage has bugs:** Users can manually create artefacts via Redis CLI
* **If Redis port exposure is problematic:** Revert M1.4 modification, document workaround
* **Steps:**
  1. Identify issue (e.g., port conflicts)
  2. Document workaround (e.g., manual port forwarding)
  3. Fix in next release
  4. No data migration needed (artefacts unchanged)

**Data migration for rollback:**
* **No migration needed:** Artefacts already created remain valid
* **Side effect:** New forage commands won't work until fix applied

**Recovery time objective (RTO):**
* **Forage bug fix:** < 1 hour (patch release)
* **M1.4 revert:** < 30 minutes (restart instances)

### **9.3. Documentation and training**

**New CLI commands:**
* `holt forage --goal "..."` - Create workflow
* `holt forage --watch --goal "..."` - Create and validate
* `holt watch` - Stub for Phase 2

**User guides to update:**
* `README.md`: Add Phase 1 E2E workflow section
* `cmd/holt/commands/forage.md`: Complete usage guide
* Phase 1 validation guide: Document expected behavior

**Command help text:**
```
holt forage --help

Create a new workflow by submitting a goal description.

Usage:
  holt forage [--name <instance>] [--watch] --goal "description"

Flags:
  --goal string     Goal description (required)
  --name string     Target instance name (auto-inferred if omitted)
  --watch           Wait for orchestrator to create claim (Phase 1 validation)

Examples:
  # Create workflow on inferred instance
  holt forage --goal "Build a REST API for user management"

  # Target specific instance
  holt forage --name prod --goal "Refactor authentication module"

  # Validate orchestrator response
  holt forage --watch --goal "Add logging to all endpoints"

Prerequisites:
  - Git repository with clean workspace (no uncommitted changes)
  - Running Holt instance (start with 'holt up')

Phase 1 Note:
  This command creates the initial GoalDefined artefact. Agent execution
  is planned for Phase 2.
```

**Troubleshooting guides:**
* **Issue:** "Git workspace is not clean"
  * **Check:** Run `git status` to see dirty files
  * **Fix:** Commit or stash changes
  * **Command:** `git add . && git commit -m "message"` or `git stash`

* **Issue:** "No Holt instances found"
  * **Check:** Run `holt list` to verify
  * **Fix:** Start instance with `holt up`

* **Issue:** "Multiple instances found"
  * **Check:** Run `holt list` to see instances
  * **Fix:** Use `--name` flag to specify which instance

* **Issue:** "Timeout waiting for claim"
  * **Check:** Verify orchestrator is running: `docker ps | grep orchestrator`
  * **Fix:** Check orchestrator logs: `docker logs holt-orchestrator-<instance>`
  * **Verify:** Run `holt hoard` to confirm artefact was created

**Team training needs:**
* Developers need to understand:
  * Phase 1 scope (artefact creation only, no agent execution)
  * Git workspace clean requirement
  * Instance inference logic (workspace-based)
  * How to use --watch for E2E validation
  * Troubleshooting failed forage commands

---

## **10. Self-validation checklist**

### **Before starting implementation:**

- [x] I understand how this feature completes Phase 1 (E2E validation)
- [x] All success criteria (section 1.3) are measurable and testable
- [x] I have considered every component in section 2 explicitly
- [x] All design decisions (section 3.1) are justified and documented
- [x] I understand the M1.4 modification required (Redis port exposure)
- [x] I understand the difference between forage and watch commands

### **During implementation:**

- [ ] I am implementing the simplest solution that meets success criteria
- [ ] All error scenarios (section 6) are being handled, not just happy path
- [ ] Tests are being written before or alongside code (TDD approach)
- [ ] I am validating that M1.4 containers work with port modification
- [ ] Git validation is strict (fails on any dirty files)
- [ ] Error messages follow M1.4 format pattern

### **Before submission:**

- [ ] All items in Definition of Done (section 5) are complete
- [ ] Feature has been tested in clean environment from scratch
- [ ] E2E test passes: `holt init && holt up && holt forage --watch --goal "hello world"`
- [ ] Phase 1 success criteria validated: CLI → Artefact → Orchestrator → Claim
- [ ] Documentation is updated and accurate (command help, troubleshooting)
- [ ] I have considered the operational impact (section 9) of this feature
- [ ] No TODOs remain in production code
- [ ] M1.4 design document updated with Redis port modification note
- [ ] Code review completed and feedback addressed

---

## **Appendix A: GoalDefined Artefact Structure**

```go
// Example GoalDefined artefact created by holt forage
artefact := &blackboard.Artefact{
    ID:               "f47ac10b-58cc-4372-a567-0e02b2c3d479", // New UUID
    LogicalID:        "a1b2c3d4-5e6f-7890-abcd-ef1234567890", // New UUID (new thread)
    Version:          1,                                       // First in thread
    StructuralType:   blackboard.StructuralTypeStandard,      // Not Terminal
    Type:             "GoalDefined",                           // Special reserved type
    Payload:          "Build a REST API for user management", // Raw goal string
    SourceArtefacts:  []string{},                             // No predecessors
    ProducedByRole:   "user",                                  // From CLI, not agent
}

// Validation ensures:
// - ID is valid UUID
// - LogicalID is valid UUID
// - StructuralType is valid enum value
// - Type is non-empty string
// - ProducedByRole is non-empty string
```

---

## **Appendix B: Port Allocation Algorithm**

```go
package instance

import (
    "context"
    "fmt"
    "net"
    "strconv"

    "github.com/docker/docker/api/types"
    "github.com/docker/docker/api/types/filters"
    "github.com/docker/docker/client"
)

// FindNextAvailablePort finds the next available port for Redis, starting from 6379.
// Returns the port number or error if all ports in range (6379-6478) are exhausted.
func FindNextAvailablePort(ctx context.Context, cli *client.Client) (int, error) {
    const (
        startPort = 6379
        endPort   = 6478
        maxPorts  = 100
    )

    // Query Docker for existing holt.redis.port labels
    filters := filters.NewArgs()
    filters.Add("label", "holt.project=true")
    filters.Add("label", "holt.component=redis")

    containers, err := cli.ContainerList(ctx, types.ContainerListOptions{
        All:     true,
        Filters: filters,
    })
    if err != nil {
        return 0, fmt.Errorf("failed to query Docker containers: %w", err)
    }

    // Build set of used ports
    usedPorts := make(map[int]bool)
    for _, container := range containers {
        if portStr, ok := container.Labels["holt.redis.port"]; ok {
            if port, err := strconv.Atoi(portStr); err == nil {
                usedPorts[port] = true
            }
        }
    }

    // Find first available port
    for port := startPort; port <= endPort; port++ {
        if usedPorts[port] {
            continue
        }

        // Verify port is bindable on host
        if isPortBindable(port) {
            return port, nil
        }
    }

    return 0, fmt.Errorf("no available Redis ports (range %d-%d exhausted)", startPort, endPort)
}

// isPortBindable checks if a port can be bound on localhost.
// Returns true if port is available, false if in use.
func isPortBindable(port int) bool {
    addr := fmt.Sprintf(":%d", port)
    listener, err := net.Listen("tcp", addr)
    if err != nil {
        return false
    }
    listener.Close()
    return true
}
```

---

## **Appendix C: Git Validation Functions**

```go
package git

import (
    "fmt"
    "os/exec"
    "strings"
)

// IsGitRepository checks if the current directory is inside a Git repository.
func IsGitRepository() bool {
    cmd := exec.Command("git", "rev-parse", "--git-dir")
    return cmd.Run() == nil
}

// GetGitRoot returns the absolute path to the Git repository root.
// Returns error if not in a Git repository.
func GetGitRoot() (string, error) {
    cmd := exec.Command("git", "rev-parse", "--show-toplevel")
    output, err := cmd.Output()
    if err != nil {
        return "", fmt.Errorf("not a Git repository")
    }
    return strings.TrimSpace(string(output)), nil
}

// IsWorkspaceClean returns true if the Git working directory has no uncommitted changes.
// This includes staged, unstaged, and untracked files.
func IsWorkspaceClean() (bool, error) {
    cmd := exec.Command("git", "status", "--porcelain")
    output, err := cmd.Output()
    if err != nil {
        return false, fmt.Errorf("failed to check Git status: %w", err)
    }
    return len(strings.TrimSpace(string(output))) == 0, nil
}

// GetDirtyFiles returns a formatted list of uncommitted changes for error messages.
// Returns empty string if workspace is clean.
func GetDirtyFiles() (string, error) {
    cmd := exec.Command("git", "status", "--porcelain")
    output, err := cmd.Output()
    if err != nil {
        return "", fmt.Errorf("failed to check Git status: %w", err)
    }

    porcelain := strings.TrimSpace(string(output))
    if porcelain == "" {
        return "", nil
    }

    // Parse porcelain output into categorized lists
    var modified, untracked []string
    for _, line := range strings.Split(porcelain, "\n") {
        if len(line) < 3 {
            continue
        }
        status := line[:2]
        file := strings.TrimSpace(line[3:])

        if strings.HasPrefix(status, "??") {
            untracked = append(untracked, file)
        } else {
            modified = append(modified, file)
        }
    }

    // Format output
    var parts []string
    if len(modified) > 0 {
        parts = append(parts, "Uncommitted changes:")
        for _, file := range modified {
            parts = append(parts, fmt.Sprintf(" M %s", file))
        }
    }
    if len(untracked) > 0 {
        if len(parts) > 0 {
            parts = append(parts, "")
        }
        parts = append(parts, "Untracked files:")
        for _, file := range untracked {
            parts = append(parts, fmt.Sprintf("?? %s", file))
        }
    }

    return strings.Join(parts, "\n"), nil
}
```

---

## **Appendix D: runWatch Implementation**

```go
package watch

import (
    "context"
    "fmt"
    "time"

    "github.com/dyluth/holt/pkg/blackboard"
)

// runWatch polls for claim creation for a given artefact ID.
// Prints progress indicator and returns when claim is found or timeout occurs.
func runWatch(ctx context.Context, client *blackboard.Client, artefactID string, timeout time.Duration) error {
    fmt.Printf("⏳ Waiting for orchestrator to create claim...\n")

    ticker := time.NewTicker(200 * time.Millisecond)
    defer ticker.Stop()

    timeoutCh := time.After(timeout)

    for {
        select {
        case <-ctx.Done():
            return ctx.Err()

        case <-timeoutCh:
            return fmt.Errorf(`timeout waiting for claim

No claim created after %v seconds.

Possible causes:
  - Orchestrator container not running
  - Orchestrator not subscribed to artefact_events
  - Redis Pub/Sub issue

Check orchestrator status:
  docker ps | grep orchestrator
  docker logs holt-orchestrator-<instance>

Check artefact was created:
  holt hoard`, int(timeout.Seconds()))

        case <-ticker.C:
            claim, err := client.GetClaimByArtefactID(ctx, artefactID)
            if err != nil {
                if blackboard.IsNotFound(err) {
                    // Not found yet, continue polling
                    continue
                }
                return fmt.Errorf("failed to query for claim: %w", err)
            }

            // Success!
            fmt.Printf("✓ Claim created: %s (status: %s)\n", claim.ID, claim.Status)
            return nil
        }
    }
}
```

---

**End of Design Document**
