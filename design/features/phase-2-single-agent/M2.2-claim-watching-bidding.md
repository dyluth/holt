# **Feature design: M2.2 - Claim Watching & Bidding**

**Purpose**: Implement claim event subscription, bidding logic, consensus model, and agent container launching
**Scope**: Agent cub, orchestrator, and CLI enhancements for claim-bid-grant cycle
**Estimated tokens**: ~6,000 tokens

Associated phase: **Single Agent (Phase 2)**
Status: **Draft**

***Template purpose:*** *This document is a blueprint for a single, implementable milestone. Its purpose is to provide an unambiguous specification for a developer (human or AI) to build a feature that is consistent with Sett's architecture and guiding principles.*

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Implement the complete claim-bid-grant cycle, enabling the agent cub to discover claims via event subscription, submit bids, receive grant notifications, and enabling the orchestrator to manage consensus and notify winners.

### **1.2. User story**

As a Sett developer, I need agents to automatically discover and bid on work opportunities, and I need the orchestrator to coordinate the bidding process and grant claims to winning agents, so that the system can autonomously execute workflows without manual intervention.

### **1.3. Success criteria**

* A developer can run `sett up` and the CLI successfully launches Redis, orchestrator, AND agent containers as defined in `sett.yml`
* When a GoalDefined artefact is created on the blackboard:
  1. The orchestrator creates a corresponding claim and publishes to `claim_events`
  2. The agent cub receives the claim event, submits an "exclusive" bid via `SetBid()`
  3. The orchestrator detects consensus (all known agents have bid), updates the claim with `GrantedExclusiveAgent` field
  4. The orchestrator publishes a grant notification to the agent's personal event channel
  5. The agent cub receives the grant notification and places the claim on its internal work queue
* All steps above complete within 2 seconds and are fully auditable via Redis inspection
* Agent containers remain running and healthy (health checks passing)

**Validation questions:**
* ✅ Can each success criterion be automated as a test? Yes - full integration test with testcontainers
* ✅ Does each criterion represent user-visible value? Yes - enables autonomous agent coordination
* ✅ Are the criteria specific enough to avoid ambiguity? Yes - step-by-step workflow defined

### **1.4. Non-goals**

* **NOT in scope**: Work execution (M2.3) - work queue receives claims but executor remains stubbed
* **NOT in scope**: Context assembly (M2.4) - no historical graph traversal yet
* **NOT in scope**: Multiple agent types (Phase 3) - Phase 2 tests single agent only
* **NOT in scope**: Review or parallel phases (Phase 3) - only exclusive bidding
* **NOT in scope**: Bid strategy logic (Phase 3) - hardcoded "exclusive" for all claims
* **NOT in scope**: Agent image building (future) - developers manually build images
* **NOT in scope**: Timeout-based consensus (Phase 3) - V1 waits indefinitely
* **NOT in scope**: Controller-worker pattern (Phase 3) - only standard mode (replicas: 1)

## **2. The 'what': component impact analysis**

**Critical validation questions for this entire section:**
* ✅ Have I explicitly considered EVERY component (Blackboard, Orchestrator, Cub, CLI)?
* ✅ For components marked "No changes" - am I absolutely certain this feature doesn't affect them?
* ✅ Do my changes maintain the contracts and interfaces defined in the design documents?
* ✅ Will this feature work correctly with both single-instance and scaled agents (controller-worker pattern)?

### **2.1. Blackboard changes**

**New/modified data structures:** No changes to Artefact or Claim schemas

**New Pub/Sub channels:**

1. **Agent-specific event channel (NEW):**
   - **Pattern:** `sett:{instance}:agent:{agent_name}:events`
   - **Purpose:** Orchestrator publishes grant notifications to individual agents
   - **Message format:** JSON: `{"event_type": "grant", "claim_id": "<uuid>"}`
   - **Publisher:** Orchestrator only
   - **Subscriber:** Agent cub (one subscriber per agent)

**New schema helper functions needed:**

Add to `pkg/blackboard/schema.go`:
```go
// AgentEventsChannel returns the agent-specific event channel name.
// Pattern: sett:{instance}:agent:{agent_name}:events
func AgentEventsChannel(instanceName, agentName string) string {
    return fmt.Sprintf("sett:%s:agent:%s:events", instanceName, agentName)
}
```

**Rationale:** Agent-specific channels enable targeted notifications without polling. Each agent subscribes only to its own channel, avoiding broadcast noise.

### **2.2. Orchestrator changes**

**New/modified logic:**

1. **Configuration loading (NEW):**
   - Load and parse `/workspace/sett.yml` at startup using `config.Load()`
   - Extract agent registry: map of `agent_name → agent_role`
   - Validate at least one agent exists
   - Store agent registry in Engine struct for consensus checks

2. **Consensus polling (NEW):**
   - After creating claim and publishing to `claim_events`, enter consensus wait loop
   - Poll `GetAllBids(ctx, claimID)` every 100ms
   - Wait until `len(receivedBids) == len(knownAgents)`
   - Timeout: None (V1 waits indefinitely - document as limitation)
   - Once consensus achieved, proceed to granting

3. **Claim granting logic (NEW):**
   - Determine winner: first agent with `bid_type == "exclusive"` (Phase 2 has only one agent)
   - Update claim: `claim.GrantedExclusiveAgent = winnerName`
   - Keep status as `pending_exclusive` (no new status needed for M2.2)
   - Call `UpdateClaim(ctx, claim)` to persist changes
   - Publish grant notification to winner's channel: `AgentEventsChannel(instanceName, winnerName)`

4. **Grant notification format:**
   ```json
   {
     "event_type": "grant",
     "claim_id": "550e8400-e29b-41d4-a716-446655440000"
   }
   ```

**New/modified configurations (sett.yml):**

Add `image` field to Agent definition:
```yaml
agents:
  go-coder:
    image: sett-agent-go-coder:latest  # NEW: explicit image name
    role: coder
    command: ["/app/run.sh"]
    workspace:
      mode: ro
```

**Impact on existing orchestrator:**
- `processArtefact()` method extended with consensus + granting logic after claim creation
- No changes to artefact subscription or claim creation logic (M1.5 remains intact)

### **2.3. Agent cub changes**

**New/modified logic:**

1. **Dual-subscription architecture in `claimWatcher` goroutine:**
   - Subscribe to TWO Pub/Sub channels concurrently:
     * `claim_events` → triggers bidding
     * `agent:{agent_name}:events` → receives grant notifications
   - Single select loop handles both event streams

2. **Claim bidding workflow:**
   ```
   claim event received → fetch claim details → submit "exclusive" bid → log bid submission
   ```
   - No conditional logic: always bid "exclusive" for ALL claims (Phase 2 simplification)
   - Use `bbClient.SetBid(ctx, claimID, agentName, blackboard.BidTypeExclusive)`

3. **Grant detection workflow:**
   ```
   grant event received → parse claim_id → fetch full claim → validate granted to this agent → push to work queue
   ```
   - Parse JSON message to extract `claim_id`
   - Fetch claim via `GetClaim(ctx, claimID)`
   - Verify `claim.GrantedExclusiveAgent == config.AgentName` (safety check)
   - Push claim onto `workQueue` channel for work executor

4. **Configuration enhancement:**
   - Add `AgentRole` field to `internal/cub/config.go` (from `SETT_AGENT_ROLE` env var)
   - Required for audit trails in future milestones

**Changes to the tool execution contract (stdin/stdout):** None - work executor remains stubbed in M2.2

**New files:**
- `internal/cub/watcher.go` - Extracted bidding logic from engine.go (optional refactoring)

### **2.4. CLI changes**

**New/modified commands:**

Enhance `sett up` to launch agent containers:

1. **Agent container lifecycle:**
   - After starting orchestrator, iterate through `cfg.Agents` map
   - For each agent, launch container with proper configuration
   - Container naming: `sett-{instance}-agent-{agent_name}`
   - Label with instance metadata (same pattern as orchestrator/Redis)

2. **Agent container configuration:**
   - **Image:** Use `agent.Image` field from sett.yml (fail if image not found locally)
   - **Network:** Attach to instance network (`sett-{instance}`)
   - **Environment variables:**
     * `SETT_INSTANCE_NAME={instanceName}`
     * `SETT_AGENT_NAME={agentName}` (key from agents map)
     * `SETT_AGENT_ROLE={agent.Role}`
     * `REDIS_URL=redis://{redisContainerName}:6379`
   - **Workspace mount:** `{workspacePath}:/workspace:{mode}`
     * Mode: `agent.Workspace.Mode` (default "ro")
   - **Entrypoint:** Use image's default entrypoint (cub binary)

3. **Image validation:**
   - Before launching, verify image exists locally via `ImageList()`
   - If not found, fail with clear error:
     ```
     Error: Agent image 'sett-agent-go-coder:latest' not found

     Build the image first:
       cd agents/go-coder
       docker build -t sett-agent-go-coder:latest .

     Then retry: sett up
     ```

**Changes to user output:**

Update success message to include agent containers:
```
Instance 'default-1' started successfully

Containers:
  • sett-default-1-redis (running)
  • sett-default-1-orchestrator (running)
  • sett-default-1-agent-go-coder (running)  ← NEW

Network:
  • sett-default-1

Workspace: /home/user/my-project

Next steps:
  1. Run 'sett forage --goal "your goal"' to start a workflow
  2. Run 'sett logs go-coder' to view agent logs
  3. Run 'sett down' when finished
```

**Modified commands:**
- `sett down` - Already handles agent cleanup via instance labels (no changes needed)
- `sett list` - Already shows agent containers via labels (no changes needed)

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

**Key design decisions:**

1. **Dual-subscription pattern in single goroutine:**
   - **Decision:** Use one goroutine with two subscriptions in a single select loop
   - **Rationale:** Simpler than two goroutines, avoids synchronization complexity
   - **Alternative rejected:** Separate goroutines for each subscription (more complex state management)

2. **Event-driven grant detection (no polling):**
   - **Decision:** Agent blocks on `agent:{name}:events` channel for grant notifications
   - **Rationale:** Eliminates polling overhead, instant notification delivery via Redis Pub/Sub
   - **Alternative rejected:** Poll claim status every 500ms (wasteful, higher latency)

3. **Orchestrator polls for consensus:**
   - **Decision:** Orchestrator polls `GetAllBids()` every 100ms until all agents bid
   - **Rationale:** Simpler than event-driven consensus, acceptable for Phase 2 (single agent)
   - **Future enhancement:** Event-driven consensus in Phase 3 (subscribe to bid events)

4. **Explicit image field in sett.yml:**
   - **Decision:** Add `image: string` to Agent config (not auto-derived)
   - **Rationale:** Flexibility for versioned tags, private registries, custom naming
   - **Alternative rejected:** Auto-derive as `sett-agent-{name}:latest` (too rigid)

5. **Indefinite consensus wait (V1):**
   - **Decision:** Orchestrator waits forever for all agent bids
   - **Rationale:** Deterministic, debuggable workflows (YAGNI - timeout in Phase 3)
   - **Risk:** Dead agent blocks system (mitigated by operational monitoring)

**Potential risks:**

* **Risk:** Pub/Sub message loss (Redis at-most-once delivery)
  * **Mitigation:** Phase 2 assumption - reliable network, no message loss expected. Document limitation.
  * **Future fix:** Add claim status polling as fallback (Phase 3)

* **Risk:** Agent subscribes to events AFTER orchestrator publishes grant
  * **Mitigation:** Agent subscribes to both channels before entering main loop (setup phase)
  * **Testing:** Integration test with fast claim creation

* **Risk:** Multiple agents with same name (config error)
  * **Mitigation:** Config validation ensures unique agent names in sett.yml

* **Risk:** Agent image not found, partial startup
  * **Mitigation:** Validate ALL agent images exist before starting ANY containers (fail-fast)

**Alignment with Sett's architectural principles:**
* **Event-driven architecture:** ✅ Uses Pub/Sub for all coordination (no polling in cub)
* **Auditability:** ✅ All bids and grants recorded in Redis (complete trail)
* **YAGNI:** ✅ Hardcoded "exclusive" bidding, no complex strategy logic
* **Fail-fast:** ✅ Image validation before container launch

### **3.2. Implementation steps**

**Phase 1: Blackboard schema extension**

1. **[Blackboard]** Add `AgentEventsChannel()` helper to `pkg/blackboard/schema.go`
2. **[Blackboard]** Add unit tests for channel name generation
3. **[Blackboard]** Verify no breaking changes to existing schema functions

**Phase 2: Configuration enhancement**

4. **[Config]** Add `Image string` field to `internal/config/config.go` Agent struct
5. **[Config]** Update `Agent.Validate()` to require non-empty `image` field
6. **[Config]** Add unit test for image field validation
7. **[Config]** Update example/test sett.yml files with `image` field

**Phase 3: Agent cub enhancements**

8. **[Cub]** Add `AgentRole` field to `internal/cub/config.go` Config struct
9. **[Cub]** Update `LoadConfig()` to read `SETT_AGENT_ROLE` env var
10. **[Cub]** Update `claimWatcher()` in `engine.go`:
    - Subscribe to `claim_events` channel
    - Subscribe to `agent:{name}:events` channel
    - Implement dual-subscription select loop
    - Add claim event handler → bidding logic
    - Add grant event handler → work queue handoff
11. **[Cub]** Add helper function: `submitBid(ctx, claim)` - always bid "exclusive"
12. **[Cub]** Add helper function: `handleGrantNotification(ctx, message)` - parse JSON, fetch claim, push to queue
13. **[Cub]** Update `engine.go` to create both subscriptions in `Start()` before launching goroutines
14. **[Cub]** Add unit tests for bidding logic (mock blackboard client)
15. **[Cub]** Add integration test: claim event → bid submission → grant reception

**Phase 4: Orchestrator enhancements**

16. **[Orchestrator]** Update `NewEngine()` to accept `*config.SettConfig` parameter
17. **[Orchestrator]** Add `agentRegistry map[string]string` field to Engine struct (name → role)
18. **[Orchestrator]** Load sett.yml at startup, build agent registry
19. **[Orchestrator]** Update `processArtefact()` to add consensus + granting after claim creation:
    - Create claim (existing logic)
    - Enter consensus wait loop: poll `GetAllBids()` every 100ms
    - Once all agents bid, execute granting logic
20. **[Orchestrator]** Implement `grantClaim(ctx, claim, bids)` method:
    - Determine winner (first exclusive bidder)
    - Update claim with `GrantedExclusiveAgent`
    - Persist via `UpdateClaim()`
    - Publish grant notification to agent's channel
21. **[Orchestrator]** Add unit tests for consensus logic (mock client)
22. **[Orchestrator]** Add unit tests for granting logic
23. **[Orchestrator]** Add integration test: claim → bids → grant with real Redis

**Phase 5: CLI enhancements**

24. **[CLI]** Update `cmd/orchestrator/main.go` to load sett.yml and pass config to engine
25. **[CLI]** Update `cmd/sett/commands/up.go`:
    - Add `validateAgentImages()` function - verify all images exist
    - Add `launchAgentContainers()` function after orchestrator startup
    - Implement agent container creation logic with proper env vars
    - Update success message to list agent containers
26. **[CLI]** Add helper: `internal/docker/agent.go` - agent container naming, labels
27. **[CLI]** Add integration test: `sett up` with agent in sett.yml → verify container running

**Phase 6: Example agent**

28. **[Agent]** Create `agents/example-echo/Dockerfile`:
    - FROM golang:1.24 AS builder
    - Copy cub source, build binary
    - FROM alpine:latest
    - COPY cub binary
    - ENTRYPOINT ["/app/cub"]
29. **[Agent]** Create `agents/example-echo/README.md` with build instructions
30. **[Docs]** Update main README with M2.2 status

**Phase 7: Integration testing**

31. **[Test]** Create `internal/cub/integration_bidding_test.go`:
    - Start Redis via testcontainers
    - Start cub process
    - Manually create claim and publish to `claim_events`
    - Assert cub submits bid
    - Manually publish grant notification
    - Assert cub receives grant and logs it
32. **[Test]** Create `cmd/sett/commands/up_integration_test.go`:
    - Use testcontainers for Redis
    - Mock sett.yml with one agent
    - Pre-build simple test agent image
    - Run `sett up` logic
    - Assert Redis, orchestrator, AND agent containers running
33. **[Test]** Create E2E test in `cmd/sett/commands/e2e_bidding_test.go`:
    - Full workflow: `sett up` → create artefact → assert claim created → assert bid submitted → assert grant received
    - Verify complete audit trail in Redis

### **3.3. Performance & resource considerations**

**Resource usage:**
* **Memory:** Agent cub +5MB for two subscriptions (buffered channels)
* **CPU:** Orchestrator polling adds ~0.1% CPU (100ms interval)
* **Network:** Two persistent Pub/Sub connections per agent (claim_events + agent:events)
* **Redis:** Bid storage: HSET per agent per claim (~100 bytes)

**Scalability limits:**
* **Single agent only:** Phase 2 tests with `replicas: 1` only
* **Consensus polling:** Acceptable for 1-5 agents, needs optimization for 10+ agents (Phase 3)

**Performance requirements:**
* **Claim-to-bid latency:** <100ms (event-driven, no polling)
* **Consensus detection:** <1 second (100ms polling with single agent)
* **Grant notification delivery:** <50ms (Pub/Sub delivery time)
* **End-to-end (claim → bid → grant):** <2 seconds total

### **3.4. Testing strategy**

**Unit tests:**

Location: `internal/cub/*_test.go`, `internal/orchestrator/*_test.go`

1. **Cub configuration (`config_test.go`):**
   - Test `LoadConfig()` with `SETT_AGENT_ROLE` present and missing
   - Test validation with new role field

2. **Cub bidding logic (`watcher_test.go` - if extracted):**
   - Test bid submission with mock blackboard client
   - Test grant notification parsing (valid JSON, invalid JSON)
   - Test claim validation (granted to correct agent, granted to different agent)

3. **Orchestrator consensus (`consensus_test.go`):**
   - Test agent registry loading from config
   - Test consensus detection (all agents bid, partial bids)
   - Test granting logic (single exclusive bid, multiple bids, no exclusive bids)

4. **Orchestrator grant notification (`granting_test.go`):**
   - Test grant message formatting
   - Test channel name generation
   - Test publish success/failure handling

**Integration tests:**

Location: `internal/cub/*_integration_test.go`, `cmd/sett/commands/*_integration_test.go`

1. **Cub bidding cycle (with real Redis):**
   - Start Redis via testcontainers
   - Start cub process as subprocess
   - Create claim, publish to `claim_events`
   - Read bids from Redis, assert exclusive bid present
   - Publish grant notification to agent channel
   - Check cub logs for grant received message

2. **Orchestrator consensus cycle (with real Redis):**
   - Start Redis, load test config with one agent
   - Start orchestrator process
   - Create artefact → assert claim created
   - Manually submit bid via blackboard client
   - Assert orchestrator detects consensus
   - Assert claim updated with `GrantedExclusiveAgent`
   - Assert grant notification published

3. **CLI agent container launching:**
   - Mock sett.yml with test agent
   - Pre-build minimal test agent image (alpine + sleep)
   - Run `createInstance()` logic from up.go
   - Assert agent container created with correct env vars
   - Assert container connected to instance network
   - Assert workspace mounted correctly

**E2E tests:**

Location: `cmd/sett/commands/e2e_test.go`

1. **Full claim-bid-grant workflow:**
   - Use testcontainers for Redis
   - Build example-echo agent image in test setup
   - Create test sett.yml
   - Execute `sett up` (not via CLI, but via internal functions)
   - Create GoalDefined artefact via blackboard client
   - Poll Redis for claim creation (max 5 seconds)
   - Poll Redis for bid submission (max 5 seconds)
   - Poll claim for `GrantedExclusiveAgent` field (max 5 seconds)
   - Check agent cub logs for grant received
   - Verify complete audit trail: artefact → claim → bid → granted claim

**Test coverage target:** 90%+ for new/modified packages

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**New third-party dependencies:**
* None - uses existing Docker SDK, Redis client, testcontainers

**Justification:**
* All features directly support M2.2 success criteria
* Hardcoded "exclusive" bidding avoids premature abstraction
* No bid timeout logic (deferred to Phase 3)
* No fancy consensus algorithms (simple polling sufficient)

### **4.2. Auditability**

**New artefacts created:** None in M2.2 (artefact creation in M2.3)

**State changes captured:**
* Bids recorded in Redis: `sett:{instance}:claim:{id}:bids` hash
* Claim updates with `GrantedExclusiveAgent` field persisted
* Grant notifications published to agent channels (ephemeral, but logged)

**Audit trail completeness:**
* Query Redis to see all bids for any claim
* Query claim to see who was granted access
* Complete timeline reconstructable from Redis data + logs

### **4.3. Small, single-purpose components**

**Component responsibility boundaries:**

* **Agent Cub:** Watches claims, submits bids, receives grants (no execution yet)
* **Orchestrator:** Manages consensus, grants claims, publishes notifications (no artefact creation)
* **CLI:** Launches infrastructure containers (no business logic)

**No responsibility bleed:** Each component has clear, well-defined role.

### **4.4. Security considerations**

**Attack surfaces:**
* **Agent event channels:** Agent could subscribe to another agent's channel
  * **Mitigation:** Agent validates `GrantedExclusiveAgent` matches its own name (defense in depth)
* **Bid tampering:** Agent could submit bids for other agents
  * **Mitigation:** Phase 2 has trusted environment, single developer machine
  * **Future:** Agent authentication in Phase 4

**Data exposure risks:**
* **Agent names in channel names:** Low risk (internal Docker network only)
* **Grant notifications:** Contain claim IDs (public on blackboard anyway)

**Container isolation:**
* Agents run on isolated Docker network (no internet access by default)
* Read-only workspace mount prevents accidental modifications (Phase 2 default)

**Network communications:**
* All Redis traffic on internal Docker network (localhost:port only for CLI)

**Security recommendations for M2.2:**
* Bind Redis to internal network only (already enforced by up.go)
* Agent containers run as non-root (add to Dockerfile best practices)

### **4.5. Backward compatibility**

**API/data structure changes:**
* Added `image` field to Agent config (breaking change for sett.yml)
  * **Migration:** Users must add `image` field when upgrading to M2.2
  * **Validation:** Config loading fails with clear error if missing
* Added `AgentEventsChannel()` schema function (additive, no breaking changes)
* No changes to Artefact or Claim structures (fully compatible)

**Existing workflows preserved:**
* `sett init`, `sett list`, `sett down` - no changes
* Phase 1 tests remain valid (orchestrator still creates claims for artefacts)

**Feature additive:** M2.2 adds agent launching to `sett up` without breaking existing functionality

**Deprecation path:** None needed (new config field required immediately)

### **4.6. Dependency impact**

**Redis usage changes:**
* **New pattern:** Agent-specific Pub/Sub channels (one per agent)
* **Load increase:** Minimal - one additional subscription per agent
* **No new Redis requirements:** Same Redis 7.x version

**Docker/container requirements:**
* **New pattern:** Agent containers launched by CLI
* **Image management:** Developer responsibility (manual build)
* **No new Docker API calls:** Uses existing ContainerCreate/Start

**Go version:**
* **No change:** Uses same Go 1.24 as existing codebase
* **No new language features:** Standard goroutines, channels, select

**Build dependencies:**
* **No new tools:** Uses existing Makefile, go build, docker build

**Development environment impact:**
* **Agent image building:** Developers must manually build agent images before `sett up`
* **Build order:** 1) Build cub binary, 2) Build agent images, 3) Run `sett up`

**CI/CD pipeline impact:**
* **Test agents:** Tests must build minimal agent images in setup phase
* **Integration tests:** Require Docker and testcontainers (already in use)

## **5. Definition of done**

*This checklist must be fully satisfied for the milestone to be considered complete.*

* [ ] All implementation steps from section 3.2 are complete
* [ ] All tests defined in section 3.4 are implemented and passing
* [ ] Performance requirements from section 3.3 are met and verified:
  * [ ] Claim-to-bid latency <100ms
  * [ ] Consensus detection <1 second
  * [ ] End-to-end claim→bid→grant <2 seconds
* [ ] Overall test coverage has not decreased (project-wide)
* [ ] New/modified packages have 90%+ test coverage
* [ ] `sett up` successfully launches Redis, orchestrator, AND agent containers
* [ ] Agent cub receives claim events and submits bids to Redis
* [ ] Orchestrator detects consensus and grants claims
* [ ] Agent cub receives grant notifications via agent-specific channel
* [ ] Complete E2E test passes: artefact → claim → bid → grant → agent notified
* [ ] Example agent Dockerfile created and documented
* [ ] All TODOs from the specification documents relevant to this milestone have been resolved
* [ ] All failure modes identified in section 6.1 have been implemented and tested
* [ ] Concurrency considerations from section 6.2 have been addressed
* [ ] All open questions from section 7 have been resolved or documented as future work
* [ ] AI agent implementation guidance has been followed and integration checklist completed
* [ ] Security considerations from section 4.4 have been addressed and validated
* [ ] Backward compatibility impact from section 4.5 is documented (breaking: `image` field required)
* [ ] Dependency impact analysis from section 4.6 has been completed and approved
* [ ] Operational readiness checklist from section 9 is fully satisfied

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Configuration failures (fail-fast at startup):**

1. **Missing agent image field in sett.yml:**
   - **Scenario:** Agent config has no `image` field
   - **Expected behavior:** `config.Load()` fails with error: "agent 'go-coder': image is required"
   - **Exit code:** Non-zero (1)
   - **Testing:** Unit test in `config_test.go`

2. **Agent image not found locally:**
   - **Scenario:** sett.yml references `image: my-agent:latest` but image doesn't exist
   - **Expected behavior:** `sett up` fails before creating any containers with error:
     ```
     Error: Agent image 'my-agent:latest' not found

     Build the image first:
       docker build -t my-agent:latest ./agents/my-agent

     Then retry: sett up
     ```
   - **Exit code:** Non-zero (1)
   - **Testing:** Integration test in `up_integration_test.go`

3. **Invalid agent name (conflicts with Docker naming):**
   - **Scenario:** Agent name contains invalid characters (e.g., `agent_with_underscores`)
   - **Expected behavior:** Docker container creation fails with naming error
   - **Mitigation:** Add agent name validation to `config.Agent.Validate()` (alphanumeric + hyphens only)
   - **Testing:** Unit test in `config_test.go`

**Runtime failures (cub operation):**

4. **Claim event subscription fails:**
   - **Scenario:** Redis unavailable when cub subscribes to `claim_events`
   - **Expected behavior:** Cub fails to start, exits with error
   - **Retry logic:** None - fail fast (same behavior as M2.1)
   - **Testing:** Integration test with Redis stopped

5. **Agent event subscription fails:**
   - **Scenario:** Redis unavailable when subscribing to `agent:{name}:events`
   - **Expected behavior:** Cub fails to start, exits with error
   - **Retry logic:** None - fail fast
   - **Testing:** Integration test with Redis stopped

6. **Bid submission fails:**
   - **Scenario:** Redis connection lost during `SetBid()` call
   - **Expected behavior:** Bid submission returns error, log error message, continue watching (don't crash)
   - **Impact:** Agent won't be granted claim (orchestrator won't see bid)
   - **Testing:** Integration test with Redis restart during bid

7. **Grant notification received for wrong claim:**
   - **Scenario:** Grant message contains unknown claim ID
   - **Expected behavior:** Log warning, continue watching (ignore invalid message)
   - **Testing:** Unit test with mock invalid message

8. **Grant notification received but claim not granted to this agent:**
   - **Scenario:** Grant message valid but `claim.GrantedExclusiveAgent != agentName`
   - **Expected behavior:** Log warning, do NOT push to work queue, continue watching
   - **Security:** Prevents agent from stealing work
   - **Testing:** Integration test with mismatched grant

**Runtime failures (orchestrator operation):**

9. **sett.yml not found in mounted workspace:**
   - **Scenario:** Orchestrator container starts but `/workspace/sett.yml` doesn't exist
   - **Expected behavior:** Orchestrator fails to start with error: "Failed to load sett.yml: file not found"
   - **Exit code:** Non-zero (1)
   - **Prevention:** CLI validates sett.yml exists before launching orchestrator
   - **Testing:** Integration test with missing sett.yml

10. **No agents defined in sett.yml:**
    - **Scenario:** sett.yml has empty `agents` map
    - **Expected behavior:** Config validation fails: "no agents defined"
    - **Exit code:** Non-zero (1)
    - **Testing:** Unit test in `config_test.go`

11. **Consensus never achieved (no bids received):**
    - **Scenario:** Agent crashes before bidding
    - **Expected behavior:** Orchestrator waits indefinitely (V1 limitation)
    - **Logging:** Log warning every 10 seconds: "Still waiting for bids from: [agent-name]"
    - **Mitigation:** Operational monitoring, documented limitation
    - **Testing:** Manual test with killed agent (document as known limitation)

12. **Grant notification publish fails:**
    - **Scenario:** Redis connection lost during grant notification publish
    - **Expected behavior:** Log error, claim remains granted (persisted to Redis)
    - **Impact:** Agent never receives notification (V1 limitation - no retry)
    - **Future fix:** Add fallback polling in Phase 3
    - **Testing:** Integration test with Redis disconnect

### **6.2. Concurrency considerations**

**Race conditions:**

1. **Dual subscription in single goroutine:**
   - **Scenario:** Both channels receive messages simultaneously
   - **Mitigation:** Go select is inherently race-free (non-deterministic choice OK)
   - **Testing:** Run tests with `-race` flag

2. **Work queue access:**
   - **Scenario:** Claim Watcher writes to queue while Work Executor reads
   - **Mitigation:** Go channels are thread-safe (no explicit locking needed)
   - **Testing:** Run tests with `-race` flag

3. **Orchestrator concurrent artefact processing:**
   - **Scenario:** Multiple artefacts arrive simultaneously
   - **Mitigation:** Each artefact processed sequentially in main loop (no parallelism in M2.2)
   - **Future:** Parallel claim processing in Phase 3
   - **Testing:** Integration test with rapid artefact creation

**Deadlock scenarios:**

4. **Work queue full during shutdown:**
   - **Scenario:** Claim Watcher tries to push to queue after Work Executor exits
   - **Mitigation:** Claim Watcher select loop includes `ctx.Done()` case to abort send
   - **Testing:** Integration test with shutdown during grant handling

5. **Subscription cleanup on shutdown:**
   - **Scenario:** Subscriptions not closed, goroutine hangs
   - **Mitigation:** Both subscriptions closed via defer in `Start()` method
   - **Testing:** Integration test verifies clean shutdown

**Message ordering:**

6. **Claim event arrives before agent subscribes:**
   - **Scenario:** Orchestrator publishes claim event before agent cub fully starts
   - **Mitigation:** Agent subscribes to both channels BEFORE entering select loop (setup phase)
   - **Impact:** If message lost, agent never bids (V1 limitation - no replay)
   - **Future fix:** Add claim backlog fetch on startup (Phase 3)
   - **Testing:** Integration test with fast claim creation

7. **Grant notification arrives before bid submitted:**
   - **Scenario:** Network delay causes grant notification to overtake bid in flight
   - **Mitigation:** Impossible - orchestrator only grants AFTER receiving bid in Redis
   - **Redis guarantees:** HSET + PUBLISH are sequential operations

### **6.3. Edge case handling**

**Empty or minimal inputs:**

1. **Claim with empty source_artefacts:**
   - **Scenario:** GoalDefined artefact (first artefact) has no sources
   - **Expected behavior:** Agent bids normally (doesn't check sources in M2.2)
   - **Testing:** Standard E2E test covers this (GoalDefined is first artefact)

2. **Single agent in registry:**
   - **Scenario:** sett.yml defines only one agent (Phase 2 standard)
   - **Expected behavior:** Consensus achieved immediately after single bid
   - **Testing:** All M2.2 tests use single agent

**Maximum scale scenarios:**

3. **Rapid claim creation (100+ artefacts):**
   - **Scenario:** Orchestrator processes 100 artefacts in 10 seconds
   - **Expected behavior:** All claims created, all bids submitted (may be slow)
   - **Performance degradation:** Consensus polling bottleneck (100ms per claim minimum)
   - **Phase 2 scope:** Not optimized for high throughput
   - **Testing:** Performance test in M2.5

4. **Long-running consensus wait:**
   - **Scenario:** Agent takes 60 seconds to submit bid
   - **Expected behavior:** Orchestrator waits entire time (no timeout)
   - **Logging:** Warning logs every 10 seconds
   - **Testing:** Manual test with slow agent (document as expected behavior)

**Network partitions or timeouts:**

5. **Redis Pub/Sub message loss:**
   - **Scenario:** Network blip causes claim event to be dropped
   - **Expected behavior:** Agent never receives event, never bids, orchestrator waits forever
   - **Mitigation:** None in V1 (document as limitation, requires reliable network)
   - **Future fix:** Add claim polling fallback in Phase 3
   - **Testing:** Not testable in Phase 2 (requires network fault injection)

6. **Agent container network disconnected:**
   - **Scenario:** Docker network failure, agent can't reach Redis
   - **Expected behavior:** Agent health check fails, Redis operations error
   - **Recovery:** None - agent must be restarted (operational concern)
   - **Testing:** Not in scope for M2.2 (requires network fault injection)

**Configuration edge cases:**

7. **Agent with empty command array:**
   - **Scenario:** sett.yml has `command: []`
   - **Expected behavior:** Config validation fails: "agent 'x': command is required"
   - **Testing:** Unit test in `config_test.go` (already exists)

8. **Agent role same as agent name:**
   - **Scenario:** `agent-name: coder` and `role: coder`
   - **Expected behavior:** Allowed (no conflict, different namespaces)
   - **Testing:** Integration test with matching name/role

## **7. Open questions & decisions**

**Q1: Consensus polling interval**
- **Question:** 100ms polling interval acceptable for Phase 2?
- **Recommendation:** Yes - with single agent, 100ms adds max 100ms latency (acceptable)
- **Status:** ✅ Decided - use 100ms

**Q2: Grant notification retry logic**
- **Question:** Should orchestrator retry if grant notification publish fails?
- **Recommendation:** No - V1 assumes reliable network, retry in Phase 3
- **Rationale:** Simplifies implementation, document as limitation
- **Status:** ✅ Decided - no retry in M2.2

**Q3: Claim status after grant**
- **Question:** Should we add new status like `pending_exclusive_execution`?
- **Recommendation:** No - keep status as `pending_exclusive`, rely on `GrantedExclusiveAgent` field
- **Rationale:** Fewer status transitions, simpler state machine
- **Status:** ✅ Decided - no new status

**Q4: Agent image auto-build**
- **Question:** Should `sett up` auto-build agent images from `build.context`?
- **Recommendation:** No - defer to future milestone (YAGNI)
- **Rationale:** Manual build gives developer full control, simpler CLI logic
- **Status:** ✅ Decided - manual build only in M2.2

**Q5: Multiple claims granted simultaneously**
- **Question:** Can agent receive multiple grants while executing first claim?
- **Answer:** Yes - Phase 2 allows this (work queue has buffer size 1)
- **Handling:** If queue full, grant notification blocks briefly (acceptable)
- **Future:** Phase 3 may add queuing strategy
- **Status:** ✅ Decided - allow concurrent grants, queue may block

**Q6: Agent startup health check**
- **Question:** Should CLI wait for agent health check before considering `sett up` complete?
- **Recommendation:** No - start container and return immediately (async startup)
- **Rationale:** Matches orchestrator startup behavior, user can check logs
- **Status:** ✅ Decided - async startup, no health wait

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**Start with the simplest path that satisfies success criteria:**

1. **Blackboard schema first:** Add `AgentEventsChannel()` helper, test it, commit.

2. **Config enhancement next:** Add `image` field, validate, test, commit. This unblocks both orchestrator and CLI work.

3. **Agent cub bidding (isolated):** Implement dual-subscription select loop in `claimWatcher()` with placeholder handlers first. Then add bidding logic, then grant handling. Test each piece with mocks before integration.

4. **Orchestrator consensus (isolated):** Implement consensus polling in separate function, test with mocks, then integrate into `processArtefact()`.

5. **Orchestrator granting (isolated):** Implement granting logic separately, test with mocks, then integrate after consensus.

6. **CLI agent launching last:** After cub and orchestrator work independently, add agent container launching to `sett up`.

7. **Integration testing:** Wire everything together in E2E test only after unit/integration tests pass.

**Implement comprehensive error handling from the beginning:**

* Every subscription creation must handle errors
* Every bid submission must handle Redis failures (log and continue)
* Every grant notification parse must validate JSON schema
* Every config load must validate required fields

**Write tests before implementation (TDD approach):**

* Write unit test for `AgentEventsChannel()` before implementing
* Write test for config validation with `image` field before adding it
* Write test for consensus detection before implementing loop

**Use defensive programming - validate all inputs and assumptions:**

* Validate grant messages have correct JSON schema before parsing
* Validate `GrantedExclusiveAgent` matches agent name before pushing to queue
* Validate agent images exist before launching containers

### **8.2. Common pitfalls to avoid**

**Forgetting to subscribe to agent channel:**
* **Pitfall:** Only subscribing to `claim_events`, missing `agent:{name}:events`
* **Result:** Agent never receives grant notifications
* **Solution:** Create both subscriptions in `Start()` before launching goroutines

**Race condition in subscription setup:**
* **Pitfall:** Publishing grant notification before agent subscribes to channel
* **Result:** Grant message lost
* **Solution:** Agent subscribes BEFORE orchestrator is started (enforced by container startup order)

**Blocking on work queue:**
* **Pitfall:** Work queue send blocks if executor slow to consume
* **Solution:** Use buffered channel (size 1), include `ctx.Done()` in send select

**Polling too aggressively:**
* **Pitfall:** 10ms polling interval wastes CPU
* **Solution:** Use 100ms interval (acceptable latency, minimal CPU)

**Not validating grant messages:**
* **Pitfall:** Trusting all messages on agent channel
* **Result:** Agent processes invalid work
* **Solution:** Validate `GrantedExclusiveAgent` field matches before pushing to queue

**Forgetting config migration:**
* **Pitfall:** Not documenting breaking change (new `image` field required)
* **Result:** Users' sett.yml files invalid after upgrade
* **Solution:** Clear error message, migration guide in release notes

**Image validation timing:**
* **Pitfall:** Validating images AFTER starting some containers
* **Result:** Partial instance on error
* **Solution:** Validate ALL images before starting ANY containers (fail-fast)

### **8.3. Integration checklist**

**Pre-implementation verification:**

* [ ] All prerequisite features are complete:
  * [ ] M1.1: Redis Blackboard Foundation (✅ completed)
  * [ ] M1.2: Blackboard Client Operations (✅ completed)
  * [ ] M1.5: Orchestrator Claim Engine (✅ completed)
  * [ ] M2.1: Agent Cub Foundation (✅ completed)
* [ ] No breaking changes to existing contracts:
  * [ ] Artefact and Claim schemas unchanged
  * [ ] Existing Pub/Sub channels unchanged (only adding new agent channel)
  * [ ] Blackboard client API unchanged
* [ ] New data structures are backward compatible:
  * [ ] Config `image` field is NEW (breaking, but intentional)
  * [ ] `AgentEventsChannel` is new helper (additive)
* [ ] All component interfaces remain stable:
  * [ ] Blackboard client interface unchanged
  * [ ] Cub config structure extended (additive: `AgentRole`)

**Post-implementation verification:**

* [ ] All Phase 1 tests still pass (no regressions)
* [ ] All M2.1 tests still pass (cub foundation intact)
* [ ] New tests pass with `-race` flag (no data races)
* [ ] Agent containers launched successfully by `sett up`
* [ ] Bidding cycle works: claim event → bid submission → grant notification → queue
* [ ] Complete E2E test passes (artefact → claim → bid → grant)
* [ ] Health checks pass for all containers (Redis, orchestrator, agent)
* [ ] No goroutine leaks detected in integration tests

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Metrics to track:**
* **Agent cub:**
  * Claims received count
  * Bids submitted count
  * Grants received count
  * Bid submission errors count
* **Orchestrator:**
  * Consensus wait time (histogram)
  * Claims granted count
  * Grant notification errors count
  * Agent registry size (static)

**Log messages:**

**Agent cub:**
* `[INFO] Claim Watcher subscribed to claim_events and agent:go-coder:events`
* `[INFO] Received claim event: claim_id=<uuid> artefact_id=<uuid>`
* `[INFO] Submitted exclusive bid for claim_id=<uuid>`
* `[ERROR] Failed to submit bid for claim_id=<uuid>: <error>`
* `[INFO] Received grant notification: claim_id=<uuid>`
* `[WARN] Grant notification for claim not granted to this agent: claim_id=<uuid>`

**Orchestrator:**
* `[INFO] Loaded agent registry: 1 agents`
* `[INFO] Waiting for consensus on claim_id=<uuid>`
* `[WARN] Still waiting for bids from: [go-coder] (waited 10s)`
* `[INFO] Consensus achieved for claim_id=<uuid>: received 1/1 bids`
* `[INFO] Granted claim_id=<uuid> to agent 'go-coder'`
* `[ERROR] Failed to publish grant notification: <error>`

**Structured logging events (if using structured logging):**
* Not required in M2.2 (standard `log` package sufficient)
* Can be added in Phase 3 if needed

**Health check modifications:**
* Cub health check unchanged (Redis PING only)
* Orchestrator health check unchanged
* Agent health check uses cub's `/healthz` endpoint (already implemented in M2.1)

**Issue detection and diagnosis:**
* **Agent not bidding:** Check cub logs for claim event reception
* **Consensus never achieved:** Check orchestrator logs for "waiting for bids" warnings
* **Grant not received:** Check orchestrator logs for grant notification publish, check agent logs for grant reception

### **9.2. Rollback and disaster recovery**

**Feature disable via configuration:**
* Not applicable - M2.2 is core functionality (no feature flag)

**Rollback procedure if feature causes issues:**
1. Run `sett down` to stop all containers
2. Revert to M2.1 git commit
3. Rebuild cub binary: `make build-cub`
4. Rebuild orchestrator image: `make docker-orchestrator`
5. Update sett.yml to remove `image` field from agents
6. Run `sett up` (will start orchestrator only, no agents)

**Data migration or cleanup requirements:**
* None - M2.2 doesn't modify blackboard data structures
* Bids stored in Redis are ephemeral (deleted when instance stops)

**Recovery time objective:**
* **RTO:** <5 minutes (stop containers, rebuild, restart)
* **RPO:** N/A (no persistent state modified)

### **9.3. Documentation and training**

**CLI command documentation:**
* Update `sett up --help` to mention agent container launching
* No new CLI commands in M2.2

**Feature coverage in user guides:**
* Add section to README: "Agent Container Management"
* Document agent image build workflow
* Document sett.yml `image` field requirement

**API documentation:**
* **GoDoc comments:** All new functions in cub and orchestrator must have clear documentation
* **Config field docs:** Document `image` field in config.Agent struct

**Troubleshooting guides:**

**Problem:** `sett up` fails with "Agent image 'x' not found"
* **Solution:** Build agent image first: `docker build -t x ./agents/...`

**Problem:** Agent container starts but never bids
* **Solution:**
  1. Check agent logs: `sett logs agent-name`
  2. Verify Redis connectivity from agent container
  3. Verify claim events being published by orchestrator

**Problem:** Orchestrator waits forever for consensus
* **Solution:**
  1. Check orchestrator logs for "waiting for bids" warnings
  2. Verify agent container is running: `docker ps | grep sett-{instance}-agent`
  3. Check agent logs for errors

**Problem:** Grant notification not received by agent
* **Solution:**
  1. Check orchestrator logs for grant notification publish errors
  2. Verify agent subscribed to correct channel (check cub logs on startup)
  3. Check Redis Pub/Sub connections: `redis-cli CLIENT LIST`

**Team training requirements:**
* Brief team on agent image build workflow before M2.2 rollout
* Demo full claim-bid-grant cycle in team meeting
* Document debugging workflow for bidding issues

## **10. Self-validation checklist**

### **Before starting implementation:**

* [ ] I understand how this feature aligns with the current phase (Phase 2: Single Agent)
* [ ] All success criteria (section 1.3) are measurable and testable
* [ ] I have considered every component in section 2 explicitly
* [ ] All design decisions (section 3.1) are justified and documented
* [ ] I understand the dual-subscription pattern in claimWatcher
* [ ] I understand the consensus polling mechanism in orchestrator
* [ ] I understand the grant notification flow

### **During implementation:**

* [ ] I am implementing the simplest solution that meets success criteria
* [ ] All error scenarios (section 6) are being handled, not just happy path
* [ ] Tests are being written before or alongside code (TDD approach)
* [ ] I am validating that existing functionality is not broken (run Phase 1 tests)
* [ ] All select loops include `case <-ctx.Done()` for graceful shutdown
* [ ] Both subscriptions created before entering select loop (prevent race)
* [ ] Agent image validation happens before launching any containers (fail-fast)
* [ ] Grant notification validation before pushing to work queue (security)

### **Before submission:**

* [ ] All items in Definition of Done (section 5) are complete
* [ ] Feature has been tested in a clean environment from scratch
* [ ] All tests pass with `-race` flag (no race conditions)
* [ ] Documentation is updated and accurate (GoDoc comments, README)
* [ ] I have considered the operational impact (section 9) of this feature
* [ ] E2E test passes: artefact → claim → bid → grant → agent notified
* [ ] Agent containers launch successfully via `sett up`
* [ ] Complete audit trail visible in Redis (bids, granted claims)
* [ ] No goroutine leaks detected (process exits cleanly)
* [ ] Config breaking change documented (migration guide for `image` field)
