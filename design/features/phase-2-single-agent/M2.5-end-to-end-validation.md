# **Feature design: M2.5 - End-to-End Validation**

**Purpose**: Complete Phase 2 with comprehensive E2E testing, failure validation, and production-ready documentation
**Scope**: E2E test suite, failure scenarios, performance validation, documentation updates, Phase 2 completion
**Estimated tokens**: ~10,000 tokens

Associated phase: **Single Agent (Phase 2)**
Status: **Draft**

***Template purpose:*** *This document is a blueprint for a single, implementable milestone. Its purpose is to provide an unambiguous specification for a developer (human or AI) to build a feature that is consistent with Holt's architecture and guiding principles.*

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Complete Phase 2 with comprehensive end-to-end testing that validates the full single-agent workflow in production-like conditions, validates critical failure scenarios, and delivers production-ready documentation for end users and agent developers.

### **1.2. User story**

As a Holt developer preparing to release Phase 2, I need a comprehensive E2E test suite that validates the complete workflow from `holt forage` through agent execution to CodeCommit artefact creation, proves that critical failure scenarios are handled correctly with appropriate error messages and Failure artefacts, and provides clear documentation so new users can get started and agent developers can build their own agents with confidence.

### **1.3. Success criteria**

**End-to-End Happy Path:**
* Test validates complete workflow: `holt forage --goal "create hello.txt"` → orchestrator creates claim → git agent bids → git agent wins grant → agent executes → creates file in workspace → commits to Git → returns commit hash → CodeCommit artefact created on blackboard
* Test verifies audit trail: GoalDefined artefact exists with correct payload → CodeCommit artefact exists with source_artefacts chain → both artefacts retrievable from blackboard
* Test validates Git commit: commit hash from artefact payload exists in repository → committed file exists with expected content → workspace remains clean after execution
* Test runs in complete isolation using t.TempDir() with fresh Git repo and holt.yml

**Failure Scenario Validation (Critical Guardrails):**
* **Dirty Git Workspace Test**: Creates uncommitted changes → executes `holt up` → asserts command fails with clear error message → no containers launched
* **Agent Script Failure Test**: Agent tool exits with non-zero code → Failure artefact created on blackboard with error details → claim status is "terminated" → orchestrator does not create new claims for Failure artefacts
* **Invalid Tool Output Test**: Agent tool outputs malformed JSON → Failure artefact created with JSON parsing error → claim terminated

**Performance Validation (Separate Tests):**
* **Startup Performance Test**: Measures `holt up` duration → logs measured time → fails if > 10 seconds
* **Claim-to-Execution Latency Test**: Measures time from claim creation to agent execution start → logs measured time → fails if > 2 seconds
* **Context Assembly Performance Test**: Creates 10-level artefact graph → measures context assembly duration → logs measured time → fails if > 1 second
* **Git Commit Performance Test**: Measures agent's git commit operation → logs measured time → fails if > 5 seconds

**Example Git Agent:**
* New `agents/example-git-agent` created with dedicated Dockerfile and run.sh
* Agent reads target artefact payload (e.g., "hello.txt")
* Agent creates file with that name in workspace
* Agent commits file with descriptive message including claim ID
* Agent outputs CodeCommit JSON with commit hash
* Agent README documents the Git workflow pattern

**Documentation Deliverables:**
* **README.md (Hybrid Structure)**: Project summary → Quick Start (clone to running workflow in <10 commands) → Core Concepts overview → Links to detailed docs
* **docs/agent-development.md (Primary Focus)**: Tool contract specification → stdin/stdout JSON schemas → Git workflow patterns → context_chain usage → example implementations → derivative artefact patterns
* **docs/troubleshooting.md (Moderate Detail)**: Common error messages → likely causes → solutions → essential debug commands (holt logs, docker ps, holt hoard)

**Regression Testing:**
* All Phase 1 tests continue to pass (no regressions)
* Test coverage maintained or improved (90%+ for all packages)

**Validation questions:**
* ✅ Can each success criterion be automated as a test? Yes - all criteria are testable with Go test framework
* ✅ Does each criterion represent user-visible value? Yes - working system + clear docs = usable Phase 2
* ✅ Are the criteria specific enough to avoid ambiguity? Yes - exact workflows, thresholds, and deliverables defined

### **1.4. Non-goals**

* **NOT in scope**: Multi-agent coordination testing (Phase 3)
* **NOT in scope**: Controller-worker pattern testing (Phase 3)
* **NOT in scope**: Review or parallel claim phases (Phase 3)
* **NOT in scope**: Question/Answer artefact testing (Phase 4)
* **NOT in scope**: Human interaction commands (`holt questions`, `holt answer`) - Phase 4
* **NOT in scope**: Performance optimization of test suite execution time (5-10 minutes is acceptable)
* **NOT in scope**: Load testing or stress testing
* **NOT in scope**: Shared container pools or test execution parallelization
* **NOT in scope**: Comprehensive agent failure recovery mechanisms (enhanced in Phase 3)
* **NOT in scope**: Advanced Git branching strategies (Phase 3 concern)
* **NOT in scope**: LLM-based agent examples (simple shell scripts sufficient for M2.5)
* **NOT in scope**: Container crash detection beyond basic failure scenarios
* **NOT in scope**: Deep architectural documentation for contributors (tertiary priority)

## **2. The 'what': component impact analysis**

**Critical validation questions for this entire section:**
* ✅ Have I explicitly considered EVERY component (Blackboard, Orchestrator, Pup, CLI)?
* ✅ For components marked "No changes" - am I absolutely certain this feature doesn't affect them?
* ✅ Do my changes maintain the contracts and interfaces defined in the design documents?
* ✅ Will this feature work correctly with both single-instance and scaled agents (controller-worker pattern)?

### **2.1. Blackboard changes**

**New/modified data structures:** No changes

**New Pub/Sub channels:** No changes

**Rationale:** M2.5 is a validation milestone. All blackboard functionality required for Phase 2 has been implemented in M1.1-M1.2. This milestone validates existing blackboard operations work correctly in E2E scenarios.

### **2.2. Orchestrator changes**

**New/modified logic:** No changes to orchestrator code

**New/modified configurations (holt.yml):** No changes to schema

**Rationale:** M2.5 validates existing orchestrator functionality (claim creation, consensus, granting) works correctly. All orchestrator logic for Phase 2 single-agent workflow was implemented in M1.5 and M2.2.

### **2.3. Agent pup changes**

**New/modified logic:** No changes to pup code

**Changes to tool execution contract:** No changes

**Rationale:** M2.5 validates existing pup functionality (claim watching, bidding, context assembly, tool execution, git validation) works correctly. All pup logic for Phase 2 was implemented in M2.1-M2.4.

### **2.4. CLI changes**

**New/modified commands:** No changes to CLI code

**Changes to user output:** No changes

**Rationale:** M2.5 validates existing CLI commands (`init`, `up`, `down`, `forage`, `watch`, `hoard`, `logs`, `list`) work correctly in E2E workflows. All Phase 2 CLI functionality was implemented in M1.3-M1.4, M1.6.

### **2.5. Test Infrastructure Changes (NEW COMPONENT FOR M2.5)**

**New test files:**
* `cmd/holt/commands/e2e_phase2_test.go` - Complete Phase 2 E2E test suite
* `cmd/holt/commands/e2e_performance_test.go` - Performance validation tests
* `cmd/holt/commands/e2e_failures_test.go` - Failure scenario tests

**Test isolation infrastructure:**
* Each test function uses `t.TempDir()` for isolated temporary directory
* Test setup: creates Git repo, holt.yml, agent scripts programmatically
* Test execution: all CLI commands run with working directory set to temp dir
* Test cleanup: automatic via t.TempDir() deferred cleanup

**Test utilities (if needed):**
* Helper functions for common E2E setup tasks (create test holt.yml, initialize Git repo, wait for containers)
* Helper functions for blackboard state assertions (verify artefact exists, verify claim status)
* Helper functions for Git repository assertions (verify commit exists, verify file content)

### **2.6. Example Agent Changes**

**New agent: `agents/example-git-agent`**

**Purpose:** Canonical reference implementation for code-generating agents that produce CodeCommit artefacts.

**Structure:**
```
agents/example-git-agent/
├── Dockerfile          # Multi-stage build: Go pup + git tools
├── run.sh             # Tool script: read stdin → create file → git commit → output JSON
└── README.md          # Git workflow documentation
```

**Tool behavior:**
1. Read stdin JSON (target_artefact + context_chain)
2. Extract filename from target_artefact.payload (e.g., "hello.txt")
3. Create file in /workspace with simple content
4. Execute: `git add <filename>`
5. Execute: `git commit -m "[holt-agent: git-agent] Created <filename>\n\nClaim-ID: <claim-id>"`
6. Extract commit hash: `git rev-parse HEAD`
7. Output JSON: `{"artefact_type": "CodeCommit", "artefact_payload": "<commit-hash>", "summary": "Created <filename> and committed"}`

**Existing agent: `agents/example-echo-agent`**

**Changes:** No code changes

**Rationale:** Keep as minimal "hello world" example for basic tool contract understanding.

### **2.7. Documentation Changes**

**New files:**
* `README.md` - Root project README (hybrid structure)
* `docs/agent-development.md` - Primary guide for building agents
* `docs/troubleshooting.md` - Moderate-detail troubleshooting guide

**Content requirements detailed in section 3.2.**

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

**Decision 1: Complete Test Isolation with t.TempDir()**

Each E2E test function operates in its own isolated temporary directory created by `t.TempDir()`. This ensures:
- No pollution of project workspace
- Tests are hermetic and repeatable
- Automatic cleanup on test completion
- Parallel test execution safety (future optimization)

**Risk:** Increased test execution time (5-10 minutes for full suite)
**Mitigation:** Explicitly accepted as worthwhile trade-off for correctness and isolation

**Decision 2: Separate Example Agents (echo vs git)**

Create dedicated `example-git-agent` rather than enhancing existing echo agent.

**Rationale:** Aligns with "small, single-purpose components" principle. Provides maximum clarity for new agent developers learning different patterns.

**Risk:** Slight increase in maintenance burden
**Mitigation:** Both agents are simple shell scripts, minimal maintenance expected

**Decision 3: Separate Performance Test Functions**

Performance tests are isolated test functions (e.g., `TestPerformance_Startup`) rather than integrated into happy-path tests.

**Rationale:** Clear separation of concerns - functional failures vs performance regressions. Makes test failures unambiguous and easier to diagnose.

**Risk:** Longer total test suite execution time
**Mitigation:** Explicitly accepted as worthwhile trade-off for clarity

**Decision 4: Prioritize Guardrail Testing Over Exhaustive Coverage**

Focus on three critical failure scenarios (dirty Git, script failure, invalid JSON) rather than attempting to test every possible failure mode.

**Rationale:** These three scenarios validate the most critical safety and auditability features. YAGNI principle applies - comprehensive failure testing can expand in Phase 3.

**Risk:** Some edge case failures might not be caught
**Mitigation:** Phase 2 is single-agent only with limited complexity. Phase 3 will expand failure handling.

**Decision 5: Documentation Prioritizes Agent Developers**

Primary focus on `docs/agent-development.md` over deep architectural docs.

**Rationale:** Phase 2 success requires others to be able to build agents. Architectural docs are lower value until Phase 3+ when system complexity increases.

**Risk:** Contributors might struggle to understand internals
**Mitigation:** Code is well-commented, design documents exist, can expand docs in Phase 3+

### **3.2. Implementation steps**

**Step 1: Create Example Git Agent**

* [ ] Create `agents/example-git-agent/` directory structure
* [ ] Write `Dockerfile` (multi-stage: build pup + install git)
* [ ] Write `run.sh` tool script with Git workflow
* [ ] Write `README.md` documenting Git agent pattern
* [ ] Manually test agent in development environment

**Step 2: Implement E2E Happy Path Test**

* [ ] Create `cmd/holt/commands/e2e_phase2_test.go`
* [ ] Implement test setup: `t.TempDir()`, Git repo init, holt.yml creation
* [ ] Implement test step 1: `holt up` launches orchestrator + git agent
* [ ] Implement test step 2: `holt forage --goal "hello.txt"` creates GoalDefined artefact
* [ ] Implement test step 3: Wait for and verify CodeCommit artefact created
* [ ] Implement test step 4: Verify Git commit exists with correct file content
* [ ] Implement test step 5: Verify audit trail (artefact chain on blackboard)
* [ ] Implement test cleanup: `holt down` stops containers

**Step 3: Implement Failure Scenario Tests**

* [ ] Create `cmd/holt/commands/e2e_failures_test.go`
* [ ] Implement `TestE2E_DirtyGitWorkspace`: uncommitted changes → `holt up` fails with clear error
* [ ] Implement `TestE2E_AgentScriptFailure`: tool exits non-zero → Failure artefact created
* [ ] Implement `TestE2E_InvalidToolOutput`: tool outputs malformed JSON → Failure artefact created
* [ ] For each test: verify error messages, blackboard state, claim termination

**Step 4: Implement Performance Tests**

* [ ] Create `cmd/holt/commands/e2e_performance_test.go`
* [ ] Implement `TestPerformance_Startup`: measure `holt up` duration, assert < 10s
* [ ] Implement `TestPerformance_ClaimToExecution`: measure claim → execution latency, assert < 2s
* [ ] Implement `TestPerformance_ContextAssembly`: create 10-level graph, measure assembly, assert < 1s
* [ ] Implement `TestPerformance_GitCommit`: measure git commit operation, assert < 5s
* [ ] Each test logs measured time for visibility

**Step 5: Verify Phase 1 Regression Tests**

* [ ] Run full Phase 1 test suite: `make test-integration`
* [ ] Verify all tests pass (no regressions)
* [ ] Verify test coverage maintained or improved
* [ ] Fix any regressions discovered

**Step 6: Write Agent Development Guide**

* [ ] Create `docs/` directory
* [ ] Create `docs/agent-development.md`
* [ ] Document tool contract: stdin JSON schema with examples
* [ ] Document tool contract: stdout JSON schema with examples
* [ ] Document context_chain structure and usage patterns
* [ ] Document Git workflow for CodeCommit agents
* [ ] Document derivative artefact patterns (new logical_id, version=1)
* [ ] Include complete example-echo-agent walkthrough
* [ ] Include complete example-git-agent walkthrough
* [ ] Document recommended commit message format

**Step 7: Write Troubleshooting Guide**

* [ ] Create `docs/troubleshooting.md`
* [ ] Section 1: "Holt won't start" - Redis connection issues, port conflicts
* [ ] Section 2: "Agent won't execute" - container crashes, missing environment variables
* [ ] Section 3: "Git workspace errors" - dirty workspace, wrong directory
* [ ] Section 4: "Blackboard state issues" - artefact not found, claim stuck
* [ ] For each issue: exact error message → cause → solution → debug commands

**Step 8: Update Main README**

* [ ] Restructure `README.md` with hybrid approach
* [ ] Section 1: Project Summary (what is Holt? 2-3 paragraphs)
* [ ] Section 2: Quick Start (clone → build → run example-git-agent workflow)
* [ ] Section 3: Core Concepts (Blackboard, Orchestrator, Pup, Claims, Artefacts)
* [ ] Section 4: Further Reading (links to agent-development.md, troubleshooting.md, design docs)
* [ ] Add Phase 2 completion status badge/note

**Step 9: Final Phase 2 Validation**

* [ ] Run complete test suite: `make test && make test-integration`
* [ ] Verify all Definition of Done items (section 5) are satisfied
* [ ] Verify Phase 2 success criteria from MILESTONES.md are met
* [ ] Manual smoke test: fresh clone → `holt init` → `holt up` → `holt forage` → verify CodeCommit
* [ ] Review all documentation for accuracy and completeness

### **3.3. Performance & resource considerations**

**Resource usage:**

**E2E Test Suite:**
- Each isolated test spins up 2-3 Docker containers (Redis, orchestrator, agent)
- Temporary directories: ~10MB per test (Git repos + workspace)
- Total disk usage during test run: ~100MB
- Memory: ~500MB peak (multiple containers running)
- Network: Docker bridge networks per test instance

**Performance requirements:**

**Test Execution Time:**
- Full E2E suite: 5-10 minutes (explicitly accepted)
- Individual E2E test: 30-60 seconds (container startup + workflow execution)
- Performance tests: 20-30 seconds each (measure specific operations)
- Failure tests: 10-20 seconds each (shorter - fail fast scenarios)

**Component Performance Thresholds (from MILESTONES.md):**
- Pup startup: < 1 second (implicit - not separately tested)
- `holt up` startup: < 10 seconds (explicit test)
- Claim-to-execution latency: < 2 seconds (explicit test)
- Context assembly (10-level graph): < 1 second (explicit test)
- Git commit operation: < 5 seconds (explicit test)

**Scalability limits:**
- Phase 2 is single-agent only - scalability testing deferred to Phase 3
- Test isolation ensures no interference between concurrent test runs (future parallel execution)

### **3.4. Testing strategy**

**Unit tests:**

M2.5 is primarily a validation milestone - no new production code to unit test. Existing unit tests from M2.1-M2.4 provide coverage for:
- Pup configuration parsing (`internal/pup/config_test.go`)
- Context assembly algorithm (`internal/pup/context_test.go`)
- Git validation logic (`internal/pup/git_test.go`)
- Tool contract serialization (`internal/pup/contract_test.go`)

**Integration tests:**

Existing integration tests from M2.1-M2.4 provide coverage for:
- Blackboard client operations with real Redis
- Orchestrator claim engine with real blackboard
- Pup engine lifecycle with real containers

**E2E tests (PRIMARY FOCUS OF M2.5):**

**File: `cmd/holt/commands/e2e_phase2_test.go`**

```go
// TestE2E_Phase2_HappyPath validates the complete single-agent workflow:
// forage → claim → bid → execute → CodeCommit artefact
func TestE2E_Phase2_HappyPath(t *testing.T) {
    // Setup: t.TempDir(), Git init, holt.yml creation
    // Step 1: holt up (verify containers running)
    // Step 2: holt forage --goal "hello.txt"
    // Step 3: Wait for CodeCommit artefact (poll blackboard)
    // Step 4: Verify Git commit exists
    // Step 5: Verify file content
    // Step 6: Verify audit trail (GoalDefined → CodeCommit)
    // Cleanup: holt down
}
```

**File: `cmd/holt/commands/e2e_failures_test.go`**

```go
// TestE2E_DirtyGitWorkspace verifies holt up fails with dirty workspace
func TestE2E_DirtyGitWorkspace(t *testing.T) {
    // Setup: t.TempDir(), Git repo, uncommitted file
    // Execute: holt up
    // Assert: command fails with clear error message
    // Assert: no containers launched
}

// TestE2E_AgentScriptFailure verifies Failure artefact on non-zero exit
func TestE2E_AgentScriptFailure(t *testing.T) {
    // Setup: t.TempDir(), agent with script that exits non-zero
    // Execute: holt up + holt forage
    // Assert: Failure artefact created
    // Assert: claim status = terminated
    // Assert: error details in Failure artefact payload
}

// TestE2E_InvalidToolOutput verifies Failure artefact on bad JSON
func TestE2E_InvalidToolOutput(t *testing.T) {
    // Setup: t.TempDir(), agent with script that outputs malformed JSON
    // Execute: holt up + holt forage
    // Assert: Failure artefact created
    // Assert: JSON parse error in payload
}
```

**File: `cmd/holt/commands/e2e_performance_test.go`**

```go
// TestPerformance_Startup measures holt up duration
func TestPerformance_Startup(t *testing.T) {
    // Setup: t.TempDir()
    // Measure: time.Now() → holt up → time.Since()
    // Log: measured duration
    // Assert: duration < 10 seconds
}

// TestPerformance_ClaimToExecution measures latency
func TestPerformance_ClaimToExecution(t *testing.T) {
    // Setup: running instance
    // Measure: timestamp before forage → poll for execution start → calculate delta
    // Log: measured latency
    // Assert: latency < 2 seconds
}

// TestPerformance_ContextAssembly measures graph traversal
func TestPerformance_ContextAssembly(t *testing.T) {
    // Setup: create 10-level artefact chain on blackboard
    // Measure: time for pup to assemble context
    // Log: measured duration
    // Assert: duration < 1 second
}

// TestPerformance_GitCommit measures git operation time
func TestPerformance_GitCommit(t *testing.T) {
    // Setup: running git agent
    // Measure: time from execution start to commit completion
    // Log: measured duration
    // Assert: duration < 5 seconds
}
```

**Test execution:**

```bash
# Run all tests (unit + integration + E2E)
make test-all

# Run only E2E tests
go test -v -tags=integration ./cmd/holt/commands/e2e_*.go

# Run specific E2E test
go test -v -tags=integration ./cmd/holt/commands -run TestE2E_Phase2_HappyPath
```

**Test isolation guarantee:**

Every E2E test follows this pattern:

```go
func TestE2E_Something(t *testing.T) {
    // Automatic isolated directory
    tmpDir := t.TempDir() // Cleaned up automatically

    // Initialize Git repo in temp dir
    exec.Command("git", "init", tmpDir).Run()

    // Create holt.yml in temp dir
    os.WriteFile(filepath.Join(tmpDir, "holt.yml"), []byte(config), 0644)

    // Change to temp dir for all operations
    originalDir, _ := os.Getwd()
    defer os.Chdir(originalDir)
    os.Chdir(tmpDir)

    // Run test operations (all isolated to tmpDir)
    // ...

    // Cleanup handled by t.TempDir() deferred function
}
```

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**New dependencies:** None

M2.5 introduces no new third-party dependencies. It uses existing test infrastructure:
- `testing` (Go standard library)
- `testcontainers-go` (already used in M1.x integration tests)
- `github.com/stretchr/testify/require` (already used in existing tests)

**Functionality justification:**

All new code is test code validating existing functionality. No new production features are added. This aligns perfectly with YAGNI - M2.5 validates that what we've built works, rather than adding speculative features.

### **4.2. Auditability**

**Artefacts created:** No new artefact types

M2.5 tests validate that existing artefacts maintain the immutable audit trail:
- GoalDefined artefacts created by CLI
- CodeCommit artefacts created by git agents
- Failure artefacts created by pup on errors

**Audit trail validation:**

E2E tests explicitly verify:
- Artefact chains via `source_artefacts` field
- Immutability (artefacts never modified, only new versions created)
- Complete provenance (every artefact traces back to initial GoalDefined)

### **4.3. Small, single-purpose components**

**Component responsibilities:**

M2.5 maintains clear separation:
- **Test code**: Validates existing functionality, does not implement new features
- **Example agents**: Single-purpose (echo-agent = minimal demo, git-agent = Git workflow demo)
- **Documentation**: Each doc file has single purpose (agent dev guide, troubleshooting, README overview)

**No tight coupling introduced:**

Tests interact with components via public interfaces (CLI commands, blackboard client) rather than reaching into internals. This ensures tests validate the actual user experience and component contracts.

### **4.4. Security considerations**

**No new security implications:**

M2.5 is a validation milestone with no new production code. However, tests do validate existing security features:

**Git workspace validation:**

E2E tests verify that `holt up` correctly rejects dirty Git workspaces, preventing accidental data loss or corruption.

**Container isolation:**

Tests validate that agents run in isolated containers with mounted workspaces, maintaining the principle of least privilege.

**No credential exposure:**

Test agents use simple shell scripts with no API keys or credentials. Documentation emphasizes that real agents should use environment variables for secrets (existing pattern from holt.yml).

**Temporary directory cleanup:**

t.TempDir() ensures no sensitive test data persists after test completion.

### **4.5. Backward compatibility**

**No breaking changes:**

M2.5 introduces no changes to production code. All existing APIs, data structures, and workflows remain unchanged.

**Documentation updates:**

README and new docs describe existing functionality - no behavior changes.

**Test compatibility:**

All Phase 1 tests must continue to pass (explicit verification in implementation steps).

**Future compatibility:**

Documentation and example agents follow patterns that will extend naturally to Phase 3 (multi-agent) and Phase 4 (human-in-the-loop).

### **4.6. Dependency impact**

**No new runtime dependencies:**

M2.5 uses existing development dependencies (testcontainers-go, testify) already present in go.mod.

**Go version:** No change (continues to use Go 1.21+)

**Redis version:** No change (continues to use Redis 7-alpine)

**Docker requirements:** No change (continues to require Docker daemon for containers)

**Git requirements:** No change (continues to require Git 2.x+ for workspace management)

**CI/CD impact:**

E2E test suite execution time (5-10 minutes) will increase CI pipeline duration. This is explicitly accepted as worthwhile trade-off for thorough validation.

## **5. Definition of done**

*This checklist must be fully satisfied for the milestone to be considered complete.*

### **Code Implementation**

* [ ] All implementation steps from section 3.2 are complete
* [ ] `agents/example-git-agent/` created with Dockerfile, run.sh, README.md
* [ ] Example git agent tested manually and works correctly
* [ ] `cmd/holt/commands/e2e_phase2_test.go` created with happy path test
* [ ] `cmd/holt/commands/e2e_failures_test.go` created with 3 failure scenario tests
* [ ] `cmd/holt/commands/e2e_performance_test.go` created with 4 performance tests

### **Testing**

* [ ] All E2E tests pass reliably (can run 10 times consecutively without failures)
* [ ] All performance tests pass with measured values logged
* [ ] All failure scenario tests pass and validate correct error handling
* [ ] All Phase 1 tests continue to pass (no regressions)
* [ ] Test coverage maintained or improved (90%+ for all packages)
* [ ] Each E2E test runs in complete isolation (t.TempDir() used)
* [ ] Test suite execution completes in < 15 minutes

### **Documentation**

* [ ] `README.md` restructured with hybrid approach (summary → quick start → concepts → links)
* [ ] `docs/agent-development.md` created with comprehensive agent developer guide
* [ ] `docs/troubleshooting.md` created with moderate-detail troubleshooting guide
* [ ] All new CLI commands/flags documented (none in M2.5)
* [ ] Example git agent README documents Git workflow pattern
* [ ] All documentation reviewed for accuracy and clarity

### **Phase 2 Completion**

* [ ] All Phase 2 success criteria from MILESTONES.md are satisfied:
  * [ ] `holt forage --goal "test"` creates initial artefact
  * [ ] Orchestrator creates corresponding claim
  * [ ] Agent pup bids on claim and wins
  * [ ] Agent executes work and creates Git commit artefact
  * [ ] Full audit trail visible on blackboard (GoalDefined → CodeCommit)
  * [ ] Git workspace integration functional (clean repo validation, commit workflow)

### **Quality Gates**

* [ ] No panics or crashes in 100 consecutive workflow executions (verified via E2E test loop)
* [ ] All critical failure scenarios produce clear error messages
* [ ] All Failure artefacts contain actionable error details
* [ ] Git workspace remains clean after successful workflow execution
* [ ] Makefile has been updated with new test targets (if needed)
* [ ] Developer onboarding time remains < 10 minutes (verified via fresh clone test)

### **Operational Readiness**

* [ ] All failure modes identified in section 6.1 have been validated
* [ ] All concurrency considerations from section 6.2 remain addressed
* [ ] All open questions from section 7 have been resolved or documented
* [ ] Security considerations from section 4.4 have been validated
* [ ] Performance requirements from section 3.3 are met and verified

### **Final Validation**

* [ ] Manual smoke test: fresh repo clone → build → holt up → holt forage → verify result
* [ ] README quick start successfully followed by independent reviewer
* [ ] Agent development guide successfully followed to create new test agent
* [ ] Troubleshooting guide successfully used to diagnose and fix induced errors

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Failure Mode 1: Dirty Git Workspace**

**Scenario:** User runs `holt up` in repository with uncommitted changes

**Expected behavior:**
1. `holt up` command detects uncommitted changes via `git status --porcelain`
2. Command fails immediately with error: "Git workspace is dirty. Please commit or stash changes before running holt up."
3. No containers are launched
4. User can inspect changes, commit or stash, then retry

**Test validation:** `TestE2E_DirtyGitWorkspace` creates uncommitted file and verifies failure

**Failure Mode 2: Agent Script Non-Zero Exit**

**Scenario:** Agent tool script encounters error and exits with non-zero code

**Expected behavior:**
1. Pup executes tool subprocess with timeout
2. Process exits with code != 0
3. Pup captures stderr output
4. Pup creates Failure artefact with structural_type="Failure"
5. Failure artefact payload contains: exit code, stderr output, error message
6. Pup updates claim status to "terminated"
7. Orchestrator does not create new claim for Failure artefact

**Test validation:** `TestE2E_AgentScriptFailure` uses agent with `exit 1` and verifies Failure artefact

**Failure Mode 3: Agent Script Malformed JSON Output**

**Scenario:** Agent tool outputs invalid JSON to stdout

**Expected behavior:**
1. Pup executes tool subprocess successfully (exit code 0)
2. Pup attempts to parse stdout JSON
3. JSON parsing fails
4. Pup creates Failure artefact with structural_type="Failure"
5. Failure artefact payload contains: "JSON parse error: ...", raw stdout output
6. Pup updates claim status to "terminated"

**Test validation:** `TestE2E_InvalidToolOutput` uses agent that outputs "not json" and verifies Failure artefact

**Failure Mode 4: Git Commit Validation Failure**

**Scenario:** Agent outputs CodeCommit artefact with non-existent commit hash

**Expected behavior:**
1. Pup parses tool output successfully (valid JSON)
2. Pup sees artefact_type="CodeCommit"
3. Pup executes `git cat-file -e <hash>` for validation
4. Git command fails (hash doesn't exist)
5. Pup creates Failure artefact with structural_type="Failure"
6. Failure artefact payload contains: "Git commit validation failed: commit <hash> does not exist"
7. Claim status updated to "terminated"

**Test validation:** Can be added to failure test suite if time permits (lower priority)

**Failure Mode 5: Redis Connection Lost**

**Scenario:** Redis becomes unavailable during workflow execution

**Expected behavior:**
1. Orchestrator loses Redis connection
2. Orchestrator health check endpoint returns 503
3. Orchestrator logs error and continues attempting to reconnect
4. Pup loses Redis connection
5. Pup health check endpoint returns 503
6. Pup logs error, pending operations fail
7. User can inspect logs via `holt logs <component>`
8. User can restart Redis and recover

**Test validation:** Defer to Phase 3 (complex to test reliably, low priority for single-agent)

**Failure Mode 6: Container Crash**

**Scenario:** Agent container crashes unexpectedly (OOM, SIGKILL, etc.)

**Expected behavior:**
1. Docker daemon stops container
2. Orchestrator detects container no longer running (via Docker events or health check failure)
3. Orchestrator logs error
4. Claim remains in granted state (no automatic recovery in Phase 2)
5. User can inspect logs via `holt logs <agent>`
6. User must manually intervene (fix issue, restart via `holt down && holt up`)

**Test validation:** Defer to Phase 3 (complex orchestration recovery logic needed)

### **6.2. Concurrency considerations**

**Consideration 1: Test Isolation**

Each E2E test runs in isolated temporary directory with unique Docker container names. This prevents race conditions when tests eventually run in parallel (future optimization).

**Current state:** Tests run sequentially (no `-parallel` flag). Safe by design.

**Consideration 2: Single-Agent Concurrency**

Phase 2 implements only single-agent workflow (replicas: 1, standard mode). No controller-worker pattern. No concurrent claim processing.

**Current state:** Pup processes one claim at a time via work queue channel (buffer size 1). Safe by design.

**Consideration 3: Blackboard Concurrent Access**

Multiple components (CLI, orchestrator, pup) access Redis concurrently. Redis handles concurrency natively via single-threaded command processing.

**Current state:** No additional locking needed. Artefacts are immutable (write-once). Claims use atomic operations.

**Consideration 4: Git Workspace Concurrent Access**

Phase 2 has single agent with read-write workspace access. No concurrent Git operations possible.

**Current state:** Safe by design. Phase 3 will need Git locking mechanisms for multi-agent coordination.

### **6.3. Edge case handling**

**Edge Case 1: Empty Goal in Forage**

**Scenario:** `holt forage --goal ""`

**Expected behavior:** CLI validation rejects empty goal with error message

**Test coverage:** Defer to CLI enhancement milestone (low priority)

**Edge Case 2: Very Long Context Chain**

**Scenario:** Artefact with 50+ levels of source_artefacts (beyond 10-level depth limit)

**Expected behavior:** Context assembly stops at 10 levels (safety valve), logs warning, continues with truncated context

**Test coverage:** `TestPerformance_ContextAssembly` tests 10-level limit

**Edge Case 3: Large Artefact Payload**

**Scenario:** Agent outputs CodeCommit artefact with 1MB+ payload (e.g., embedded base64 data)

**Expected behavior:** Redis accepts payload (default max string length 512MB). Pup creates artefact successfully.

**Test coverage:** Defer to stress testing (YAGNI for Phase 2)

**Edge Case 4: Agent Timeout**

**Scenario:** Agent tool runs for > 5 minutes (subprocess timeout)

**Expected behavior:** Pup kills subprocess after timeout, creates Failure artefact with "execution timeout" error

**Test coverage:** Defer to Phase 3 (5-minute test would slow suite significantly)

**Edge Case 5: Multiple Instances in Same Workspace**

**Scenario:** User runs `holt up --name instance1`, then `holt up --name instance2` in same directory

**Expected behavior:** Second `holt up` fails with error "workspace already in use by instance1" (workspace locking implemented in M1.4)

**Test coverage:** Covered by Phase 1 tests (workspace collision detection)

**Edge Case 6: Binary Files in Git Commits**

**Scenario:** Agent creates binary file (e.g., image) and commits

**Expected behavior:** Git accepts binary file, commit succeeds, CodeCommit artefact created normally

**Test coverage:** Not tested in M2.5 (example-git-agent uses text files). Safe to defer.

## **7. Open questions & decisions**

**Question 1: Should performance test thresholds be configurable?**

**Context:** Performance tests have hardcoded thresholds (10s, 2s, 1s, 5s). Different hardware may have different performance characteristics.

**Options:**
- A: Keep hardcoded (simpler, consistent CI expectations)
- B: Make configurable via environment variables (flexible but more complex)

**Recommendation:** Keep hardcoded (Option A). Thresholds are generous and should work on any reasonable CI environment. Can revisit in Phase 3 if CI failures occur.

**Decision:** Keep hardcoded ✅

---

**Question 2: Should E2E tests clean up Docker containers on failure?**

**Context:** If test fails, t.TempDir() still cleans up temp directory, but Docker containers might be left running for debugging.

**Options:**
- A: Always cleanup (clean but harder to debug)
- B: Cleanup on success, leave on failure (easier debugging)
- C: Configurable via environment variable

**Recommendation:** Option A (always cleanup) with clear test output showing how to manually reproduce failure. Developers can run tests with `-failfast` to preserve first failure.

**Decision:** Always cleanup ✅

---

**Question 3: Should we create a test helper package?**

**Context:** E2E tests will have common patterns (setup Git repo, create holt.yml, wait for containers, query blackboard).

**Options:**
- A: Inline all helper code in test files (duplication but clear)
- B: Create `internal/testutil` package with helpers (DRY but adds indirection)

**Recommendation:** Option B (create testutil package). Helper functions improve test readability and maintainability. Common patterns: `SetupE2EEnvironment`, `WaitForContainer`, `VerifyArtefactExists`.

**Decision:** Create `internal/testutil` package ✅

---

**Question 4: Should example-git-agent support custom file content?**

**Context:** Current design has agent create file with simple hardcoded content. Could support custom content via artefact payload.

**Options:**
- A: Hardcoded content (simpler, clearer for learning)
- B: Custom content from payload (more realistic but added complexity)

**Recommendation:** Option A (hardcoded). Primary goal is demonstrating Git workflow pattern, not sophisticated content handling. Can enhance in Phase 3.

**Decision:** Hardcoded content ✅

---

**Question 5: Should troubleshooting guide include Docker debugging?**

**Context:** Users might need to debug Docker networking, volume mounts, container logs beyond `holt logs`.

**Options:**
- A: Only document `holt` commands (keeps users in Holt's interface)
- B: Include Docker commands (`docker ps`, `docker inspect`, etc.) for advanced debugging

**Recommendation:** Option B (include Docker commands). Moderate detail requirement means providing escape hatch for advanced users who need lower-level debugging.

**Decision:** Include Docker debugging commands ✅

---

**All open questions resolved. No blockers to implementation.**

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**For implementing M2.5, follow this systematic approach:**

**Phase 1: Foundation (Example Git Agent)**

Start with the most concrete deliverable - the example git agent. This provides a working component to test against.

1. Create `agents/example-git-agent/` directory structure
2. Write minimal `run.sh` that reads stdin JSON and outputs stdout JSON
3. Write `Dockerfile` that builds pup and installs Git
4. Test manually in development environment before automating tests
5. Write comprehensive README documenting the Git workflow pattern

**Phase 2: Happy Path Test (Prove It Works)**

Implement the end-to-end happy path test to validate the complete workflow works.

1. Create `internal/testutil` package with helper functions first (setup, assertions)
2. Implement `TestE2E_Phase2_HappyPath` incrementally:
   - Start with just setup and teardown
   - Add step 1 (holt up) and verify containers running
   - Add step 2 (holt forage) and verify artefact created
   - Add step 3 (wait for CodeCommit) with polling
   - Add step 4 (verify Git commit exists)
   - Add step 5 (verify audit trail)
3. Run test frequently during development - failing tests guide implementation

**Phase 3: Failure Tests (Prove It Fails Safely)**

Implement failure scenario tests to validate guardrails work correctly.

1. Start with `TestE2E_DirtyGitWorkspace` (simplest - just setup failure)
2. Implement `TestE2E_AgentScriptFailure` (requires custom test agent)
3. Implement `TestE2E_InvalidToolOutput` (similar to script failure)
4. For each test, verify not just failure but also error message quality

**Phase 4: Performance Tests (Prove It's Fast Enough)**

Implement performance tests to validate threshold requirements.

1. Start with `TestPerformance_Startup` (measures holt up)
2. Use `time.Now()` and `time.Since()` for measurements
3. Log measured values clearly (helps debug CI failures)
4. Set assertions to fail on threshold violations
5. Run tests multiple times to check for variance

**Phase 5: Documentation (Prove It's Usable)**

Write documentation with new users and agent developers as primary audience.

1. Start with `docs/agent-development.md` (highest priority)
2. Include complete working examples from example-echo-agent and example-git-agent
3. Write `docs/troubleshooting.md` using errors you encountered during test development
4. Restructure `README.md` last (synthesizes all other work)
5. Have someone unfamiliar with Holt review docs for clarity

### **8.2. Common pitfalls to avoid**

**Pitfall 1: Forgetting Test Isolation**

❌ **Wrong:** Running tests in current working directory
```go
func TestE2E(t *testing.T) {
    exec.Command("holt", "up").Run() // Pollutes project directory!
}
```

✅ **Right:** Always use t.TempDir()
```go
func TestE2E(t *testing.T) {
    tmpDir := t.TempDir()
    os.Chdir(tmpDir)
    defer os.Chdir(originalDir)
    exec.Command("holt", "up").Run() // Isolated!
}
```

**Pitfall 2: Not Waiting for Async Operations**

❌ **Wrong:** Assuming immediate completion
```go
runForage(cmd, []string{})
artefact := bbClient.GetArtefact(ctx, id) // May not exist yet!
```

✅ **Right:** Poll with timeout
```go
runForage(cmd, []string{})
var artefact *blackboard.Artefact
for i := 0; i < 30; i++ { // 30 seconds max
    artefact, err = bbClient.GetArtefact(ctx, id)
    if err == nil { break }
    time.Sleep(1 * time.Second)
}
require.NoError(t, err, "Artefact not created within timeout")
```

**Pitfall 3: Hardcoding Container Names**

❌ **Wrong:** Fixed names cause conflicts
```go
instanceName := "test-instance" // Conflicts if tests run in parallel!
```

✅ **Right:** Unique names per test
```go
instanceName := "test-e2e-" + time.Now().Format("20060102-150405-000000")
```

**Pitfall 4: Incomplete Cleanup**

❌ **Wrong:** Only cleaning up on success
```go
err := runTest()
if err == nil {
    runDown(cmd, []string{}) // Only cleans up if no error!
}
```

✅ **Right:** Always cleanup with defer
```go
defer func() {
    downCmd.Flags().Set("name", instanceName)
    runDown(downCmd, []string{}) // Always runs
}()
err := runTest()
```

**Pitfall 5: Vague Assertions**

❌ **Wrong:** No context on failure
```go
require.NoError(t, err) // What operation failed?
```

✅ **Right:** Clear failure messages
```go
require.NoError(t, err, "Failed to start orchestrator container")
```

**Pitfall 6: Over-Complex Test Agents**

❌ **Wrong:** LLM-based agents in tests (slow, flaky, requires API keys)
```go
run.sh:
  curl -X POST anthropic.com/api/v1/messages ... # Too complex!
```

✅ **Right:** Simple shell scripts
```go
run.sh:
  #!/bin/bash
  INPUT=$(cat)
  echo '{"artefact_type":"CodeCommit","artefact_payload":"abc123","summary":"Done"}'
```

**Pitfall 7: Not Testing Error Messages**

❌ **Wrong:** Only checking that error occurred
```go
err := runUp(cmd, []string{})
require.Error(t, err) // Did it fail for the RIGHT reason?
```

✅ **Right:** Verify specific error message
```go
err := runUp(cmd, []string{})
require.Error(t, err)
require.Contains(t, err.Error(), "Git workspace is dirty") // Correct reason!
```

### **8.3. Integration checklist**

**Pre-implementation verification:**

* [ ] All M2.1-M2.4 features are complete and tests passing
* [ ] Example-echo-agent exists and works correctly
* [ ] Pup's git validation logic is implemented (M2.4)
* [ ] Context assembly algorithm is implemented (M2.4)
* [ ] Phase 1 test suite passes completely

**During implementation:**

* [ ] Each E2E test runs in isolated temporary directory (t.TempDir())
* [ ] Each test creates fresh Git repository programmatically
* [ ] Each test creates holt.yml programmatically (no reliance on project files)
* [ ] Each test uses unique instance names to avoid Docker conflicts
* [ ] Each test has proper cleanup (defer statements for holt down)
* [ ] Helper functions in testutil reduce duplication across tests
* [ ] All async operations have timeout-based polling logic
* [ ] All assertions include descriptive error messages

**Post-implementation validation:**

* [ ] All E2E tests pass when run individually
* [ ] All E2E tests pass when run as complete suite
* [ ] All E2E tests can run 10 times consecutively without failure
* [ ] No Docker containers left running after test suite completion
* [ ] No temporary files/directories left after test suite completion
* [ ] All Phase 1 tests still pass (no regressions)
* [ ] Documentation can be followed by someone unfamiliar with Holt

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Test Metrics:**

M2.5 establishes baseline metrics through performance tests:
- Startup time (logged)
- Claim-to-execution latency (logged)
- Context assembly duration (logged)
- Git commit duration (logged)

These logged metrics provide baseline for detecting regressions in future milestones.

**Error Visibility:**

Failure scenario tests validate that errors are observable:
- Failure artefacts contain actionable error details
- Error messages guide users to solutions
- `holt logs` command provides debugging visibility
- Health check endpoints indicate component health

**Future Monitoring (Phase 3+):**

Documentation should note future observability improvements:
- Structured logging with log levels
- Metrics export (Prometheus/StatsD)
- Distributed tracing for multi-agent workflows
- Audit log export for compliance

### **9.2. Rollback and disaster recovery**

**No Rollback Needed:**

M2.5 introduces no production code changes. It validates existing functionality and adds documentation.

**Test Suite as Safety Net:**

The E2E test suite itself serves as rollback protection for future milestones:
- All future changes must pass Phase 2 E2E tests
- Regressions detected immediately
- Clear failure messages guide fixes

**Workspace Recovery:**

Documentation (troubleshooting guide) must include workspace recovery procedures:
- How to safely stop holt if workflow is stuck
- How to clean up Docker containers manually if `holt down` fails
- How to recover from partial Git commits
- When to use `git reset --hard` (data loss warning)

### **9.3. Documentation and training**

**Documentation Deliverables (from section 3.2):**

* [ ] `README.md` - Onboarding for new users
* [ ] `docs/agent-development.md` - Guide for building agents
* [ ] `docs/troubleshooting.md` - Solutions for common problems
* [ ] `agents/example-git-agent/README.md` - Git workflow pattern documentation

**Documentation Quality Standards:**

Each document must:
- Be accurate (reflect actual system behavior)
- Be complete (cover all essential topics for audience)
- Include working examples (code snippets that actually run)
- Be tested (quick start must be followed successfully by reviewer)

**Training Materials:**

For Phase 2 release, documentation serves as self-service training. No separate training materials needed.

**Future Training (Phase 3+):**

Consider video walkthroughs, interactive tutorials, or workshops for broader adoption.

## **10. Self-validation checklist**

### **Before starting implementation:**

* [x] I understand how this feature aligns with Phase 2 completion (validation milestone)
* [x] All success criteria (section 1.3) are measurable and testable
* [x] I have considered every component in section 2 explicitly
* [x] All design decisions (section 3.1) are justified and documented
* [x] I understand the test isolation requirement (t.TempDir())
* [x] I understand the performance threshold requirements
* [x] I understand the documentation priority (agent developers first)

### **During implementation:**

* [ ] I am implementing the simplest solution that meets success criteria
* [ ] All error scenarios (section 6) are being handled, not just happy path
* [ ] Tests are being written before or alongside code (TDD approach)
* [ ] I am validating that existing functionality is not broken (Phase 1 tests)
* [ ] Each E2E test runs in isolated temporary directory
* [ ] Each test has clear, descriptive assertions with error messages
* [ ] Documentation is being written for the actual user experience (not ideal/planned behavior)
* [ ] I am testing documentation by following it step-by-step

### **Before submission:**

* [ ] All items in Definition of Done (section 5) are complete
* [ ] Feature has been tested in a clean environment from scratch (fresh clone)
* [ ] Documentation is updated and accurate (README, agent guide, troubleshooting)
* [ ] I have considered the operational impact (section 9) of this feature
* [ ] All Phase 2 success criteria from MILESTONES.md are satisfied
* [ ] Manual smoke test completed: clone → build → holt up → forage → verify
* [ ] E2E test suite runs reliably (10 consecutive passes)
* [ ] All Phase 1 tests continue to pass (no regressions)
* [ ] Performance tests pass with logged measurements within thresholds
* [ ] Failure tests validate correct error handling and Failure artefacts
* [ ] Documentation reviewed by independent reviewer for clarity
