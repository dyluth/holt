# **Feature design: M2.3 - Work Execution & Tool Contract**

**Purpose**: Implement work executor loop and tool execution contract with stdin/stdout JSON
**Scope**: Agent cub work executor, tool contract, artefact creation, failure handling
**Estimated tokens**: ~7,000 tokens

Associated phase: **Single Agent (Phase 2)**
Status: **Draft**

***Template purpose:*** *This document is a blueprint for a single, implementable milestone. Its purpose is to provide an unambiguous specification for a developer (human or AI) to build a feature that is consistent with Sett's architecture and guiding principles.*

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Implement the work executor goroutine to execute agent tools, establish the stdin/stdout JSON contract, create result artefacts from tool output, and handle failures robustly.

### **1.2. User story**

As a Sett developer, I need the agent cub to execute granted work by invoking the agent's command script, passing context via stdin, receiving results via stdout, and creating new artefacts on the blackboard, so that agents can actually perform work and produce outputs.

### **1.3. Success criteria**

* A developer can create a simple echo agent that receives a claim, reads JSON from stdin, and produces a valid JSON artefact on stdout
* When the orchestrator grants a claim to an agent:
  1. Agent cub's work executor receives the claim from the work queue
  2. Work executor fetches the target artefact from blackboard
  3. Work executor prepares JSON input: `{"claim_type": "exclusive", "target_artefact": {...}, "context_chain": []}`
  4. Work executor executes the agent command (from `sett.yml`) as a subprocess
  5. Work executor passes JSON to subprocess stdin
  6. Work executor reads JSON from subprocess stdout: `{"artefact_type": "...", "artefact_payload": "...", "summary": "..."}`
  7. Work executor creates new artefact with proper provenance (new logical thread)
  8. Work executor publishes artefact to blackboard
  9. New artefact appears on `artefact_events` channel
* Tool execution completes within 5 minutes or is terminated with Failure artefact created
* If tool fails (non-zero exit, invalid JSON), Failure artefact is created with exit code, stdout, stderr
* Work executor continues processing next claim after success or failure (does not crash)
* Full E2E test passes: goal → claim → bid → grant → execute → artefact → orchestrator sees artefact

**Validation questions:**
* ✅ Can each success criterion be automated as a test? Yes - full integration test with real subprocess
* ✅ Does each criterion represent user-visible value? Yes - enables actual agent work execution
* ✅ Are the criteria specific enough to avoid ambiguity? Yes - step-by-step workflow with exact JSON schemas

### **1.4. Non-goals**

* **NOT in scope**: Context assembly / graph traversal (M2.4) - context_chain is hardcoded empty array
* **NOT in scope**: Git operations (M2.4) - echo agent doesn't use Git
* **NOT in scope**: LLM integration - echo agent is deterministic
* **NOT in scope**: Review or parallel claim types (Phase 3) - only exclusive claims
* **NOT in scope**: Question/Answer artefacts (Phase 4) - only Standard and Failure
* **NOT in scope**: Retry logic for failed tools - one execution attempt only
* **NOT in scope**: Streaming output from tool - read all stdout at end
* **NOT in scope**: Tool resource limits beyond timeout - no memory/CPU constraints

## **2. The 'what': component impact analysis**

**Critical validation questions for this entire section:**
* ✅ Have I explicitly considered EVERY component (Blackboard, Orchestrator, Cub, CLI)?
* ✅ For components marked "No changes" - am I absolutely certain this feature doesn't affect them?
* ✅ Do my changes maintain the contracts and interfaces defined in the design documents?
* ✅ Will this feature work correctly with both single-instance and scaled agents (controller-worker pattern)?

### **2.1. Blackboard changes**

**New/modified data structures:** No changes to schemas

**Artefact creation requirements (already supported):**
- `CreateArtefact()` - create new artefact after successful tool execution
- `AddVersionToThread()` - register new artefact in its logical thread

**Rationale:** All required blackboard operations already exist. M2.3 is the first milestone to actually USE these operations to create artefacts from agent work.

### **2.2. Orchestrator changes**

**New/modified logic:** No changes

**Rationale:** Orchestrator already creates claims for artefacts. M2.3 creates new artefacts which will trigger new claims, creating a workflow loop.

### **2.3. Agent cub changes**

**New/modified logic:**

1. **Work Executor implementation (MAJOR CHANGE):**
   - Replace placeholder ticker loop with real work execution
   - Block on `workQueue` channel waiting for granted claims
   - Implement tool execution workflow:
     ```
     receive claim → fetch target artefact → prepare stdin JSON →
     execute tool subprocess → read stdout JSON → create artefact →
     publish to blackboard → loop
     ```

2. **Tool input preparation:**
   - Fetch target artefact from blackboard via `GetArtefact(ctx, claim.ArtefactID)`
   - Marshal tool input JSON with schema:
     ```json
     {
       "claim_type": "exclusive",
       "target_artefact": {<full artefact object>},
       "context_chain": []
     }
     ```
   - M2.3 hardcodes `claim_type: "exclusive"` and `context_chain: []`

3. **Tool subprocess execution:**
   - Use `os/exec.CommandContext` with 5-minute timeout
   - Command: agent's `command` field from `sett.yml` (e.g., `["/app/run.sh"]`)
   - Stdin: Pipe with prepared JSON
   - Stdout: Capture for parsing
   - Stderr: Capture for error reporting
   - Working directory: `/workspace` (mounted Git repo)

4. **Tool output parsing:**
   - Read all stdout bytes
   - Unmarshal JSON with expected schema:
     ```json
     {
       "artefact_type": "EchoSuccess",
       "artefact_payload": "echo-123",
       "summary": "Successfully echoed the claim"
     }
     ```
   - Validate required fields present

5. **Artefact creation (CRITICAL - Derivative Relationship):**
   - Generate new UUID for artefact ID
   - Set `logical_id` = new artefact `id` (creates NEW logical thread)
   - Set `version` = 1 (first version in this new thread)
   - Set `structural_type` = "Standard" (default if not specified by tool)
   - Set `type` = from tool output `artefact_type`
   - Set `payload` = from tool output `artefact_payload`
   - Set `source_artefacts` = `[claim.ArtefactID]` (target artefact)
   - Set `metadata.summary` = from tool output `summary`
   - Call `client.CreateArtefact(ctx, artefact)` - writes to Redis and publishes event
   - Call `client.AddVersionToThread(ctx, logical_id, artefact_id, version)` - register in thread

6. **Failure handling:**
   - Non-zero exit code → create Failure artefact
   - Invalid JSON on stdout → create Failure artefact
   - Tool timeout (5 min) → create Failure artefact
   - Failure artefact structure:
     ```
     structural_type: "Failure"
     type: "ToolExecutionFailure"
     payload: JSON string with {exit_code, stdout, stderr}
     source_artefacts: [claim.ArtefactID]
     logical_id: new UUID (new thread)
     version: 1
     ```
   - Log error details
   - Continue to next work item (DO NOT exit work executor)

**Changes to the tool execution contract (stdin/stdout):**

**STDIN CONTRACT (NEW):**
```json
{
  "claim_type": "exclusive",
  "target_artefact": {
    "id": "uuid",
    "type": "GoalDefined",
    "payload": "Implement user login",
    "structural_type": "Standard",
    "version": 1,
    "logical_id": "uuid",
    "source_artefacts": [],
    "created_at": "2025-10-09T19:00:00Z",
    "metadata": {}
  },
  "context_chain": []
}
```

**STDOUT CONTRACT (NEW):**
```json
{
  "artefact_type": "EchoSuccess",
  "artefact_payload": "claim-abc123",
  "summary": "Successfully processed the claim"
}
```

**Optional fields in STDOUT:**
- `structural_type` - defaults to "Standard" if not provided
- Future: `metadata` object for custom fields

**New files:**
- `internal/cub/executor.go` - Work executor implementation (extracted from engine.go)
- `internal/cub/contract.go` - Tool contract types (ToolInput, ToolOutput)

**Modified files:**
- `internal/cub/engine.go` - Call new executor implementation instead of placeholder

### **2.4. CLI changes**

**New/modified commands:** No changes

**Modified agent:**
- `agents/example-agent/run.sh` - Replace `sleep infinity` with working echo script
- Example implementation:
  ```bash
  #!/bin/sh
  # Read JSON from stdin
  input=$(cat)

  # Extract claim info (optional - just for logging)
  echo "Received claim, processing..." >&2

  # Output success JSON to stdout
  cat <<EOF
  {
    "artefact_type": "EchoSuccess",
    "artefact_payload": "echo-$(date +%s)",
    "summary": "Echo agent successfully processed the claim"
  }
  EOF
  ```

**Rationale:** Echo agent must be functional to test M2.3. Simple shell script reads stdin, logs to stderr, outputs JSON to stdout.

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

**Key design decisions:**

1. **Derivative vs Evolutionary Relationship Model (CRITICAL):**

   This is a **foundational concept** for all Sett agent development. Understanding the difference is essential:

   **Evolutionary Relationship** (NOT used in M2.3):
   - New artefact is a **new version of the same logical work**
   - Same `logical_id` as previous artefact
   - Incremented `version` number
   - Example: Design v1 → Design v2 (same design, evolved)
   - Future use case: Agent improves its own previous output

   **Derivative Relationship** (USED in M2.3):
   - New artefact is a **different work product derived from input**
   - New `logical_id` (creates new logical thread)
   - `version` = 1 (first version of this new work)
   - `source_artefacts` links to input artefact(s)
   - Example: Design → Code (different types of work)
   - M2.3 pattern: GoalDefined → EchoSuccess (echo is derived from goal)

   **Why this matters:**
   - Determines how context assembly works (M2.4)
   - Affects how agents reason about artefact history
   - Critical for audit trails and provenance tracking
   - Ensures blackboard graph represents actual work relationships

   **M2.3 rule:** When an agent executes work on a claim, it produces a **derivative** artefact. The output is a new type of work, not a new version of the input.

2. **5-minute hardcoded timeout:**
   - **Decision:** Use `context.WithTimeout(5 * time.Minute)` for tool execution
   - **Rationale:** Prevents runaway tools, ensures responsive system
   - **Alternative rejected:** No timeout (risk of infinite loops)
   - **Future enhancement:** Configurable per-agent timeout in sett.yml

3. **Single-attempt execution (no retries):**
   - **Decision:** Execute tool once, create Failure artefact if it fails
   - **Rationale:** Simplicity, determinism, avoid retry storms
   - **Alternative rejected:** Auto-retry N times (adds complexity, may mask bugs)
   - **Future enhancement:** Retry logic in orchestrator or separate agent

4. **Subprocess vs in-process execution:**
   - **Decision:** Use `os/exec` to spawn subprocess
   - **Rationale:** Isolation, timeout support, any language/tool
   - **Alternative rejected:** In-process Go plugin (limited to Go, unsafe)

5. **Failure artefact as derivative:**
   - **Decision:** Failure artefacts also use derivative model (new logical_id)
   - **Rationale:** Failure is a result of attempting work on input, not evolution of input
   - **Consistency:** All tool outputs follow same provenance pattern

6. **Empty context_chain in M2.3:**
   - **Decision:** Hardcode `context_chain: []` in stdin JSON
   - **Rationale:** Context assembly is complex, defer to M2.4
   - **Benefit:** Proves tool contract works before adding complexity

**Potential risks:**

* **Risk:** Tool writes partial JSON to stdout before crashing
  * **Mitigation:** Read ALL stdout after process exit, validate complete JSON
  * **Testing:** Test with tool that writes partial output and exits non-zero

* **Risk:** Tool writes multiple JSON objects to stdout
  * **Mitigation:** Document contract: "SINGLE JSON object only", validate with JSON parser
  * **Testing:** Test with tool that outputs multiple objects, verify failure

* **Risk:** Tool hangs reading stdin
  * **Mitigation:** Close stdin pipe after writing JSON
  * **Testing:** Test with tool that reads multiple lines from stdin

* **Risk:** 5-minute timeout is too short/long
  * **Mitigation:** Document as V1 limitation, make configurable in future
  * **Monitoring:** Log execution time for all tools to inform future tuning

* **Risk:** Artefact creation fails after successful tool execution
  * **Mitigation:** Log error, create Failure artefact describing Redis failure
  * **Impact:** Tool output lost (acceptable for V1, consider persistence later)

**Alignment with Sett's architectural principles:**
* **Auditability:** ✅ Complete stdin/stdout captured, artefact provenance tracked
* **YAGNI:** ✅ No retry logic, no streaming, no resource limits beyond timeout
* **Immutability:** ✅ Artefacts are immutable, failures create new artefacts
* **Event-driven:** ✅ New artefacts trigger artefact_events, enabling workflow loops

### **3.2. Implementation steps**

**Phase 1: Tool contract types**

1. **[Cub]** Create `internal/cub/contract.go`:
   - Define `ToolInput` struct with `ClaimType`, `TargetArtefact`, `ContextChain`
   - Define `ToolOutput` struct with `ArtefactType`, `ArtefactPayload`, `Summary`, `StructuralType`
   - Add JSON tags for marshaling/unmarshaling
   - Add validation methods

2. **[Cub]** Add unit tests for contract types:
   - Test JSON marshaling of `ToolInput`
   - Test JSON unmarshaling of `ToolOutput`
   - Test validation (required fields)

**Phase 2: Work executor implementation**

3. **[Cub]** Create `internal/cub/executor.go`:
   - Extract work executor logic from `engine.go` into new file
   - Implement `executeWork(ctx, claim)` method
   - Implement `fetchTargetArtefact(ctx, claim)` helper
   - Implement `prepareToolInput(claim, artefact)` helper - creates ToolInput JSON
   - Implement `executeToolSubprocess(ctx, input)` helper - runs subprocess, returns stdout/stderr
   - Implement `parseToolOutput(stdout)` helper - unmarshals ToolOutput JSON
   - Implement `createResultArtefact(ctx, claim, output)` helper - builds artefact with proper provenance
   - Implement `createFailureArtefact(ctx, claim, exitCode, stdout, stderr)` helper

4. **[Cub]** Implement subprocess execution in `executeToolSubprocess()`:
   - Use `exec.CommandContext()` with 5-minute timeout
   - Get command from `e.config.Command` (passed from sett.yml)
   - Set working directory to `/workspace`
   - Create stdin pipe, write JSON, close pipe
   - Capture stdout and stderr bytes
   - Wait for process, handle timeout
   - Return exit code, stdout, stderr

5. **[Cub]** Implement artefact creation in `createResultArtefact()`:
   - Generate new UUID for artefact ID
   - Set `logical_id = artefact.ID` (NEW logical thread)
   - Set `version = 1`
   - Set `structural_type` from output or default "Standard"
   - Set `type` from output `artefact_type`
   - Set `payload` from output `artefact_payload`
   - Set `source_artefacts = []string{claim.ArtefactID}`
   - Set `metadata.summary` from output `summary`
   - Call `client.CreateArtefact(ctx, artefact)`
   - Call `client.AddVersionToThread(ctx, artefact.LogicalID, artefact.ID, artefact.Version)`

6. **[Cub]** Update `engine.go` work executor loop:
   - Replace ticker/placeholder with call to `executeWork(ctx, claim)`
   - Add error handling (log errors, continue to next work)
   - Remove placeholder ticker and logs

7. **[Cub]** Update `Config` struct to include `Command []string`:
   - Add field to config struct
   - Load from environment variable `SETT_AGENT_COMMAND` (JSON array)
   - Add validation

**Phase 3: Example agent update**

8. **[Agent]** Update `agents/example-agent/run.sh`:
   - Remove `sleep infinity`
   - Add bash script that reads stdin JSON
   - Add logic to parse claim info (optional logging)
   - Add output of valid JSON with `artefact_type: "EchoSuccess"`
   - Make script executable

9. **[Agent]** Update `agents/example-agent/README.md`:
   - Document new echo behavior
   - Show example stdin/stdout
   - Explain derivative relationship

10. **[Config]** Update `sett.example.yml`:
    - Ensure `command` field matches new run.sh

**Phase 4: Unit tests**

11. **[Cub]** Add unit tests for `contract.go`:
    - Test ToolInput JSON marshaling
    - Test ToolOutput JSON unmarshaling with all fields
    - Test ToolOutput JSON unmarshaling with optional fields missing
    - Test validation failures

12. **[Cub]** Add unit tests for `executor.go` helpers:
    - Test `prepareToolInput()` with mock claim/artefact
    - Test `parseToolOutput()` with valid JSON
    - Test `parseToolOutput()` with invalid JSON (should error)
    - Test `createResultArtefact()` with mock output (verify provenance)
    - Test `createFailureArtefact()` with mock error data

**Phase 5: Integration tests**

13. **[Cub]** Create `internal/cub/executor_integration_test.go`:
    - Start Redis via testcontainers
    - Create cub with config pointing to test script
    - Create mock claim and artefact in Redis
    - Push claim to work queue
    - Verify new artefact created in Redis
    - Verify artefact has correct provenance (new logical_id, version 1, source_artefacts)
    - Verify artefact published to artefact_events

14. **[Cub]** Create integration test with failing tool:
    - Test script that exits with non-zero code
    - Verify Failure artefact created
    - Verify Failure artefact payload contains exit code, stdout, stderr
    - Verify work executor continues after failure

15. **[Cub]** Create integration test with timeout:
    - Test script that sleeps for 10 minutes
    - Verify tool killed after 5 minutes
    - Verify Failure artefact created with timeout error

**Phase 6: E2E test**

16. **[Test]** Create `cmd/sett/commands/e2e_work_execution_test.go`:
    - Use testcontainers for Redis
    - Build example-agent image in test setup
    - Create test sett.yml with example-agent
    - Start orchestrator (loads sett.yml)
    - Start example-agent cub
    - Create GoalDefined artefact
    - Wait for orchestrator to create claim
    - Wait for agent to bid
    - Wait for orchestrator to grant
    - Wait for agent to execute work
    - Wait for new EchoSuccess artefact to appear
    - Verify artefact provenance (new logical thread)
    - Verify complete audit trail in Redis

**Phase 7: Documentation**

17. **[Docs]** Update main README with M2.3 status
18. **[Docs]** Add example showing stdin/stdout contract
19. **[Docs]** Document derivative vs evolutionary relationships

### **3.3. Performance & resource considerations**

**Resource usage:**
* **Memory:** Subprocess adds ~10MB per execution (Go runtime + tool overhead)
* **CPU:** Tool execution is main CPU consumer (varies by tool)
* **Disk I/O:** Subprocess stdout/stderr buffered in memory (<10MB typically)
* **Redis:** One artefact creation per work execution (~1KB)

**Scalability limits:**
* **Work queue size:** 1 (can only process one claim at a time)
* **Concurrent agents:** Each agent processes work serially (V1 design)
* **Tool timeout:** 5 minutes per execution (hardcoded)

**Performance requirements:**
* **Tool startup latency:** <1 second (subprocess spawn)
* **JSON marshaling:** <10ms (typical artefact size <1KB)
* **Artefact creation:** <100ms (Redis write + publish)
* **End-to-end (grant → execute → artefact):** <5 minutes + tool execution time

### **3.4. Testing strategy**

**Unit tests:**

Location: `internal/cub/contract_test.go`, `internal/cub/executor_test.go`

1. **Contract types (`contract_test.go`):**
   - Test ToolInput JSON marshaling with all fields
   - Test ToolOutput JSON unmarshaling with required fields only
   - Test ToolOutput JSON unmarshaling with all optional fields
   - Test ToolOutput validation (missing required fields)

2. **Executor helpers (`executor_test.go`):**
   - Test `prepareToolInput()` creates correct JSON structure
   - Test `parseToolOutput()` with valid JSON
   - Test `parseToolOutput()` with invalid JSON returns error
   - Test `parseToolOutput()` with missing required fields returns error
   - Test `createResultArtefact()` sets correct provenance
   - Test `createFailureArtefact()` creates correct structure

**Integration tests:**

Location: `internal/cub/executor_integration_test.go`

1. **Successful tool execution (with real Redis):**
   - Create test script that reads stdin, outputs valid JSON
   - Start Redis, create cub, push claim to queue
   - Verify new artefact created with correct provenance
   - Verify artefact published to artefact_events
   - Verify thread tracking updated

2. **Tool failure - non-zero exit code:**
   - Create test script that exits with code 1
   - Verify Failure artefact created
   - Verify payload contains exit code, stdout, stderr
   - Verify work executor continues

3. **Tool failure - invalid JSON:**
   - Create test script that outputs plain text
   - Verify Failure artefact created
   - Verify payload contains stdout

4. **Tool timeout:**
   - Create test script that sleeps for 10 minutes
   - Verify tool killed after 5 minutes
   - Verify Failure artefact created

5. **Tool success with context_chain:**
   - Verify context_chain is empty array in stdin
   - Verify tool receives correct JSON

**E2E tests:**

Location: `cmd/sett/commands/e2e_work_execution_test.go`

1. **Full workflow with example-agent:**
   - Build example-agent image
   - Start Redis, orchestrator, agent
   - Create GoalDefined artefact
   - Verify claim created
   - Verify bid submitted
   - Verify claim granted
   - Verify work executed
   - Verify EchoSuccess artefact created
   - Verify artefact has proper provenance (new logical_id, version 1)
   - Verify thread tracking correct
   - Verify complete audit trail

**Test coverage target:** 85%+ for new/modified packages (executor.go, contract.go)

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**New third-party dependencies:**
* None - uses standard library `os/exec`, `encoding/json`, `context`

**Justification:**
* No retry logic (deferred until proven necessary)
* No streaming output (read all at end is simpler)
* No resource limits beyond timeout (YAGNI)
* No custom logging formats (structured logs sufficient)

### **4.2. Auditability**

**New artefacts created:**
* Standard artefacts from successful tool execution
* Failure artefacts from failed tool execution

**State changes captured:**
* Tool input (stdin JSON) logged before execution
* Tool output (stdout/stderr) captured and logged
* Exit code recorded
* Artefact creation includes full provenance chain
* Thread tracking updated with `AddVersionToThread()`

**Audit trail completeness:**
* Query Redis to see all artefacts
* Query thread to see version history
* Query artefact to see source_artefacts (input chain)
* Complete timeline reconstructable from Redis data + logs

### **4.3. Small, single-purpose components**

**Component responsibility boundaries:**

* **Work Executor:** Executes granted work, creates result artefacts (clear single purpose)
* **Claim Watcher:** Watches claims, submits bids (unchanged)
* **Orchestrator:** Manages consensus, grants claims (unchanged)

**No responsibility bleed:** Executor only executes work, doesn't bid or manage consensus.

### **4.4. Security considerations**

**Attack surfaces:**
* **Command injection:** Agent command from sett.yml executed as subprocess
  * **Mitigation:** sett.yml is trusted config file (developer-controlled)
  * **Future:** Validate command array format, no shell execution

* **Malicious tool output:** Tool could output huge JSON, malformed data
  * **Mitigation:** Read stdout with size limit (10MB), timeout (5 min)
  * **Validation:** JSON parsing validates structure

**Data exposure risks:**
* **Stdin contains full target artefact:** Potential sensitive data in workspace
  * **Mitigation:** Tool runs in isolated container, read-only workspace (M2.2)
  * **Context:** Artefact data is project work product (expected to be available)

**Container isolation:**
* Tool subprocess runs within agent container (already isolated)
* Read-only workspace mount prevents modification (unless agent has rw mode)

**Security recommendations for M2.3:**
* Document that agent command must be trusted code
* Limit stdout/stderr buffer size to prevent memory exhaustion
* Consider adding command validation in future milestones

### **4.5. Backward compatibility**

**API/data structure changes:**
* Added fields to cub Config: `Command []string` (breaking change for config loading)
  * **Migration:** Command now comes from sett.yml (already required in M2.2)
  * **Impact:** No impact - M2.2 already defines command in sett.yml
* Tool contract is NEW - no backward compatibility concerns

**Existing workflows preserved:**
* M2.1 health checks - no changes
* M2.2 bidding/granting - no changes
* Orchestrator claim creation - no changes

**Feature additive:** M2.3 makes work executor functional without breaking existing functionality

### **4.6. Dependency impact**

**Go standard library usage:**
* `os/exec` - subprocess execution (standard library, well-tested)
* `context` - timeout support (already used throughout codebase)
* `encoding/json` - JSON marshaling (already used)

**No new external dependencies**

**Build dependencies:**
* No new tools required
* Existing Makefile, go build sufficient

**Development environment impact:**
* Developers must update agents/example-agent/run.sh to be executable
* Test suite requires ability to execute shell scripts

**CI/CD pipeline impact:**
* Integration tests now execute subprocesses (requires Linux-compatible environment)
* E2E tests build Docker images (already required for M2.2)

## **5. Definition of done**

*This checklist must be fully satisfied for the milestone to be considered complete.*

* [ ] All implementation steps from section 3.2 are complete
* [ ] All tests defined in section 3.4 are implemented and passing
* [ ] Performance requirements from section 3.3 are met and verified:
  * [ ] Tool startup latency <1 second
  * [ ] JSON marshaling <10ms
  * [ ] Artefact creation <100ms
* [ ] Overall test coverage has not decreased (project-wide)
* [ ] New/modified packages have 85%+ test coverage
* [ ] Work executor successfully executes example-agent tool
* [ ] Tool receives correct JSON on stdin (claim_type, target_artefact, context_chain)
* [ ] Tool output JSON is correctly parsed and artefact created
* [ ] Artefact provenance is correct:
  * [ ] New logical_id (creates new thread)
  * [ ] Version = 1
  * [ ] source_artefacts = [claim.ArtefactID]
  * [ ] Thread tracking updated via AddVersionToThread()
* [ ] Tool timeout (5 min) works correctly and creates Failure artefact
* [ ] Tool failures (non-zero exit, invalid JSON) create Failure artefacts
* [ ] Work executor continues after failure (doesn't crash)
* [ ] Complete E2E test passes: goal → claim → bid → grant → execute → artefact
* [ ] Example agent updated with working echo script
* [ ] Documentation updated with derivative vs evolutionary relationship explanation
* [ ] All TODOs from the specification documents relevant to this milestone have been resolved
* [ ] All failure modes identified in section 6.1 have been implemented and tested
* [ ] Concurrency considerations from section 6.2 have been addressed
* [ ] All open questions from section 7 have been resolved or documented as future work
* [ ] AI agent implementation guidance has been followed and integration checklist completed
* [ ] Security considerations from section 4.4 have been addressed and validated
* [ ] Backward compatibility impact from section 4.5 is documented
* [ ] Dependency impact analysis from section 4.6 has been completed and approved
* [ ] Operational readiness checklist from section 9 is fully satisfied

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Tool execution failures:**

1. **Tool exits with non-zero code:**
   - **Scenario:** Tool script has bug, exits with code 1
   - **Expected behavior:** Create Failure artefact with exit_code=1, full stdout/stderr
   - **Work executor:** Log error, create Failure artefact, continue to next work
   - **Testing:** Integration test with script that exits non-zero

2. **Tool outputs invalid JSON to stdout:**
   - **Scenario:** Tool prints plain text, malformed JSON, or multiple JSON objects
   - **Expected behavior:** Create Failure artefact with parse error, stdout content
   - **Work executor:** Log JSON parse error, create Failure artefact, continue
   - **Testing:** Integration test with script that outputs "invalid json"

3. **Tool outputs partial JSON (crashes mid-write):**
   - **Scenario:** Tool starts writing JSON, crashes before completing
   - **Expected behavior:** Create Failure artefact with incomplete stdout
   - **Work executor:** Wait for process exit, read all stdout, fail JSON parse, create Failure
   - **Testing:** Integration test with script that writes "{"artefact_" then exits

4. **Tool times out (runs > 5 minutes):**
   - **Scenario:** Tool enters infinite loop or long computation
   - **Expected behavior:** Kill process after 5 minutes, create Failure artefact
   - **Work executor:** Context timeout triggers, kill subprocess, create Failure with timeout message
   - **Testing:** Integration test with script that sleeps 10 minutes (with 1-second timeout for test)

5. **Tool hangs reading stdin:**
   - **Scenario:** Tool expects multiple inputs on stdin, waits forever
   - **Expected behavior:** Close stdin pipe after writing JSON, tool should EOF
   - **Work executor:** Write JSON to stdin, close pipe immediately
   - **Testing:** Integration test with script that reads two lines from stdin

6. **Tool outputs nothing to stdout:**
   - **Scenario:** Tool completes but produces no output
   - **Expected behavior:** Create Failure artefact with "empty stdout" error
   - **Work executor:** Check stdout bytes length, create Failure if empty
   - **Testing:** Integration test with script that exits successfully with no output

**Artefact creation failures:**

7. **CreateArtefact() fails (Redis unavailable):**
   - **Scenario:** Redis connection lost during artefact creation
   - **Expected behavior:** Log error, create Failure artefact describing Redis failure
   - **Work executor:** Catch CreateArtefact error, log, create Failure artefact (if Redis available again)
   - **Impact:** Tool output lost (acceptable for V1)
   - **Testing:** Integration test with Redis restart during artefact creation

8. **AddVersionToThread() fails:**
   - **Scenario:** Redis error during thread tracking update
   - **Expected behavior:** Log error, artefact still created (thread tracking is auxiliary)
   - **Work executor:** Log error, continue (artefact exists, thread tracking can be fixed)
   - **Testing:** Integration test with Redis error injection

**Configuration failures:**

9. **Missing SETT_AGENT_COMMAND environment variable:**
   - **Scenario:** Agent container started without command config
   - **Expected behavior:** Cub fails to start with clear error message
   - **Exit code:** Non-zero
   - **Testing:** Unit test in config_test.go

10. **Invalid command in sett.yml:**
    - **Scenario:** Command array is empty or contains non-executable path
    - **Expected behavior:** Tool execution fails, Failure artefact created
    - **Work executor:** exec.Command returns "not found" error, create Failure artefact
    - **Testing:** Integration test with command `["/nonexistent/binary"]`

### **6.2. Concurrency considerations**

**Race conditions:**

1. **Work queue concurrent access:**
   - **Scenario:** Claim Watcher writes to queue while Work Executor reads
   - **Mitigation:** Go channels are thread-safe (no explicit locking needed)
   - **Testing:** Run tests with `-race` flag

2. **Multiple artefact creations:**
   - **Scenario:** Work executor processes claims sequentially (no parallelism in M2.3)
   - **Mitigation:** Work queue size is 1, only one claim processed at a time
   - **Testing:** Verify work executor blocks until current work completes

**Deadlock scenarios:**

3. **Work queue full during shutdown:**
   - **Scenario:** Claim Watcher tries to push to queue after Work Executor exits
   - **Mitigation:** Claim Watcher select loop includes `ctx.Done()` case to abort send
   - **Testing:** Integration test with shutdown during claim queuing

4. **Subprocess doesn't exit:**
   - **Scenario:** Tool ignores SIGTERM, doesn't respond to timeout
   - **Mitigation:** Use `CommandContext` which sends SIGKILL after context cancel
   - **Testing:** Integration test with script that traps SIGTERM

**Process isolation:**

5. **Subprocess inherits file descriptors:**
   - **Scenario:** Subprocess could interfere with parent's Redis connection
   - **Mitigation:** `exec.Command` isolates file descriptors by default
   - **Testing:** Verify subprocess cannot access parent's network connections

6. **Subprocess zombie processes:**
   - **Scenario:** Process killed by timeout but not reaped
   - **Mitigation:** `Wait()` reaps process after context timeout
   - **Testing:** Verify no zombie processes after timeout test

### **6.3. Edge case handling**

**Empty or minimal inputs:**

1. **Target artefact with empty payload:**
   - **Scenario:** GoalDefined artefact has `payload: ""`
   - **Expected behavior:** Pass to tool as-is, tool decides how to handle
   - **Testing:** Integration test with empty payload artefact

2. **Target artefact with no metadata:**
   - **Scenario:** Artefact has `metadata: {}`
   - **Expected behavior:** Include empty metadata object in stdin JSON
   - **Testing:** Unit test for JSON marshaling

**Large inputs/outputs:**

3. **Target artefact with large payload (1MB):**
   - **Scenario:** Artefact payload contains large design document
   - **Expected behavior:** Pass to tool via stdin, tool reads large JSON
   - **Limit:** No limit in M2.3 (acceptable risk for V1)
   - **Testing:** Integration test with 1MB artefact payload

4. **Tool outputs large JSON (10MB):**
   - **Scenario:** Tool creates artefact with huge payload
   - **Expected behavior:** Accept output, create artefact (Redis stores payload)
   - **Limit:** No limit in M2.3 (document as limitation)
   - **Future:** Add stdout size limit (e.g., 10MB max)
   - **Testing:** Integration test with large output

**Timing edge cases:**

5. **Tool completes in <1 second:**
   - **Scenario:** Echo agent is very fast
   - **Expected behavior:** Normal execution, artefact created immediately
   - **Testing:** Standard integration tests (echo agent is fast)

6. **Tool completes exactly at 5-minute timeout:**
   - **Scenario:** Tool finishes just as timeout expires
   - **Expected behavior:** Race condition - may succeed or fail
   - **Accept:** Either outcome acceptable (tool should be faster than timeout)
   - **Testing:** Manual test with tool that sleeps 4:59

**Structural type handling:**

7. **Tool output omits structural_type:**
   - **Scenario:** Tool outputs `{"artefact_type": "X", "artefact_payload": "Y", "summary": "Z"}`
   - **Expected behavior:** Default to `structural_type: "Standard"`
   - **Testing:** Unit test for ToolOutput parsing

8. **Tool output specifies structural_type: "Failure":**
   - **Scenario:** Tool intentionally creates Failure artefact
   - **Expected behavior:** Accept tool's choice, create Failure artefact
   - **Future consideration:** Should tools be allowed to create Failure artefacts?
   - **M2.3 decision:** Allow it (tool knows best if it failed semantically)
   - **Testing:** Integration test with tool that outputs structural_type: "Failure"

## **7. Open questions & decisions**

**Q1: Should tools be allowed to specify structural_type?**
- **Question:** Can tool output include `"structural_type": "Question"` or `"Failure"`?
- **Recommendation:** Allow it in M2.3, document behavior
- **Rationale:** Tool may want to escalate question or report semantic failure
- **Status:** ✅ Decided - allow tools to specify structural_type, default to "Standard"

**Q2: Should SETT_AGENT_COMMAND be environment variable or from sett.yml?**
- **Question:** How does cub know what command to execute?
- **Recommendation:** Pass command from sett.yml via environment variable (JSON array)
- **Implementation:** CLI sets `SETT_AGENT_COMMAND='["/app/run.sh"]'` when launching container
- **Status:** ✅ Decided - use environment variable set by CLI from sett.yml

**Q3: Should work executor retry failed tools?**
- **Question:** If tool fails, should cub retry automatically?
- **Recommendation:** No - create Failure artefact, let orchestrator decide
- **Rationale:** Determinism, avoid retry storms, orchestrator can implement retry policy
- **Status:** ✅ Decided - no retry in M2.3

**Q4: Should stdout/stderr be size-limited?**
- **Question:** What if tool outputs gigabytes to stdout?
- **Recommendation:** No limit in M2.3, document as known limitation
- **Rationale:** YAGNI - wait for real-world data before adding limits
- **Future:** Add configurable limit (e.g., 10MB) in Phase 3
- **Status:** ✅ Decided - no limit in M2.3

**Q5: Should work executor validate artefact_type format?**
- **Question:** Should we restrict artefact_type to alphanumeric?
- **Recommendation:** No validation in M2.3 beyond "non-empty string"
- **Rationale:** Flexibility for agent developers, blackboard already stores arbitrary strings
- **Status:** ✅ Decided - minimal validation (non-empty), no format restrictions

**Q6: What if claim's target artefact doesn't exist in Redis?**
- **Question:** GetArtefact() returns NotFound error - what should cub do?
- **Recommendation:** Create Failure artefact with "target artefact not found" error
- **Rationale:** Defensive programming, catch orchestrator bugs
- **Status:** ✅ Decided - create Failure artefact, continue to next work

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**Start with the simplest path that satisfies success criteria:**

1. **Contract types first:** Define ToolInput and ToolOutput structs with JSON tags, write unit tests.

2. **Subprocess execution isolated:** Create `executeToolSubprocess()` helper that takes input JSON, returns stdout bytes. Test with real subprocess before integrating.

3. **Artefact creation isolated:** Create `createResultArtefact()` helper that takes ToolOutput, returns Artefact with correct provenance. Test with mock data.

4. **Integrate into work executor:** Wire helpers together in work executor loop.

5. **Echo agent last:** Update run.sh script only after cub logic is working.

6. **E2E test final:** Verify complete workflow after all components tested individually.

**Implement comprehensive error handling from the beginning:**

* Every subprocess execution must handle timeout
* Every JSON parse must handle malformed input
* Every artefact creation must handle Redis failures
* Every error creates Failure artefact (don't crash)

**Write tests before implementation (TDD approach):**

* Write test for ToolInput JSON marshaling before implementing
* Write test for subprocess execution with mock script before real implementation
* Write test for artefact provenance before implementing createResultArtefact()

**Use defensive programming - validate all inputs and assumptions:**

* Validate ToolOutput has required fields before creating artefact
* Validate claim has target artefact ID before fetching
* Validate subprocess exit code before assuming success
* Check stdout is non-empty before parsing JSON

### **8.2. Common pitfalls to avoid**

**Forgetting to close stdin pipe:**
* **Pitfall:** Writing JSON to stdin but not closing pipe
* **Result:** Tool hangs waiting for more input
* **Solution:** `io.WriteCloser` close after writing

**Not waiting for subprocess to exit:**
* **Pitfall:** Reading stdout before process exits
* **Result:** Partial output, race conditions
* **Solution:** Call `cmd.Wait()` before reading output

**Wrong artefact provenance:**
* **Pitfall:** Setting `logical_id` to target artefact's logical_id (evolutionary relationship)
* **Result:** Breaks context assembly, wrong graph structure
* **Solution:** Always use derivative model - new UUID for logical_id

**Forgetting to call AddVersionToThread:**
* **Pitfall:** Creating artefact but not updating thread tracking
* **Result:** Thread queries return incomplete history
* **Solution:** Always call `AddVersionToThread()` after `CreateArtefact()`

**Blocking work executor on error:**
* **Pitfall:** Returning error from executeWork(), stopping work executor
* **Result:** Agent can't process any more work
* **Solution:** Log error, create Failure artefact, continue loop

**Not setting work directory:**
* **Pitfall:** Subprocess runs in cub's directory, not /workspace
* **Result:** Tool can't access project files
* **Solution:** Set `cmd.Dir = "/workspace"`

**Subprocess inheriting environment:**
* **Pitfall:** Tool inherits all cub environment variables
* **Result:** Unexpected behavior, potential security issues
* **Solution:** Set `cmd.Env` explicitly (or accept inheritance for simplicity in M2.3)

### **8.3. Integration checklist**

**Pre-implementation verification:**

* [ ] All prerequisite features are complete:
  * [ ] M1.1: Redis Blackboard Foundation (✅ completed)
  * [ ] M1.2: Blackboard Client Operations (✅ completed)
  * [ ] M2.1: Agent Cub Foundation (✅ completed)
  * [ ] M2.2: Claim Watching & Bidding (✅ completed)
* [ ] No breaking changes to existing contracts:
  * [ ] Artefact schema unchanged (uses existing fields)
  * [ ] Claim schema unchanged
  * [ ] Blackboard client API unchanged (uses existing methods)
* [ ] New data structures follow conventions:
  * [ ] ToolInput/ToolOutput use JSON tags consistently
  * [ ] Artefact provenance follows derivative model
* [ ] All component interfaces remain stable:
  * [ ] Work queue channel type unchanged (chan *blackboard.Claim)
  * [ ] Config struct extended (additive: Command field)

**Post-implementation verification:**

* [ ] All Phase 1 and M2.1/M2.2 tests still pass (no regressions)
* [ ] New tests pass with `-race` flag (no data races)
* [ ] Work executor successfully executes echo agent
* [ ] Artefact provenance is correct (new logical_id, version 1, source_artefacts)
* [ ] Thread tracking updated correctly
* [ ] Failure handling works (tool errors create Failure artefacts)
* [ ] Timeout handling works (5-minute limit enforced)
* [ ] Work executor continues after failures (doesn't crash)
* [ ] Complete E2E test passes (goal → artefact → claim → bid → grant → execute → new artefact)

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Metrics to track:**
* **Work executor:**
  * Work executions count (total)
  * Work successes count
  * Work failures count
  * Tool execution time (histogram)
  * Timeout occurrences count
  * Artefacts created count (by type)

**Log messages:**

**Work executor:**
* `[INFO] Work Executor received claim from queue: claim_id=<uuid>`
* `[INFO] Fetched target artefact: artefact_id=<uuid> type=<type>`
* `[INFO] Executing tool: command=<cmd> claim_id=<uuid>`
* `[INFO] Tool execution completed: claim_id=<uuid> exit_code=0 duration=<ms>`
* `[INFO] Created artefact: artefact_id=<uuid> type=<type> logical_id=<uuid> version=1`
* `[ERROR] Tool execution failed: claim_id=<uuid> exit_code=<code> stdout=<truncated> stderr=<truncated>`
* `[ERROR] Tool execution timeout: claim_id=<uuid> timeout=5m`
* `[ERROR] Failed to parse tool output: claim_id=<uuid> error=<error> stdout=<truncated>`
* `[ERROR] Failed to create artefact: claim_id=<uuid> error=<error>`
* `[INFO] Created Failure artefact: artefact_id=<uuid> claim_id=<uuid> reason=<reason>`

**Structured logging events (if using structured logging):**
* Not required in M2.3 (standard `log` package sufficient)
* Can be added in Phase 3 if needed

**Health check modifications:**
* Cub health check unchanged (Redis PING only)
* Health check doesn't report work executor status (acceptable for V1)
* Future: Add /metrics endpoint for work executor stats

**Issue detection and diagnosis:**
* **Tool not executing:** Check cub logs for work queue activity
* **Tool timing out:** Check logs for timeout messages, verify tool behavior
* **Artefacts not created:** Check orchestrator logs for claim creation, agent logs for execution
* **Invalid tool output:** Check cub logs for parse errors, tool stderr

### **9.2. Rollback and disaster recovery**

**Feature disable via configuration:**
* Not applicable - M2.3 is core functionality (no feature flag)

**Rollback procedure if feature causes issues:**
1. Run `sett down` to stop all containers
2. Revert to M2.2 git commit
3. Rebuild cub binary: `make build-cub`
4. Rebuild agent images: `docker build -t example-agent:latest -f agents/example-agent/Dockerfile .`
5. Rebuild orchestrator image: `make docker-orchestrator`
6. Run `sett up` (work executor will return to placeholder mode)

**Data migration or cleanup requirements:**
* None - M2.3 creates new artefacts, doesn't modify existing data
* Artefacts created by M2.3 remain in Redis (immutable)
* If rolling back, new artefacts are orphaned but harmless

**Recovery time objective:**
* **RTO:** <5 minutes (stop containers, rebuild, restart)
* **RPO:** N/A (work-in-progress lost, but claims/artefacts persist in Redis)

### **9.3. Documentation and training**

**CLI command documentation:**
* No new CLI commands in M2.3
* Update `sett forage --help` to mention that work will now be executed

**Feature coverage in user guides:**
* Add section to README: "How Agents Execute Work"
* Document stdin/stdout contract with examples
* Document derivative vs evolutionary relationships
* Document Failure artefact behavior

**API documentation:**
* **GoDoc comments:** All new functions in executor.go and contract.go must have clear documentation
* **Contract documentation:** Document ToolInput and ToolOutput structs with field descriptions

**Troubleshooting guides:**

**Problem:** Tool never executes (work executor stuck)
* **Solution:**
  1. Check agent logs: `sett logs agent-name`
  2. Verify claim was granted (check orchestrator logs)
  3. Verify work queue has buffer space
  4. Check for panic in work executor

**Problem:** Tool outputs valid JSON but artefact not created
* **Solution:**
  1. Check agent logs for artefact creation errors
  2. Verify Redis connectivity
  3. Verify JSON matches expected schema (artefact_type, artefact_payload, summary)

**Problem:** Tool times out (runs > 5 minutes)
* **Solution:**
  1. Check tool logic for infinite loops
  2. Verify tool isn't waiting for user input
  3. Consider if tool needs longer timeout (file issue for configurable timeout)

**Problem:** Failure artefacts created for successful tool
* **Solution:**
  1. Check tool stderr for error messages
  2. Verify tool outputs single JSON object to stdout
  3. Verify tool exits with code 0
  4. Check JSON is valid (use `jq` to validate)

**Team training requirements:**
* Demo full workflow in team meeting: goal → execution → artefact
* Document stdin/stdout contract with examples
* Explain derivative vs evolutionary relationships (critical concept)
* Show how to debug failed tool executions

## **10. Self-validation checklist**

### **Before starting implementation:**

* [ ] I understand how this feature aligns with the current phase (Phase 2: Single Agent)
* [ ] All success criteria (section 1.3) are measurable and testable
* [ ] I have considered every component in section 2 explicitly
* [ ] All design decisions (section 3.1) are justified and documented
* [ ] I understand the difference between derivative and evolutionary relationships
* [ ] I understand the artefact provenance model (new logical_id, version 1, source_artefacts)
* [ ] I understand the stdin/stdout contract and JSON schemas
* [ ] I understand the 5-minute timeout requirement

### **During implementation:**

* [ ] I am implementing the simplest solution that meets success criteria
* [ ] All error scenarios (section 6) are being handled, not just happy path
* [ ] Tests are being written before or alongside code (TDD approach)
* [ ] I am validating that existing functionality is not broken (run M2.1/M2.2 tests)
* [ ] Tool subprocess execution uses context.WithTimeout for 5-minute limit
* [ ] Stdin pipe is closed after writing JSON to prevent tool hangs
* [ ] Subprocess stdout/stderr are captured before checking exit code
* [ ] Artefact provenance is correct (new logical_id = artefact ID, version = 1)
* [ ] AddVersionToThread() is called after CreateArtefact()
* [ ] Failure artefacts are created for all tool failures
* [ ] Work executor continues after errors (doesn't crash)

### **Before submission:**

* [ ] All items in Definition of Done (section 5) are complete
* [ ] Feature has been tested in a clean environment from scratch
* [ ] All tests pass with `-race` flag (no race conditions)
* [ ] Documentation is updated and accurate (GoDoc comments, README)
* [ ] I have considered the operational impact (section 9) of this feature
* [ ] E2E test passes: goal → claim → bid → grant → execute → artefact
* [ ] Example agent works correctly (reads stdin, outputs JSON)
* [ ] Artefact provenance verified in Redis (new thread, version 1, source_artefacts)
* [ ] Thread tracking verified in Redis (ZSET contains new artefact)
* [ ] Derivative vs evolutionary relationship documented and explained
* [ ] No goroutine leaks detected (process exits cleanly)
* [ ] Tool timeout tested (5-minute limit enforced)
* [ ] Tool failure handling tested (Failure artefacts created)
