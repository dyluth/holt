# **Feature design: Controller-Worker Pattern for Scaling**

**Purpose**: Enable horizontal scaling with controller-worker architecture
**Scope**: Phase 3 Coordination - Milestone 4
**Estimated tokens**: ~8,000 tokens
**Read when**: Implementing controller-worker pattern, scaling agents horizontally

Associated phase: **Coordination (Phase 3)**
Status: **Design**

***Template purpose:*** This document is a blueprint for M3.4, an implementable milestone that enables horizontal agent scaling by separating bidding (persistent controllers) from execution (ephemeral workers). It provides an unambiguous specification for implementing the controller-worker pattern while eliminating bidding race conditions.

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Enable horizontal scaling of agent execution by implementing a controller-worker pattern where:
- A single persistent **controller** container per role handles bidding
- Ephemeral **worker** containers are launched on-demand to execute granted claims
- Multiple workers for the same role can execute in parallel
- Race conditions in bidding are eliminated (single bidder per role)

### **1.2. User story**

As a Sett user, I want to scale my agents horizontally so that:
1. A single controller agent submits bids on behalf of its role
2. When the controller wins a claim, the orchestrator launches an ephemeral worker
3. The worker executes the claim and exits when complete
4. Multiple workers can run concurrently (configurable limit)
5. Race conditions in bidding are eliminated (no duplicate bids from replicas)
6. Workers are automatically cleaned up after execution

This enables efficient resource usage (workers scale to zero when idle) and eliminates the bidding race condition problem inherent in running multiple replicas of the same agent.

### **1.3. Success criteria**

1. **Controller configuration**: `sett.yml` supports `mode: "controller"` with nested `worker:` config
2. **Controller behavior**: Controller container only bids, never executes work
3. **Worker launching**: Orchestrator launches workers via Docker API when controller wins grant
4. **Worker execution**: Worker executes claim via `cub --execute-claim <claim_id>` and exits
5. **Concurrency control**: `max_concurrent` workers limit enforced per role
6. **Worker lifecycle**: Orchestrator tracks workers (created → running → exited) and cleans up
7. **Failure detection**: Worker exit code ≠ 0 creates Failure artefact
8. **Grant pausing**: When at `max_concurrent`, orchestrator pauses granting to that role
9. **Backward compatibility**: Existing `sett.yml` without `mode` continues working unchanged
10. **Configuration validation**: `sett up` validates controller configuration

### **1.4. Non-goals**

- **Persistent grant queue** (M3.5): Queue for paused grants deferred to restart resilience milestone
- **Retry logic** (Future): Automatic worker restart on failure
- **Dynamic worker scaling** (Future): Auto-scaling based on workload
- **Worker-to-worker communication** (Future): Workers are independent
- **Controller high availability** (Future): Single controller per role is sufficient
- **Cross-role worker pools** (Future): Workers are role-specific

## **2. The 'what': component impact analysis**

### **2.1. Configuration changes**

**Schema changes required**: New agent configuration fields for controller-worker pattern.

#### **2.1.1. Agent configuration schema**

```yaml
version: "1.0"

orchestrator:
  max_review_iterations: 3

agents:
  # Traditional agent (M3.3 behavior - unchanged)
  simple-agent:
    role: "SimpleAgent"
    image: "simple:latest"
    command: ["/app/run.sh"]
    bidding_strategy: "exclusive"
    workspace:
      mode: ro

  # Controller-worker agent (M3.4 new pattern)
  coder-controller:
    role: "Coder"
    mode: "controller"  # M3.4: Designates as controller
    image: "coder:latest"
    bidding_strategy: "exclusive"

    worker:  # M3.4: Worker configuration
      image: "coder:latest"  # Can differ from controller
      max_concurrent: 3  # Max parallel workers (default: 1)
      command: ["/app/run.sh"]
      workspace:
        mode: rw
      # Workers can have different resource limits
      # memory_limit: "512m"  # Future
      # cpu_limit: "0.5"      # Future

services:
  redis:
    image: redis:7-alpine
```

#### **2.1.2. Config struct changes**

**File**: `internal/config/config.go`

```go
type Agent struct {
    Role            string          `yaml:"role"`
    Image           string          `yaml:"image"`
    Command         []string        `yaml:"command"`
    BiddingStrategy string          `yaml:"bidding_strategy"`
    Workspace       WorkspaceConfig `yaml:"workspace,omitempty"`

    // M3.4: Controller-worker pattern
    Mode            string          `yaml:"mode,omitempty"`        // "controller" or empty (traditional)
    Worker          *WorkerConfig   `yaml:"worker,omitempty"`      // Required if mode="controller"
}

// M3.4: Worker configuration
type WorkerConfig struct {
    Image          string          `yaml:"image"`                  // Worker image (can differ from controller)
    MaxConcurrent  int             `yaml:"max_concurrent,omitempty"` // Default: 1
    Command        []string        `yaml:"command"`
    Workspace      WorkspaceConfig `yaml:"workspace,omitempty"`
}
```

#### **2.1.3. Configuration validation**

```go
func (c *SettConfig) Validate() error {
    // ... existing validation ...

    // M3.4: Validate controller-worker configuration
    for agentName, agent := range c.Agents {
        if agent.Mode == "controller" {
            // Validate worker config exists
            if agent.Worker == nil {
                return fmt.Errorf("agent '%s' has mode='controller' but no worker configuration", agentName)
            }

            // Validate worker image
            if agent.Worker.Image == "" {
                return fmt.Errorf("agent '%s' worker configuration missing image", agentName)
            }

            // Validate worker command
            if len(agent.Worker.Command) == 0 {
                return fmt.Errorf("agent '%s' worker configuration missing command", agentName)
            }

            // Set default max_concurrent if not specified
            if agent.Worker.MaxConcurrent == 0 {
                agent.Worker.MaxConcurrent = 1
            }

            // Validate max_concurrent is positive
            if agent.Worker.MaxConcurrent < 1 {
                return fmt.Errorf("agent '%s' worker.max_concurrent must be >= 1", agentName)
            }
        } else if agent.Mode != "" {
            // Unknown mode
            return fmt.Errorf("agent '%s' has unknown mode '%s' (valid: 'controller' or omit)", agentName, agent.Mode)
        }
    }

    return nil
}
```

### **2.2. Orchestrator changes**

**Major enhancement**: Orchestrator gains worker lifecycle management and concurrency control.

#### **2.2.1. Worker state tracking**

**File**: `internal/orchestrator/workers.go` (new file)

```go
// WorkerState tracks an active worker container
type WorkerState struct {
    ContainerID   string    // Docker container ID
    ContainerName string    // sett-{instance}-{agent}-worker-{claim}
    ClaimID       string    // Claim being executed
    Role          string    // Agent role
    AgentName     string    // Original agent name (e.g., "coder-controller")
    LaunchedAt    time.Time // When worker was launched
    Status        string    // "created", "running", "exited"
    ExitCode      int       // Container exit code (when exited)
}

// Engine maintains worker tracking state
type Engine struct {
    // ... existing fields ...

    // M3.4: Worker tracking
    activeWorkers     map[string]*WorkerState // key: container_id
    workersByRole     map[string]int          // key: role, value: active worker count
    workerLock        sync.RWMutex
}
```

#### **2.2.2. Grant decision logic (enhanced)**

**File**: `internal/orchestrator/consensus.go` (update existing)

```go
// determineGrants checks if role is controller-worker and enforces max_concurrent
func (e *Engine) determineGrants(ctx context.Context, claim *blackboard.Claim, bids map[string]blackboard.BidType) (GrantDecision, error) {
    // ... existing grant logic (M3.1/M3.2/M3.3) ...

    // M3.4: Check if granted agent is a controller
    if grantedAgent != "" {
        agent, exists := e.config.Agents[grantedAgent]
        if exists && agent.Mode == "controller" {
            // Check max_concurrent limit
            if e.isAtWorkerLimit(agent.Role, agent.Worker.MaxConcurrent) {
                e.logEvent("worker_limit_reached", map[string]interface{}{
                    "role":           agent.Role,
                    "max_concurrent": agent.Worker.MaxConcurrent,
                    "claim_id":       claim.ID,
                })

                // Pause granting to this role until worker slot available
                return GrantDecision{}, fmt.Errorf("role '%s' at max_concurrent worker limit (%d)", agent.Role, agent.Worker.MaxConcurrent)
            }
        }
    }

    return grantDecision, nil
}

// isAtWorkerLimit checks if role has reached max concurrent workers
func (e *Engine) isAtWorkerLimit(role string, maxConcurrent int) bool {
    e.workerLock.RLock()
    defer e.workerLock.RUnlock()

    activeCount := e.workersByRole[role]
    return activeCount >= maxConcurrent
}
```

#### **2.2.3. Worker launching logic**

**File**: `internal/orchestrator/workers.go`

```go
// LaunchWorker creates and starts an ephemeral worker container
func (e *Engine) LaunchWorker(ctx context.Context, claim *blackboard.Claim, agentName string, agent config.Agent) error {
    // Generate worker container name
    shortClaimID := claim.ID[:8] // First 8 chars of UUID
    containerName := fmt.Sprintf("sett-%s-%s-worker-%s", e.instanceName, agentName, shortClaimID)

    e.logEvent("worker_launching", map[string]interface{}{
        "container_name": containerName,
        "claim_id":       claim.ID,
        "role":           agent.Role,
        "agent_name":     agentName,
    })

    // Build Docker container config
    containerConfig := &container.Config{
        Image: agent.Worker.Image,
        Cmd:   append([]string{"--execute-claim", claim.ID}, agent.Worker.Command...),
        Env: []string{
            fmt.Sprintf("SETT_INSTANCE=%s", e.instanceName),
            fmt.Sprintf("SETT_AGENT_NAME=%s", agentName),
            fmt.Sprintf("SETT_AGENT_ROLE=%s", agent.Role),
            fmt.Sprintf("REDIS_ADDR=%s", e.redisAddr),
        },
    }

    hostConfig := &container.HostConfig{
        NetworkMode: container.NetworkMode(fmt.Sprintf("sett-%s", e.instanceName)),
        AutoRemove:  false, // We manage cleanup explicitly for better tracking
    }

    // Add workspace mount if configured
    if agent.Worker.Workspace.Mode != "" {
        mount := mount.Mount{
            Type:     mount.TypeBind,
            Source:   e.workspacePath,
            Target:   "/workspace",
            ReadOnly: (agent.Worker.Workspace.Mode == "ro"),
        }
        hostConfig.Mounts = []mount.Mount{mount}
    }

    // Create container
    resp, err := e.dockerClient.ContainerCreate(ctx, containerConfig, hostConfig, nil, nil, containerName)
    if err != nil {
        return fmt.Errorf("failed to create worker container: %w", err)
    }

    // Start container
    if err := e.dockerClient.ContainerStart(ctx, resp.ID, types.ContainerStartOptions{}); err != nil {
        // Cleanup on start failure
        e.dockerClient.ContainerRemove(ctx, resp.ID, types.ContainerRemoveOptions{Force: true})
        return fmt.Errorf("failed to start worker container: %w", err)
    }

    // Track worker state
    e.workerLock.Lock()
    workerState := &WorkerState{
        ContainerID:   resp.ID,
        ContainerName: containerName,
        ClaimID:       claim.ID,
        Role:          agent.Role,
        AgentName:     agentName,
        LaunchedAt:    time.Now(),
        Status:        "running",
    }
    e.activeWorkers[resp.ID] = workerState
    e.workersByRole[agent.Role]++
    e.workerLock.Unlock()

    e.logEvent("worker_launched", map[string]interface{}{
        "container_id":   resp.ID,
        "container_name": containerName,
        "claim_id":       claim.ID,
        "role":           agent.Role,
    })

    // Start monitoring worker in background
    go e.monitorWorker(ctx, resp.ID)

    return nil
}
```

#### **2.2.4. Worker monitoring & cleanup**

```go
// monitorWorker watches a worker container and handles completion/failure
func (e *Engine) monitorWorker(ctx context.Context, containerID string) {
    e.workerLock.RLock()
    worker := e.activeWorkers[containerID]
    e.workerLock.RUnlock()

    if worker == nil {
        log.Printf("[Orchestrator] Worker %s not found in tracking state", containerID)
        return
    }

    // Wait for container to exit
    statusCh, errCh := e.dockerClient.ContainerWait(ctx, containerID, container.WaitConditionNotRunning)

    select {
    case err := <-errCh:
        log.Printf("[Orchestrator] Error waiting for worker %s: %v", containerID, err)
        e.handleWorkerError(ctx, worker, err)

    case status := <-statusCh:
        log.Printf("[Orchestrator] Worker %s exited with code %d", containerID, status.StatusCode)
        e.handleWorkerExit(ctx, worker, int(status.StatusCode))
    }

    // Cleanup
    e.cleanupWorker(ctx, containerID)
}

// handleWorkerExit processes worker completion or failure
func (e *Engine) handleWorkerExit(ctx context.Context, worker *WorkerState, exitCode int) {
    if exitCode != 0 {
        // Worker failed - create Failure artefact
        e.logEvent("worker_failed", map[string]interface{}{
            "container_id": worker.ContainerID,
            "claim_id":     worker.ClaimID,
            "exit_code":    exitCode,
        })

        // Get container logs for failure details
        logs := e.getWorkerLogs(ctx, worker.ContainerID)

        // Create Failure artefact
        failurePayload := fmt.Sprintf("Worker container exited with code %d\n\nLogs:\n%s", exitCode, logs)
        failure := &blackboard.Artefact{
            ID:              uuid.New().String(),
            LogicalID:       uuid.New().String(),
            Version:         1,
            StructuralType:  blackboard.StructuralTypeFailure,
            Type:            "WorkerFailure",
            Payload:         failurePayload,
            SourceArtefacts: []string{},
            ProducedByRole:  worker.Role,
        }

        if err := e.client.CreateArtefact(ctx, failure); err != nil {
            log.Printf("[Orchestrator] Failed to create Failure artefact: %v", err)
        }

        // Terminate claim
        claim, err := e.client.GetClaim(ctx, worker.ClaimID)
        if err == nil {
            claim.Status = blackboard.ClaimStatusTerminated
            claim.TerminationReason = fmt.Sprintf("Worker failed with exit code %d", exitCode)
            e.client.UpdateClaim(ctx, claim)
        }
    } else {
        // Worker succeeded
        e.logEvent("worker_completed", map[string]interface{}{
            "container_id": worker.ContainerID,
            "claim_id":     worker.ClaimID,
        })
    }
}

// cleanupWorker removes worker from tracking and Docker
func (e *Engine) cleanupWorker(ctx context.Context, containerID string) {
    e.workerLock.Lock()
    worker := e.activeWorkers[containerID]
    if worker != nil {
        delete(e.activeWorkers, containerID)
        e.workersByRole[worker.Role]--
    }
    e.workerLock.Unlock()

    // Remove container
    e.dockerClient.ContainerRemove(ctx, containerID, types.ContainerRemoveOptions{
        Force: true,
    })

    if worker != nil {
        e.logEvent("worker_cleanup", map[string]interface{}{
            "container_id": containerID,
            "role":         worker.Role,
        })
    }
}

// getWorkerLogs retrieves container logs for failure debugging
func (e *Engine) getWorkerLogs(ctx context.Context, containerID string) string {
    options := types.ContainerLogsOptions{
        ShowStdout: true,
        ShowStderr: true,
        Tail:       "100", // Last 100 lines
    }

    reader, err := e.dockerClient.ContainerLogs(ctx, containerID, options)
    if err != nil {
        return fmt.Sprintf("(failed to retrieve logs: %v)", err)
    }
    defer reader.Close()

    logs, err := io.ReadAll(reader)
    if err != nil {
        return fmt.Sprintf("(failed to read logs: %v)", err)
    }

    return string(logs)
}
```

#### **2.2.5. Grant flow integration**

**File**: `internal/orchestrator/engine.go` (update existing grant processing)

```go
// processGrantDecision handles grant assignment and worker launching
func (e *Engine) processGrantDecision(ctx context.Context, claim *blackboard.Claim, decision GrantDecision) error {
    // ... existing grant assignment logic ...

    // M3.4: Check if granted agent is a controller
    if decision.ExclusiveAgent != "" {
        agent, exists := e.config.Agents[decision.ExclusiveAgent]
        if exists && agent.Mode == "controller" {
            // Launch worker instead of notifying controller
            if err := e.LaunchWorker(ctx, claim, decision.ExclusiveAgent, agent); err != nil {
                log.Printf("[Orchestrator] Failed to launch worker: %v", err)
                // Terminate claim with error
                claim.Status = blackboard.ClaimStatusTerminated
                claim.TerminationReason = fmt.Sprintf("Failed to launch worker: %v", err)
                return e.client.UpdateClaim(ctx, claim)
            }

            // Don't publish claim event - worker doesn't subscribe
            return nil
        }
    }

    // Traditional agent flow (M3.3)
    return e.publishClaimEvent(ctx, claim.ID)
}
```

### **2.3. Cub changes**

**Major enhancement**: Cub gains execute-claim mode and controller-only mode.

#### **2.3.1. Command-line flag handling**

**File**: `cmd/cub/main.go`

```go
func main() {
    // M3.4: Add --execute-claim flag
    executeClaimID := flag.String("execute-claim", "", "Execute specific claim ID and exit (worker mode)")
    flag.Parse()

    // Load configuration
    cfg := loadConfig()

    // M3.4: Check for controller mode
    if cfg.Mode == "controller" {
        log.Printf("[Cub] Starting in controller mode (bidder-only)")
        runControllerMode(cfg)
    } else if *executeClaimID != "" {
        log.Printf("[Cub] Starting in worker mode for claim %s", *executeClaimID)
        runWorkerMode(cfg, *executeClaimID)
    } else {
        // Traditional agent mode (M3.3)
        log.Printf("[Cub] Starting in traditional agent mode")
        runTraditionalMode(cfg)
    }
}
```

#### **2.3.2. Controller mode (bidder-only)**

**File**: `internal/cub/controller.go` (new file)

```go
// runControllerMode runs a controller that only bids, never executes
func runControllerMode(cfg *Config) {
    ctx := context.Background()

    // Connect to blackboard
    bbClient, err := blackboard.NewClient(&redis.Options{Addr: cfg.RedisAddr}, cfg.InstanceName)
    if err != nil {
        log.Fatalf("[Controller] Failed to connect to blackboard: %v", err)
    }
    defer bbClient.Close()

    log.Printf("[Controller] Controller %s (role: %s) ready", cfg.AgentName, cfg.AgentRole)

    // Subscribe to claim events
    claimEvents, err := bbClient.SubscribeClaimEvents(ctx)
    if err != nil {
        log.Fatalf("[Controller] Failed to subscribe to claims: %v", err)
    }

    // Bidding loop (never executes)
    for {
        select {
        case claimID := <-claimEvents:
            claim, err := bbClient.GetClaim(ctx, claimID)
            if err != nil {
                log.Printf("[Controller] Failed to fetch claim %s: %v", claimID, err)
                continue
            }

            // Submit bid if interested
            if shouldBid(cfg, claim) {
                bid := determineBidType(cfg, claim)
                if err := bbClient.SubmitBid(ctx, claimID, cfg.AgentName, bid); err != nil {
                    log.Printf("[Controller] Failed to submit bid: %v", err)
                }

                log.Printf("[Controller] Submitted bid: claim=%s type=%s", claimID, bid)
            }

        case <-ctx.Done():
            return
        }
    }
}
```

#### **2.3.3. Worker mode (execute-only)**

**File**: `internal/cub/worker.go` (new file)

```go
// runWorkerMode executes a specific claim and exits
func runWorkerMode(cfg *Config, claimID string) {
    ctx := context.Background()

    log.Printf("[Worker] Executing claim %s", claimID)

    // Connect to blackboard
    bbClient, err := blackboard.NewClient(&redis.Options{Addr: cfg.RedisAddr}, cfg.InstanceName)
    if err != nil {
        log.Fatalf("[Worker] Failed to connect to blackboard: %v", err)
    }
    defer bbClient.Close()

    // Fetch the claim
    claim, err := bbClient.GetClaim(ctx, claimID)
    if err != nil {
        log.Fatalf("[Worker] Failed to fetch claim: %v", err)
    }

    // Verify claim is granted to our role
    if claim.GrantedExclusiveAgent == "" || claim.Status != blackboard.ClaimStatusPendingExclusive {
        log.Fatalf("[Worker] Claim %s is not granted or not in correct status", claimID)
    }

    // Fetch target artefact
    targetArtefact, err := bbClient.GetArtefact(ctx, claim.ArtefactID)
    if err != nil {
        log.Fatalf("[Worker] Failed to fetch target artefact: %v", err)
    }

    // Assemble context
    contextChain, err := assembleContext(ctx, bbClient, targetArtefact, claim)
    if err != nil {
        log.Fatalf("[Worker] Failed to assemble context: %v", err)
    }

    // Execute tool
    toolOutput, err := executeTool(cfg, claim, targetArtefact, contextChain)
    if err != nil {
        log.Fatalf("[Worker] Tool execution failed: %v", err)
    }

    // Create result artefact
    resultArtefact, err := createResultArtefact(ctx, bbClient, cfg, claim, targetArtefact, toolOutput)
    if err != nil {
        log.Fatalf("[Worker] Failed to create result artefact: %v", err)
    }

    log.Printf("[Worker] Created artefact %s, exiting", resultArtefact.ID)

    // Exit cleanly (code 0) - orchestrator will detect completion
    os.Exit(0)
}
```

### **2.4. Instance manager changes**

**Enhancement**: Instance manager needs to handle controller containers differently.

**File**: `internal/instance/containers.go`

```go
// startAgentContainer creates agent container (controller or traditional)
func (m *Manager) startAgentContainer(ctx context.Context, agentName string, agent config.Agent) error {
    containerName := fmt.Sprintf("sett-%s-%s", m.instanceName, agentName)

    // Determine command based on mode
    var cmd []string
    if agent.Mode == "controller" {
        // Controller runs cub with no special flags
        cmd = []string{"/app/cub"}
    } else {
        // Traditional agent
        cmd = []string{"/app/cub"}
    }

    containerConfig := &container.Config{
        Image: agent.Image,
        Cmd:   cmd,
        Env: []string{
            fmt.Sprintf("SETT_INSTANCE=%s", m.instanceName),
            fmt.Sprintf("SETT_AGENT_NAME=%s", agentName),
            fmt.Sprintf("SETT_AGENT_ROLE=%s", agent.Role),
            fmt.Sprintf("SETT_MODE=%s", agent.Mode), // M3.4: Pass mode to cub
            fmt.Sprintf("REDIS_ADDR=redis:6379"),
        },
    }

    // ... rest of container creation (workspace mounts, network, etc.) ...
}
```

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

**Critical Design Decisions:**

1. **Explicit mode configuration**:
   - **Decision**: Use `mode: "controller"` in sett.yml
   - **Rationale**: Clear intent, no ambiguity about agent behavior
   - **Trade-off**: More verbose config vs. clarity

2. **Command-line claim delivery**:
   - **Decision**: `cub --execute-claim <claim_id>`
   - **Rationale**: Explicit, simple, testable
   - **Alternative rejected**: Worker watches events (adds complexity, latency)

3. **Worker lifecycle ownership**:
   - **Decision**: Orchestrator manages everything (launch, monitor, cleanup)
   - **Rationale**: Centralized control, easier debugging, audit trail
   - **Trade-off**: Orchestrator complexity vs. distributed responsibility

4. **Concurrency limit enforcement**:
   - **Decision**: Pause granting when at limit (no queue)
   - **Rationale**: Simple for M3.4, queue requires persistence (M3.5)
   - **Trade-off**: Claims may wait longer vs. implementation simplicity

5. **Failure handling**:
   - **Decision**: Non-zero exit code → Failure artefact, no retries
   - **Rationale**: Simple, deterministic, retries add complexity
   - **Trade-off**: Manual intervention required vs. auto-recovery

6. **Worker naming convention**:
   - **Decision**: `sett-{instance}-{agent_name}-worker-{short_claim_uuid}`
   - **Rationale**: Unique, traceable, debuggable
   - **Trade-off**: Longer names vs. information density

**Risks:**

1. **Docker API failures**:
   - **Risk**: Container creation fails (out of resources, network issues)
   - **Probability**: Low-Medium (depends on environment)
   - **Impact**: High (claim stuck, workflow halted)
   - **Mitigation**: Create Failure artefact, terminate claim gracefully

2. **Worker orphaning**:
   - **Risk**: Orchestrator crashes, loses track of workers
   - **Probability**: Medium (orchestrator restart)
   - **Impact**: High (orphaned containers, resource leak)
   - **Mitigation**: M3.5 restart resilience, manual cleanup via `sett down`

3. **Concurrency limit deadlock**:
   - **Risk**: All workers at limit, no progress
   - **Probability**: Low (requires sustained high load)
   - **Impact**: Medium (workflow throughput reduced)
   - **Mitigation**: Users configure appropriate `max_concurrent`, monitoring

4. **Container log truncation**:
   - **Risk**: Worker logs > 100 lines, important errors lost
   - **Probability**: Medium (verbose tool scripts)
   - **Impact**: Low (harder debugging)
   - **Mitigation**: Configurable tail limit, full logs in Docker

5. **Network race conditions**:
   - **Risk**: Worker starts before network ready
   - **Probability**: Low (Docker handles ordering)
   - **Impact**: Low (worker fails, Failure artefact created)
   - **Mitigation**: Worker cub retries Redis connection on startup

### **3.2. Implementation steps**

**Phase 1: Configuration Schema** (1-2 PRs)

1. **[Config]** Add `Mode` and `WorkerConfig` fields to `Agent` struct in `internal/config/config.go`
2. **[Config]** Implement validation for controller configuration
3. **[Config]** Add default value logic for `max_concurrent` (default: 1)
4. **[Config]** Write unit tests for configuration validation
5. **[Config]** Update example `sett.yml` with controller example

**Phase 2: Cub Controller Mode** (1 PR)

6. **[Cub]** Add `--execute-claim` flag to `cmd/cub/main.go`
7. **[Cub]** Create `internal/cub/controller.go` with controller mode logic
8. **[Cub]** Implement bidder-only mode (subscribe, bid, never execute)
9. **[Cub]** Add mode detection from env var `SETT_MODE`
10. **[Cub]** Write unit tests for controller bidding logic

**Phase 3: Cub Worker Mode** (1 PR)

11. **[Cub]** Create `internal/cub/worker.go` with worker mode logic
12. **[Cub]** Implement execute-only mode (fetch claim, execute, exit)
13. **[Cub]** Add claim fetching and validation
14. **[Cub]** Write unit tests for worker execution flow

**Phase 4: Orchestrator Worker Management** (2-3 PRs)

15. **[Orchestrator]** Create `internal/orchestrator/workers.go`:
    - WorkerState struct
    - activeWorkers and workersByRole tracking
    - Worker lifecycle methods

16. **[Orchestrator]** Implement `LaunchWorker()`:
    - Generate container name
    - Build Docker config with --execute-claim
    - Create and start container
    - Track in activeWorkers map

17. **[Orchestrator]** Implement `monitorWorker()`:
    - Wait for container exit
    - Handle exit codes (0 vs non-zero)
    - Create Failure artefact on error

18. **[Orchestrator]** Implement `cleanupWorker()`:
    - Remove from tracking
    - Remove container
    - Decrement workersByRole count

19. **[Orchestrator]** Write unit tests for worker lifecycle (mock Docker API)

**Phase 5: Orchestrator Grant Logic** (1 PR)

20. **[Orchestrator]** Update `determineGrants()` in consensus.go:
    - Detect controller mode
    - Check max_concurrent limit
    - Pause granting if at limit

21. **[Orchestrator]** Update `processGrantDecision()` in engine.go:
    - Launch worker for controller grants
    - Skip claim event publishing for workers

22. **[Orchestrator]** Write unit tests for grant decision with controllers

**Phase 6: Instance Manager Integration** (1 PR)

23. **[Instance]** Update `startAgentContainer()` to pass `SETT_MODE` env var
24. **[Instance]** Handle controller vs traditional agent startup
25. **[Instance]** Write unit tests for container creation

**Phase 7: Integration & E2E Tests** (2 PRs)

26. **[Tests]** Create `cmd/sett/commands/e2e_m3_4_test.go`:
    - Test controller-worker basic flow
    - Test max_concurrent limit enforcement
    - Test worker failure handling
    - Test worker cleanup
    - Test backward compatibility (traditional agents)

27. **[Tests]** Create integration tests with real Docker (testcontainers)

28. **[Tests]** Verify all M3.1/M3.2/M3.3 tests still pass

**Phase 8: Documentation** (1 PR)

29. **[Docs]** Update `README.md` with M3.4 status
30. **[Docs]** Update Phase 3 README with M3.4 completion
31. **[Docs]** Add controller-worker pattern guide to agent-development.md
32. **[Docs]** Update troubleshooting guide with M3.4 issues
33. **[Docs]** Add example controller configuration to examples/

### **3.3. Performance & resource considerations**

**Resource usage:**

- **Controller containers**: 1 per role (persistent, minimal CPU/memory)
- **Worker containers**: Ephemeral, scale to zero when idle
- **Orchestrator memory**: ~100 bytes per active worker (tracking state)
- **Docker API calls**: Create + Start + Wait + Remove per worker (4 calls/worker)

**Scalability limits:**

- **Workers per role**: Limited by `max_concurrent` (configurable)
- **Total workers**: Limited by Docker host resources
- **Worker startup latency**: ~500ms (Docker container creation overhead)
- **Concurrency control**: O(1) lookup in workersByRole map

**Performance requirements:**

- **Worker launch**: <1 second from grant to container running
- **Worker monitoring**: <100ms from exit to cleanup start
- **Grant pausing**: <10ms to check max_concurrent limit
- **No performance regression**: M3.3 workflows maintain same performance

### **3.4. Testing strategy**

**Unit tests:**

1. **Configuration validation**:
   - Test controller config with worker block
   - Test default max_concurrent value
   - Test invalid configurations (missing worker, negative max_concurrent)
   - Test backward compatibility (no mode specified)

2. **Orchestrator worker lifecycle**:
   - Test LaunchWorker() with mocked Docker API
   - Test worker state tracking (activeWorkers map)
   - Test max_concurrent limit checking
   - Test worker monitoring and exit handling
   - Test worker cleanup

3. **Cub controller mode**:
   - Test bidder-only behavior
   - Test that controller never executes work
   - Test bid submission logic

4. **Cub worker mode**:
   - Test claim fetching and validation
   - Test execute-only behavior
   - Test clean exit after execution

**Integration tests:**

1. **Orchestrator + Docker**:
   - Real Docker API (testcontainers)
   - Create worker container
   - Verify container running
   - Verify container exits
   - Verify cleanup

2. **Orchestrator + Blackboard**:
   - Real Redis + orchestrator
   - Grant claim to controller
   - Verify worker launched
   - Verify artefact created
   - Verify claim completed

**E2E tests:**

1. **Controller-worker basic flow**:
   ```
   Scenario: Controller bids, worker executes
   Steps:
     1. sett up with controller configuration
     2. sett forage --goal "test goal"
     3. Controller submits bid
     4. Orchestrator launches worker
     5. Worker executes and creates artefact
     6. Worker exits (code 0)
     7. Orchestrator cleans up worker
     8. Verify claim complete
   ```

2. **Max concurrent workers**:
   ```
   Scenario: Enforce max_concurrent limit
   Config: max_concurrent: 2
   Steps:
     1. Submit 3 claims simultaneously
     2. Worker 1 and 2 launch immediately
     3. Claim 3 paused (at limit)
     4. Worker 1 completes and exits
     5. Claim 3 worker launches
     6. Verify only 2 workers active at any time
   ```

3. **Worker failure handling**:
   ```
   Scenario: Worker exits with non-zero code
   Steps:
     1. Configure agent tool to exit(1)
     2. sett forage --goal "failing goal"
     3. Worker launches and fails
     4. Orchestrator detects exit code 1
     5. Failure artefact created
     6. Claim terminated with reason
     7. Worker cleaned up
   ```

4. **Backward compatibility**:
   ```
   Scenario: Traditional agent still works
   Config: No mode specified
   Steps:
     1. sett up with M3.3 config
     2. sett forage --goal "test"
     3. Agent bids and executes (no worker launched)
     4. Workflow completes as M3.3
   ```

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**No new third-party dependencies** beyond existing Docker client.

**Deferred features:**
- Persistent grant queue (M3.5)
- Retry logic on worker failure (Future)
- Dynamic worker scaling (Future)
- Cross-role worker pools (Future)

**Justification**: Controller-worker pattern is required to eliminate bidding race conditions and enable horizontal scaling. The minimal implementation (no queue, no retries) meets current needs.

### **4.2. Auditability**

**New artefacts created:**
- Failure artefacts for worker failures (structural_type: Failure)

**Immutable audit trail enhanced:**
- Worker lifecycle events logged (launch, exit, cleanup)
- Claim termination reasons include worker failure details
- Worker container logs captured in Failure artefact payload

**State changes captured:**
- Worker launch: Logged with container ID and claim ID
- Worker exit: Logged with exit code
- Worker failure: Logged with Failure artefact ID
- Worker cleanup: Logged with container removal

### **4.3. Small, single-purpose components**

**Component responsibilities remain clear:**
- **Orchestrator**: Adds worker lifecycle management (natural extension of agent coordination)
- **Cub**: Adds controller mode (bidder-only) and worker mode (execute-only)
- **Instance Manager**: Updated to pass mode configuration

**No tight coupling introduced:**
- Components communicate via blackboard (Redis)
- Worker lifecycle is orchestrator-internal
- Controllers and workers remain unaware of each other

### **4.4. Security considerations**

**No new security implications:**
- All communication via existing Redis channels
- Workers use same Docker network as M3.3 agents
- No new network exposure

**Data validation:**
- Worker configuration validated at `sett up`
- Container names validated (alphanumeric + hyphens)
- Claim IDs validated before worker launch

### **4.5. Backward compatibility**

**Breaking changes:**

None! M3.4 is fully backward compatible with M3.3.

**Non-breaking changes:**
- New configuration fields (`mode`, `worker`) are optional
- Existing agents without `mode` continue as M3.3 behavior
- No changes to blackboard schema

**Existing workflows preserved:**
- M3.1/M3.2/M3.3 workflows continue working unchanged
- All existing E2E tests must continue passing

### **4.6. Dependency impact**

**Docker API usage:**
- New dependency: `github.com/docker/docker/client` (already used by instance manager)
- 4 API calls per worker lifecycle (Create, Start, Wait, Remove)
- **Impact**: Minimal (Docker API is fast, local)

**Memory usage:**
- ~100 bytes per active worker (WorkerState struct)
- O(1) lookup in activeWorkers map
- **Impact**: Negligible (even 1000 workers = 100KB)

**CPU usage:**
- Worker monitoring goroutine per worker
- **Impact**: Minimal (goroutines are lightweight)

**Build dependencies:**
- No changes to Go version or build tools
- **Impact**: None

## **5. Definition of done**

- [ ] All implementation steps from section 3.2 are complete
- [ ] All tests defined in section 3.4 are implemented and passing
- [ ] Performance requirements from section 3.3 are met and verified
- [ ] Overall test coverage maintained (90%+ for new packages)
- [ ] All M3.1/M3.2/M3.3 E2E tests continue passing (backward compatibility)
- [ ] Configuration validation enforces controller requirements
- [ ] Worker lifecycle fully implemented (launch, monitor, cleanup)
- [ ] Controller mode (bidder-only) works correctly
- [ ] Worker mode (execute-only) works correctly
- [ ] Max concurrent workers limit enforced
- [ ] Worker failure creates Failure artefact
- [ ] Example sett.yml demonstrates controller configuration
- [ ] M3.4 limitations documented (no queue, no retries)
- [ ] All success criteria from section 1.3 are validated
- [ ] All failure modes identified in section 6 have been tested

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Orchestrator failures:**

1. **Docker API unavailable**:
   - **Behavior**: LaunchWorker() fails, logs error
   - **Detection**: Docker client returns connection error
   - **Recovery**: Create Failure artefact, terminate claim
   - **Logging**: "failed to create worker container: {error}"
   - **Termination reason**: "Failed to launch worker: Docker API unavailable"

2. **Container creation fails (out of resources)**:
   - **Behavior**: ContainerCreate() fails with resource error
   - **Detection**: Docker API returns resource limit error
   - **Recovery**: Create Failure artefact, terminate claim
   - **Logging**: "worker launch failed: insufficient resources"
   - **Termination reason**: "Failed to launch worker: insufficient resources"

3. **Max concurrent workers reached**:
   - **Behavior**: determineGrants() returns error, grant paused
   - **Detection**: workersByRole[role] >= maxConcurrent
   - **Recovery**: Claim remains in pending_consensus, retried later
   - **Logging**: "worker_limit_reached: role={role} max={max}"
   - **Impact**: Claim waits for worker slot to become available

4. **Worker container exits immediately (crash)**:
   - **Behavior**: monitorWorker() detects exit code ≠ 0
   - **Detection**: ContainerWait() returns non-zero status
   - **Recovery**: Create Failure artefact with logs, terminate claim
   - **Logging**: "worker_failed: exit_code={code}"
   - **Termination reason**: "Worker failed with exit code {code}"

**Cub failures:**

5. **Worker cannot fetch claim**:
   - **Behavior**: runWorkerMode() calls GetClaim(), returns error
   - **Detection**: Redis connection error or claim not found
   - **Recovery**: Worker exits with code 1, orchestrator creates Failure
   - **Logging**: "Failed to fetch claim: {error}"
   - **Impact**: Claim terminated

6. **Worker tool execution fails**:
   - **Behavior**: executeTool() returns error, worker exits(1)
   - **Detection**: Tool script non-zero exit or stderr output
   - **Recovery**: Orchestrator creates Failure artefact
   - **Logging**: Tool errors captured in container logs
   - **Impact**: Claim terminated

**Configuration failures:**

7. **Controller config missing worker block**:
   - **Behavior**: Config validation fails at `sett up`
   - **Detection**: agent.Mode == "controller" && agent.Worker == nil
   - **Recovery**: None (user must fix config)
   - **Error**: "agent 'X' has mode='controller' but no worker configuration"

8. **Invalid max_concurrent value**:
   - **Behavior**: Config validation fails at `sett up`
   - **Detection**: agent.Worker.MaxConcurrent < 1
   - **Recovery**: None (user must fix config)
   - **Error**: "agent 'X' worker.max_concurrent must be >= 1"

### **6.2. Concurrency considerations**

**Race condition analysis:**

1. **Multiple grants at max_concurrent**:
   - **Possibility**: Two grants check limit simultaneously
   - **Protection**: workerLock RWMutex protects workersByRole
   - **Impact**: At most maxConcurrent+1 workers (brief overshoot)
   - **Safety**: Acceptable, self-corrects when worker exits

2. **Worker exit during new grant**:
   - **Possibility**: Worker exits while grant decision in progress
   - **Protection**: Lock-protected decrement in cleanupWorker()
   - **Impact**: Grant may be paused unnecessarily for one cycle
   - **Safety**: Retried in next consensus round (claim progresses)

3. **Orchestrator restart with active workers**:
   - **Possibility**: Orchestrator loses track of active workers
   - **Protection**: None in M3.4 (deferred to M3.5)
   - **Impact**: Orphaned worker containers
   - **Recovery**: Manual cleanup via `sett down`

### **6.3. Edge case handling**

**Edge case: Controller container crashes**:
- **Behavior**: No bidding for that role, claims remain pending
- **Detection**: Docker health check failure (future)
- **Recovery**: `sett down && sett up` restarts controller
- **Logging**: Container exit logged by Docker

**Edge case: Worker container name collision**:
- **Behavior**: ContainerCreate() fails with "name already in use"
- **Detection**: Docker API returns name conflict error
- **Recovery**: Create Failure artefact, terminate claim
- **Prevention**: Short claim UUID makes collisions extremely unlikely

**Edge case: Worker completes but artefact not created**:
- **Behavior**: Worker exits(0) but no artefact published
- **Detection**: Claim remains in pending_exclusive (timeout in M3.6)
- **Recovery**: Manual investigation required
- **Logging**: Worker logs show execution but no artefact created

**Edge case: max_concurrent = 0**:
- **Behavior**: Config validation rejects (must be >= 1)
- **Detection**: Validation in config.Validate()
- **Recovery**: User sets valid value

**Edge case: Worker image doesn't exist**:
- **Behavior**: ContainerCreate() fails with image not found
- **Detection**: Docker API returns image pull error
- **Recovery**: Create Failure artefact, terminate claim
- **Termination reason**: "Failed to launch worker: image not found"

## **7. Open questions & decisions**

**All questions resolved. Ready for implementation.**

The following decisions have been finalized:

1. ✅ **Configuration approach**: Explicit `mode: "controller"` with nested `worker:` block
2. ✅ **Worker grant delivery**: Command-line arg `cub --execute-claim <claim_id>`
3. ✅ **Worker lifecycle ownership**: Orchestrator manages launch, monitor, cleanup
4. ✅ **Concurrency limit enforcement**: Pause granting when at limit (no queue)
5. ✅ **Worker failure handling**: Non-zero exit → Failure artefact, no retries
6. ✅ **Worker naming**: `sett-{instance}-{agent_name}-worker-{short_claim_uuid}`
7. ✅ **Max concurrent default**: Defaults to 1 if omitted
8. ✅ **Implementation scope**: Logical phases for incremental PRs
9. ✅ **Backward compatibility**: Existing configs without mode continue working
10. ✅ **Resource limits**: Basic enforcement, no queueing (deferred to M3.5)

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**Start with configuration and validation:**
1. Begin with config schema changes (Agent struct, WorkerConfig)
2. Then configuration validation logic
3. Then cub mode detection and flags
4. Then orchestrator worker lifecycle
5. Finally integration and comprehensive testing

**Implement comprehensive error handling from the beginning:**
- All Docker API calls with error checking
- All worker state transitions logged
- Clear Failure artefacts for all error cases
- Graceful cleanup on all error paths

**Write tests before implementation (TDD approach):**
- Start with config validation unit tests
- Then orchestrator worker lifecycle unit tests (mock Docker)
- Then cub mode unit tests
- Finally E2E tests with real Docker

**Use defensive programming:**
- Always lock workersByRole before reading/writing
- Validate claim IDs before launching workers
- Handle Docker API errors gracefully
- Never assume container names are unique

### **8.2. Common pitfalls to avoid**

**Configuration pitfalls:**

1. **Forgetting default max_concurrent**:
   - Always set default to 1 if omitted
   - Test with config that omits max_concurrent
   - Document default in config validation

2. **Not validating worker image**:
   - Check worker.Image != "" during validation
   - Prevent empty image from reaching Docker API

**Worker lifecycle pitfalls:**

3. **Not cleaning up on launch failure**:
   - If ContainerStart() fails, remove created container
   - Use defer cleanup or explicit removal

4. **Race condition in workersByRole**:
   - Always use workerLock before accessing maps
   - Use RWMutex appropriately (read vs write locks)

5. **Forgetting to decrement worker count**:
   - cleanupWorker() MUST decrement workersByRole[role]
   - Test that count returns to 0 after worker exits

**Docker API pitfalls:**

6. **Not handling Docker API retries**:
   - Docker API can be temporarily unavailable
   - Consider wrapping calls with retry logic (optional for M3.4)

7. **Container logs too large**:
   - Use Tail: "100" to limit log size
   - Document that full logs are in Docker

**Grant decision pitfalls:**

8. **Checking limit after launch**:
   - ALWAYS check isAtWorkerLimit() BEFORE granting
   - Test that grants are paused when at limit

9. **Publishing claim event for workers**:
   - Workers don't subscribe to claim_events
   - Skip publishClaimEvent() for controller grants

### **8.3. Integration checklist**

**Pre-implementation verification:**
- [x] All prerequisite features complete (M3.3)
- [x] Configuration schema designed and approved
- [x] Worker lifecycle flow documented
- [x] Cub mode handling designed

**During implementation:**
- [ ] Config fields Mode and WorkerConfig added
- [ ] Config validation enforces controller requirements
- [ ] Default max_concurrent = 1 applied
- [ ] Cub --execute-claim flag added
- [ ] Cub controller mode (bidder-only) implemented
- [ ] Cub worker mode (execute-only) implemented
- [ ] Orchestrator WorkerState tracking implemented
- [ ] Orchestrator LaunchWorker() creates containers
- [ ] Orchestrator monitorWorker() watches exit codes
- [ ] Orchestrator cleanupWorker() removes containers
- [ ] Grant decision checks max_concurrent limit
- [ ] Worker failure creates Failure artefact
- [ ] All M3.3 tests still passing

**Post-implementation:**
- [ ] All E2E tests passing (controller-worker flow, limits, failures)
- [ ] Performance benchmarks met
- [ ] Documentation updated (README, agent guide, troubleshooting)
- [ ] Example controller configuration provided and tested

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Metrics to track:**

1. **Worker lifecycle metrics**:
   - Workers launched per hour
   - Worker success rate (exit code 0 vs non-zero)
   - Average worker execution time
   - Worker launch latency (grant → running)

2. **Concurrency metrics**:
   - Active workers per role (current count)
   - Max concurrent limit hit rate (percentage of grants paused)
   - Worker queue depth (future, M3.5)

3. **Failure metrics**:
   - Worker failure rate (exit code ≠ 0)
   - Docker API failure rate
   - Container creation failures

**Structured logging events:**

```json
// Worker launched
{"level":"info","component":"orchestrator","event":"worker_launching","container_name":"sett-inst-coder-worker-abc12345","claim_id":"uuid","role":"Coder","timestamp":"2025-10-20T00:00:00Z"}

// Worker completed successfully
{"level":"info","component":"orchestrator","event":"worker_completed","container_id":"docker-id","claim_id":"uuid","timestamp":"2025-10-20T00:05:00Z"}

// Worker failed
{"level":"error","component":"orchestrator","event":"worker_failed","container_id":"docker-id","claim_id":"uuid","exit_code":1,"timestamp":"2025-10-20T00:05:00Z"}

// Worker limit reached
{"level":"warn","component":"orchestrator","event":"worker_limit_reached","role":"Coder","max_concurrent":3,"claim_id":"uuid","timestamp":"2025-10-20T00:00:00Z"}

// Worker cleanup
{"level":"info","component":"orchestrator","event":"worker_cleanup","container_id":"docker-id","role":"Coder","timestamp":"2025-10-20T00:06:00Z"}
```

**Health check modifications:**
- No changes to orchestrator `/healthz` endpoint
- Consider adding `/metrics` endpoint for worker counts (future)

**Operator diagnostics:**
- List active workers:
  ```bash
  docker ps --filter "name=sett-*-worker-*"
  ```
- Check worker logs:
  ```bash
  docker logs sett-{instance}-{agent}-worker-{claim}
  ```
- Monitor worker count:
  ```bash
  sett logs orchestrator | grep "worker_launched\|worker_cleanup"
  ```

### **9.2. Rollback and disaster recovery**

**Rollback to M3.3:**

1. **No data migration required**: No blackboard schema changes
2. **Config changes**: Remove `mode` and `worker` from sett.yml
3. **Orchestrator binary**: Rollback to M3.3 orchestrator image
4. **Behavior**: Agents run as single containers (M3.3 behavior)
5. **In-flight workers**: Will be orphaned (manual cleanup required)

**Disaster recovery:**

1. **Orchestrator crash with active workers**:
   - Workers continue running (independent containers)
   - Workers complete and exit normally
   - **Recovery**: `sett down` cleans up all containers including orphans

2. **Worker containers orphaned**:
   - Orchestrator loses tracking state (restart)
   - **Recovery**: `sett down && sett up` removes all workers
   - **Detection**: `docker ps | grep worker`

3. **Max concurrent workers exhausted**:
   - All worker slots full, claims paused
   - **Recovery**: Wait for workers to complete, or increase max_concurrent
   - **Detection**: Orchestrator logs show "worker_limit_reached" events

**Recovery time objectives:**

- Worker launch failure: Immediate (Failure artefact created)
- Worker crash detection: <5 seconds (ContainerWait returns)
- Worker cleanup: <10 seconds (container removal)
- Rollback to M3.3: <5 minutes (image swap + restart)

### **9.3. Documentation and training**

**Documentation updates required:**

1. **README.md**:
   - Add M3.4 status badge
   - Update feature list (controller-worker pattern, horizontal scaling)
   - Document benefits (race condition elimination, resource efficiency)

2. **Phase 3 README**:
   - Mark M3.4 as complete with implementation summary
   - Document controller vs traditional agent behavior

3. **Agent Development Guide**:
   - Add "Controller-Worker Pattern" section
   - Explain when to use controller vs traditional
   - Example controller configuration
   - Explain worker lifecycle (ephemeral, auto-cleanup)

4. **New guide: docs/controller-worker-pattern.md**:
   - Architecture overview
   - Configuration examples
   - Scaling strategies
   - Troubleshooting

5. **Troubleshooting Guide**:
   - Add M3.4-specific entries:
     * Worker launch failures
     * Max concurrent workers reached
     * Worker orphaning
     * Controller container crashes

6. **Example sett.yml**:
   ```yaml
   # Example: M3.4 controller-worker pattern
   version: "1.0"

   agents:
     coder-controller:
       role: "Coder"
       mode: "controller"
       image: "coder:latest"
       bidding_strategy: "exclusive"

       worker:
         image: "coder:latest"
         max_concurrent: 3
         command: ["/app/run.sh"]
         workspace:
           mode: rw
   ```

**Troubleshooting guides:**

1. **Common issues**:
   - "Worker launch failed" → Check Docker resources, image availability
   - "Max concurrent workers reached" → Increase max_concurrent or wait
   - "Worker containers orphaned" → Run `sett down` to cleanup
   - "Controller not bidding" → Check controller logs, verify mode config

2. **Debugging commands**:
   ```bash
   # List active workers
   docker ps --filter "name=sett-*-worker-*"

   # Check worker logs
   docker logs sett-{instance}-{agent}-worker-{claim}

   # Monitor worker lifecycle
   sett logs orchestrator | grep worker

   # Count active workers per role
   docker ps --filter "name=sett-*-worker-*" | wc -l
   ```

**Upgrade notes (M3.3 → M3.4):**

1. **No breaking changes**: M3.4 is fully backward compatible
2. **New capability**: Controller-worker pattern for horizontal scaling
3. **Configuration**: Add `mode: "controller"` and `worker:` block to scale
4. **Migration**: Agents can be migrated incrementally (role by role)
5. **Testing**: Test controller pattern in dev before production

## **10. Self-validation checklist**

### **Before starting implementation:**

- [x] I understand how M3.4 builds on M3.3 (adds controller-worker pattern)
- [x] All success criteria (section 1.3) are measurable and testable
- [x] I have considered every component in section 2 explicitly
- [x] All design decisions (section 3.1) are justified and documented
- [x] Configuration schema is clear and backward compatible
- [x] Worker lifecycle is well-defined (launch → monitor → cleanup)
- [x] Cub modes are unambiguous (controller vs worker vs traditional)
- [x] Orchestrator responsibilities are clear (worker management)

### **During implementation:**

- [ ] I am implementing the simplest solution that meets success criteria
- [ ] All error scenarios (section 6) are being handled, not just happy path
- [ ] Tests are being written before or alongside code (TDD approach)
- [ ] I am validating that M3.3 functionality is not broken (backward compatibility)
- [ ] Logging is comprehensive (worker lifecycle events)
- [ ] Docker API errors are handled gracefully
- [ ] Worker state tracking is thread-safe (locks used correctly)
- [ ] Configuration validation is thorough

### **Before submission:**

- [ ] All items in Definition of Done (section 5) are complete
- [ ] Feature has been tested in a clean environment from scratch
- [ ] Documentation is updated and accurate (README, guides, troubleshooting)
- [ ] I have considered the operational impact (section 9) of this feature
- [ ] All M3.1/M3.2/M3.3 tests pass (backward compatibility validated)
- [ ] E2E tests with controller-worker pattern pass consistently
- [ ] Worker failure tests pass (Failure artefact created)
- [ ] Max concurrent enforcement tests pass
- [ ] Example sett.yml with controller config provided and tested
- [ ] Worker lifecycle (launch, execute, cleanup) verified in E2E tests

---

**Status**: Ready for implementation. All architectural decisions finalized.
