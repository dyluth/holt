# **Feature design: Terraform Module Generator Demo (M3.8)**

**Purpose**: Sophisticated demonstration of hybrid LLM-and-tool workflow for Infrastructure as Code
**Scope**: Self-contained demo showcasing multi-phase orchestration with 6 specialized agents
**Estimated tokens**: ~8,500 tokens
**Read when**: Understanding advanced Holt workflows, building complex multi-agent demos

Associated phase: **Phase 3 (Coordination)** - Demonstrating multi-agent coordination capabilities
Status: **Draft**

***Demo purpose:*** This document specifies a complete, executable demonstration workflow that showcases Holt's Phase 3 multi-agent coordination features (M3.3-M3.7) by orchestrating a realistic DevOps task involving multiple LLM-based agents and tool-based agents working in sequence, with review gates, parallel transformations, and final packaging.

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Create a sophisticated demonstration workflow that showcases Holt's ability to orchestrate a hybrid (LLM + tool) multi-agent workflow for generating, validating, documenting, and packaging a Terraform infrastructure module.

### **1.2. User story**

As a DevOps engineer evaluating Holt for my organization, I want to provide a high-level goal ("Create a Terraform module for S3 static website hosting") and observe a clan of specialized agents automatically draft the HCL code, validate it with multiple linters, generate comprehensive documentation, format the documentation, and package the final module for distribution—all while maintaining a complete audit trail of every step.

This demo proves that Holt can orchestrate realistic, production-quality workflows that combine AI-powered code generation with battle-tested validation tools, demonstrating value for regulated industries that require both automation and auditability.

### **1.3. Success criteria**

1. **Complete Workflow Execution**: Running `holt forage --goal "Create a Terraform module to provision a basic S3 bucket for static website hosting"` produces a final `s3-module.tar.gz` file in the workspace containing:
   - Valid `main.tf` with S3 bucket configuration
   - Properly formatted and linted `README.md` documentation
   - Complete git history showing each agent's contribution

2. **Visible Multi-Phase Orchestration**: The `holt watch` output clearly displays:
   - Sequential review phase (Formatter + Linter)
   - Exclusive phase transition to DocGenerator
   - Parallel transformation phase (MarkdownLint)
   - Final exclusive phase (Packager)
   - Terminal artefact creation ending the workflow

3. **Audit Trail Completeness**: The `holt hoard` command shows a complete chain of artefacts:
   - GoalDefined → CodeCommit (Terraform) → Review (×2) → CodeCommit (README) → CodeCommit (formatted README) → Terminal (package)
   - Each artefact references its source artefacts, creating verifiable provenance

### **1.4. Non-goals**

- **No Cloud Provisioning**: This demo will NOT execute `terraform apply` or provision actual AWS resources. The focus is on code generation, validation, and packaging—not deployment.
- **No Interactive Questions**: This demo does not showcase Question/Answer artefacts or human-in-the-loop approval gates (those are reserved for future demos).
- **No Dynamic Configuration**: Agent configurations are static in `holt.yml`. We will not demonstrate runtime agent registration or dynamic tool selection.
- **No Error Recovery**: This initial demo assumes the happy path. Error handling and failure recovery will be addressed in future iterations.

## **2. The 'what': component impact analysis**

**Critical validation questions for this entire section:**
* Have I explicitly considered EVERY component (Blackboard, Orchestrator, Pup, CLI)? **YES**
* For components marked "No changes" - am I absolutely certain this feature doesn't affect them? **YES - this is a demo using existing capabilities**
* Do my changes maintain the contracts and interfaces defined in the design documents? **YES - no contract modifications**
* Will this feature work correctly with both single-instance and scaled agents (controller-worker pattern)? **YES - demo uses single-instance agents**

### **2.1. Blackboard changes**

**No changes to the Blackboard component.**

This demo uses only existing Holt data structures:
- **Artefacts**: GoalDefined, CodeCommit, Review, Terminal (all existing)
- **Claims**: Standard claim lifecycle with review → parallel → exclusive phases
- **Bids**: Standard bid types (review, claim, exclusive, ignore)
- **Channels**: Uses existing artefact_events, claim_events, workflow_events

**New artefact types** (domain-specific, not structural):
- `type: "TerraformCode"` (structural_type: Standard)
- `type: "TerraformDocumentation"` (structural_type: Standard)
- `type: "PackagedModule"` (structural_type: Terminal)

### **2.2. Orchestrator changes**

**No changes to the Orchestrator component.**

The orchestrator's existing phased execution model (M3.3) handles all workflow logic:
- **Review phase**: Grants claim to TerraformFmt and TfLint simultaneously
- **Phase transition**: Automatically transitions from review → exclusive after approvals
- **Exclusive phase**: Grants claim to DocGenerator after reviews pass
- **Parallel phase**: Grants claim to MarkdownLint for documentation transformation
- **Terminal detection**: Recognizes Terminal artefact and halts workflow

### **2.3. Agent pup changes**

**No changes to the Agent pup component.**

All agents use the standard pup entrypoint with existing contracts:
- **Bid contract**: `bid.sh` reads claim JSON via stdin, outputs bid type to stdout
- **Tool contract**: `run.sh` reads claim + context via stdin, outputs artefact JSON to stdout
- **Git integration**: Agents commit changes and return commit hashes as CodeCommit payloads

### **2.4. CLI changes**

**No changes to the CLI component.**

The demo uses existing CLI commands:
- `holt init` - Initialize workspace
- `holt up` - Start orchestrator and agents
- `holt forage --goal "..."` - Create initial GoalDefined artefact
- `holt watch` - Observe workflow in real-time
- `holt hoard` - View final artefact chain
- `holt down` - Stop instance

---

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

#### **Decision 1: Use Official Third-Party Docker Images**

**Decision**: Base agent images on official, public Docker images:
- `hashicorp/terraform:latest` for TerraformFmt
- `github/super-linter:latest` for TfLint (includes tflint)
- `davidanson/markdownlint-cli2:latest` for MarkdownLint

**Rationale**:
- Demonstrates Holt's container-native philosophy—orchestrate ANY tool that runs in a container
- Proves zero vendor lock-in—agents can wrap standard DevOps tools
- Reduces maintenance burden—leverage upstream tool updates

**Risk**: External image changes could break demo. **Mitigation**: Pin specific image versions in Dockerfiles.

#### **Decision 2: Smart Bidding via ProducedByRole Inspection**

**Decision**: ModulePackager distinguishes between unlinted and linted documentation by inspecting the target artefact's `produced_by_role` field.

**Bid Logic**:
```bash
# ModulePackager only packages documentation produced by DocLinter
if [ "$artefact_type" = "CodeCommit" ] && [ "$produced_by" = "DocLinter" ]; then
    echo "exclusive"
fi
```

**Rationale**:
- Avoids complex context_chain inspection
- Leverages Holt's built-in artefact metadata
- Simple, declarative agent intent

**Risk**: Agents must use consistent role names. **Mitigation**: Document role naming convention in holt.yml.

#### **Decision 3: Sequential Review via Phased Execution**

**Decision**: Both TerraformFmt and TfLint bid "review" on the same claim. The orchestrator's review phase model automatically runs them in parallel and only proceeds to DocGenerator after BOTH approve.

**Rationale**:
- Leverages M3.3's existing review phase consensus logic
- No custom orchestration code needed
- Demonstrates real-world "multiple validation gates" pattern

**Risk**: If ONE review fails, the entire claim is rejected. **Mitigation**: This is desired behavior—all validations must pass.

### **3.2. Implementation steps**

**Phase 1: Project Structure Setup**
1. Create `demos/terraform-generator/` directory
2. Create `demos/terraform-generator/README.md` with demo description
3. Create `demos/terraform-generator/holt.yml` with all 6 agent definitions
4. Create subdirectories for each agent:
   - `agents/terraform-drafter/`
   - `agents/terraform-fmt/`
   - `agents/tflint/`
   - `agents/doc-generator/`
   - `agents/markdown-lint/`
   - `agents/module-packager/`

**Phase 2: Agent Implementation (in dependency order)**

**Agent 1: TerraformDrafter (LLM-based)**
1. Create `Dockerfile` using `golang:alpine` base + pup + OpenAI client
2. Implement `bid.sh`: Bid exclusive on `GoalDefined` artefacts
3. Implement `run.sh`:
   - Call OpenAI API with goal as prompt
   - Generate `main.tf` content
   - Commit file to workspace
   - Output CodeCommit artefact with type `TerraformCode`

**Agent 2: TerraformFmt (Tool-based)**
1. Create `Dockerfile` using `hashicorp/terraform:1.6` base + pup
2. Implement `bid.sh`: Bid review on CodeCommit if payload contains `.tf` extension
3. Implement `run.sh`:
   - Extract commit hash, checkout files
   - Run `terraform fmt -check -recursive`
   - Output Review artefact (approval if exit 0, rejection with stderr if non-zero)

**Agent 3: TfLint (Tool-based)**
1. Create `Dockerfile` using `github/super-linter:v5` base + pup
2. Implement `bid.sh`: Bid review on CodeCommit if payload contains `.tf` extension
3. Implement `run.sh`:
   - Extract commit hash, checkout files
   - Run `tflint`
   - Output Review artefact (approval if exit 0, rejection with output if non-zero)

**Agent 4: DocGenerator (LLM-based)**
1. Create `Dockerfile` using `golang:alpine` base + pup + OpenAI client
2. Implement `bid.sh`: Bid exclusive on CodeCommit artefacts of type `TerraformCode`
3. Implement `run.sh`:
   - Read Terraform code from commit
   - Call OpenAI API to generate README.md (usage, inputs, outputs)
   - Commit README.md to workspace
   - Output CodeCommit artefact with type `TerraformDocumentation`

**Agent 5: MarkdownLint (Tool-based)**
1. Create `Dockerfile` using `davidanson/markdownlint-cli2:latest` base + pup
2. Implement `bid.sh`: Bid claim on CodeCommit if payload contains `.md` extension AND `produced_by_role != "DocLinter"`
3. Implement `run.sh`:
   - Extract commit hash, checkout files
   - Run `markdownlint-cli2-fix` to auto-format
   - Commit formatted README.md
   - Output CodeCommit artefact with type `TerraformDocumentation`

**Agent 6: ModulePackager (Tool-based)**
1. Create `Dockerfile` using `alpine:latest` base + pup + tar
2. Implement `bid.sh`: Bid exclusive on CodeCommit if `produced_by_role == "DocLinter"`
3. Implement `run.sh`:
   - Extract commit hash, checkout files
   - Run `tar -czf s3-module.tar.gz main.tf README.md`
   - Output Terminal artefact with type `PackagedModule`, payload `s3-module.tar.gz`

**Phase 3: Integration & Configuration**
1. Write `holt.yml` defining all 6 agents with proper roles
2. Create `build-all.sh` script to build all Docker images
3. Create `run-demo.sh` script for one-command demo execution

**Phase 4: Testing & Validation**
1. Implement `test-workflow.sh` end-to-end test
2. Create `verify-output.sh` to validate final package contents
3. Run full test suite and verify all success criteria

### **3.3. Performance & resource considerations**

**Resource usage:**
- **CPU**: Each LLM agent requires API calls (~2-5 seconds per call)
- **Memory**: Terraform and linter images are lightweight (<500MB each)
- **Storage**: Final package <10KB, workspace <50KB total
- **Network**: Requires internet for OpenAI API calls and image pulls

**Scalability limits:**
- Demo assumes single-instance agents (no controller-worker pattern)
- Maximum 1 concurrent workflow per Holt instance
- Suitable for demonstration, not high-throughput production use

**Performance requirements:**
- End-to-end workflow completion: **<60 seconds** (acceptable for demo)
- Review phase (both linters): **<10 seconds**
- Documentation generation: **<15 seconds** (LLM API latency)
- Packaging: **<2 seconds**

### **3.4. Testing strategy**

**Unit tests:**
Not applicable for demo. Agents use external tools (terraform, tflint) which have their own test suites.

**Integration tests:**
1. **Bid Logic Tests**: Test each agent's `bid.sh` script with mock claim JSON
   - Verify TerraformFmt bids review on .tf files, ignore on .md files
   - Verify ModulePackager only bids exclusive when `produced_by_role == DocLinter`

**Performance tests:**
Not required for demo. This is a showcase, not a production system.

**E2E tests (holt tests):**

**Test 1: Happy Path Workflow**
```bash
# test-terraform-generator.sh
#!/bin/bash
set -e

# Setup
cd demos/terraform-generator
./build-all.sh
mkdir -p test-workspace && cd test-workspace
git init && git commit --allow-empty -m "init"
holt init
holt up

# Execute workflow
holt forage --goal "Create a Terraform module to provision a basic S3 bucket for static website hosting"

# Wait for completion (poll for Terminal artefact)
timeout 60 bash -c 'until holt hoard | grep -q "PackagedModule"; do sleep 2; done'

# Assertions
test -f s3-module.tar.gz || { echo "ERROR: Package not created"; exit 1; }
tar -tzf s3-module.tar.gz | grep -q "main.tf" || { echo "ERROR: main.tf missing"; exit 1; }
tar -tzf s3-module.tar.gz | grep -q "README.md" || { echo "ERROR: README.md missing"; exit 1; }

# Extract and validate contents
tar -xzf s3-module.tar.gz
terraform fmt -check main.tf || { echo "ERROR: Terraform code not formatted"; exit 1; }
grep -q "bucket" main.tf || { echo "ERROR: Missing S3 bucket resource"; exit 1; }
grep -q "Usage" README.md || { echo "ERROR: README missing usage section"; exit 1; }

# Verify audit trail
holt hoard | grep -q "GoalDefined" || { echo "ERROR: Missing GoalDefined artefact"; exit 1; }
holt hoard | grep -q "TerraformCode" || { echo "ERROR: Missing Terraform artefact"; exit 1; }
holt hoard | grep -q "Review" || { echo "ERROR: Missing Review artefacts"; exit 1; }
holt hoard | grep -q "TerraformDocumentation" || { echo "ERROR: Missing Documentation artefact"; exit 1; }
holt hoard | grep -q "PackagedModule" || { echo "ERROR: Missing Terminal artefact"; exit 1; }

echo "✅ All tests passed!"
holt down
```

**Test 2: Review Rejection Scenario** (Future work)
- Manually create malformed Terraform code
- Verify TfLint rejects with Review artefact
- Verify workflow does NOT proceed to DocGenerator

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**No new dependencies introduced to Holt core.**

This demo exemplifies YAGNI by:
- **Reusing existing Holt components**: No orchestrator modifications, no pup changes
- **Leveraging standard tools**: Uses terraform, tflint, markdownlint-cli (battle-tested, maintained by experts)
- **Container-native approach**: We don't re-implement Terraform formatting or Markdown linting—we wrap the official tools

**New demo-specific dependencies** (confined to demo directory):
- OpenAI API client (for LLM agents) - common choice for AI prototypes
- Official tool Docker images (terraform, tflint, markdownlint) - zero custom logic

**Justification**: We could NOT achieve this demo with Holt alone. We NEED domain-specific tools (Terraform, linters) and AI capabilities (LLM for code generation). These are the EXACT use cases Holt was designed for.

### **4.2. Auditability**

**Complete immutable audit trail is preserved.**

Every step in the workflow creates an immutable artefact:
1. **GoalDefined** (user intent) - "Create Terraform module for S3..."
2. **CodeCommit** (TerraformCode) - commit hash `abc123` with `main.tf`
3. **Review** (TerraformFmt) - approval with timestamp, reviewer role
4. **Review** (TfLint) - approval with timestamp, reviewer role
5. **CodeCommit** (TerraformDocumentation) - commit hash `def456` with `README.md`, references `abc123`
6. **CodeCommit** (formatted doc) - commit hash `ghi789` with linted `README.md`, references `def456`
7. **Terminal** (PackagedModule) - final output `s3-module.tar.gz`, references `ghi789`

**Audit trail value**:
- **Provenance**: Trace final package back to original goal
- **Review history**: See which validators approved the code
- **Blame/credit**: See which agent (LLM or tool) produced each artefact
- **Reproducibility**: Re-run workflow from any intermediate artefact
- **Compliance**: Demonstrate to auditors that code was validated by multiple tools

**Git integration**: Each CodeCommit artefact is a git commit hash, creating a parallel audit trail in Git history viewable via `git log`.

### **4.3. Small, single-purpose components**

**Each agent has a single, well-defined responsibility:**

| Agent | Single Purpose |
|-------|---------------|
| TerraformDrafter | Generate Terraform HCL code from natural language goal |
| TerraformFmt | Validate Terraform code formatting standards |
| TfLint | Validate Terraform code for errors and best practices |
| DocGenerator | Generate README documentation from Terraform code |
| MarkdownLint | Format Markdown documentation to standards |
| ModulePackager | Package validated module for distribution |

**No tight coupling**:
- Agents communicate ONLY via blackboard artefacts
- Each agent can be developed, tested, and modified independently
- Agents can be swapped (e.g., replace TfLint with another validator) without affecting others

**Component boundaries respected**:
- Orchestrator remains ignorant of domain logic (doesn't know what Terraform is)
- Pup remains a thin integration layer (no domain logic)
- Agents contain ALL domain logic (Terraform, Markdown, packaging)

### **4.4. Security considerations**

**Security implications and mitigations:**

1. **LLM API Keys**:
   - **Risk**: Agents require OpenAI API keys, which could leak if improperly handled
   - **Mitigation**: Document that API keys should be passed via environment variables in `holt.yml`, NOT hardcoded. Example: `environment: OPENAI_API_KEY=${OPENAI_API_KEY}`

2. **Code Generation Risk**:
   - **Risk**: LLM-generated Terraform code could be malicious or insecure
   - **Mitigation**: This demo includes TWO validation agents (TerraformFmt + TfLint) that catch syntax errors and common security issues. For production, add additional security scanning agents (e.g., tfsec, checkov).

3. **Container Isolation**:
   - **Risk**: Agents run with read-write workspace access
   - **Mitigation**: Demo agents run as non-root users (UID 1000). For production, implement principle of least privilege (some agents need only read access).

4. **Supply Chain**:
   - **Risk**: Pulling public Docker images (terraform, tflint) could introduce malicious code
   - **Mitigation**: Pin specific image versions by digest (not `:latest` tag). Document image scanning process for production use.

5. **Data Exposure**:
   - **Risk**: Terraform code sent to OpenAI API exposes infrastructure designs
   - **Mitigation**: Document that this demo is for NON-SENSITIVE infrastructure. For production, demonstrate air-gapped LLM usage (local Llama model) in future demos.

**No new attack surfaces** introduced to Holt core—all security considerations are demo-specific.

### **4.5. Backward compatibility**

**This demo is fully additive—zero breaking changes.**

- **No API changes**: Uses existing Blackboard, Orchestrator, Pup, CLI contracts
- **No data structure changes**: Uses existing Artefact, Claim, Bid schemas
- **No workflow changes**: Uses existing phased execution model (review → parallel → exclusive)

**Migration path**: N/A (this is a new demo, not a modification)

**Deprecation**: N/A (nothing is deprecated)

**Compatibility testing**: The E2E test verifies the demo works with current Holt version (Phase 3 M3.7+).

### **4.6. Dependency impact**

**No changes to system dependencies.**

- **Redis**: Demo uses standard Redis operations (no new commands, no schema changes)
- **Docker**: Demo requires Docker 20.10+ (same as existing Holt requirement)
- **Go**: Demo agents can be built with Go 1.21+ (same as existing Holt requirement)
- **Git**: Demo requires Git 2.x+ (same as existing Holt requirement)

**New demo-specific dependencies** (isolated to demo directory):
- OpenAI Go client library (`go-openai`) - only for LLM agents
- External Docker images (terraform, tflint, markdownlint) - runtime dependencies

**CI/CD impact**: None. Demo is self-contained and does not affect Holt build process.

**Development environment**: No changes to `Makefile` or build process for Holt core.

## **5. Definition of done**

*This checklist must be fully satisfied for the milestone to be considered complete.*

- [ ] All implementation steps from section 3.2 are complete.
- [ ] All tests defined in section 3.4 are implemented and passing.
- [ ] Performance requirements from section 3.3 are met and verified (workflow completes in <60s).
- [ ] Overall test coverage has not decreased (demo is isolated, does not affect core coverage).
- [ ] The Makefile has been updated with demo-specific commands (e.g., `make demo-terraform`).
- [ ] All new CLI commands, flags, and holt.yml fields are documented in demo README.
- [ ] The developer onboarding time (git clone to running demo) remains under 10 minutes.
- [ ] All TODOs from this specification have been resolved.
- [ ] All failure modes identified in section 6.1 have been documented and handled.
- [ ] Concurrency considerations from section 6.2 have been addressed.
- [ ] All open questions from section 7 have been resolved or documented as future work.
- [ ] AI agent implementation guidance has been followed and integration checklist completed.
- [ ] Security considerations from section 4.4 have been documented in demo README.
- [ ] Backward compatibility requirements from section 4.5 are satisfied (N/A - additive demo).
- [ ] Dependency impact analysis from section 4.6 has been completed (no core changes).
- [ ] Operational readiness checklist from section 9 is satisfied for demo environment.

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Failure Mode 1: LLM API Unavailable**
- **Trigger**: OpenAI API returns 503 or network timeout
- **Agent behavior**: TerraformDrafter or DocGenerator fails with clear error in logs
- **Orchestrator behavior**: No artefact created, claim remains in pending state indefinitely
- **User impact**: Workflow hangs, user sees no progress in `holt watch`
- **Mitigation**: Document that demo requires internet and valid API key. Future: Implement timeout and retry logic in agents.

**Failure Mode 2: Review Rejection**
- **Trigger**: TfLint detects error in generated Terraform code
- **Agent behavior**: TfLint creates Review artefact with `payload` containing error details
- **Orchestrator behavior**: Detects review rejection, terminates claim, creates feedback claim
- **User impact**: Workflow stops at review phase, no documentation generated
- **Mitigation**: Current behavior is CORRECT (validation gate worked). Future: Implement feedback loop where Drafter retries with linter feedback.

**Failure Mode 3: Git Commit Failure**
- **Trigger**: Agent fails to commit file (e.g., disk full, git not initialized)
- **Agent behavior**: Script exits non-zero, logs error to stderr
- **Orchestrator behavior**: Agent container exits, no artefact created
- **User impact**: Workflow hangs, agent logs show error
- **Mitigation**: E2E test ensures `git init` is run before workflow. Document prerequisite in demo README.

**Failure Mode 4: Redis Connection Lost**
- **Trigger**: Redis container crashes or network partition
- **Agent behavior**: Pup loses connection to blackboard, retries indefinitely
- **Orchestrator behavior**: Orchestrator loses connection, cannot publish events
- **User impact**: Entire workflow halts
- **Mitigation**: Document that demo requires stable Redis. Holt's restart resilience (M3.5+) will handle this in production.

**Failure Mode 5: Malformed LLM Output**
- **Trigger**: LLM generates invalid JSON or non-code response
- **Agent behavior**: Agent fails to parse response, script exits non-zero
- **Orchestrator behavior**: No artefact created, claim remains pending
- **User impact**: Workflow hangs
- **Mitigation**: Implement output validation in agent scripts (verify JSON structure, check for code blocks). Log raw LLM response for debugging.

### **6.2. Concurrency considerations**

**Single-Instance Agents**:
This demo uses single-instance agents (no controller-worker pattern). Concurrency is not a concern for the demo scope.

**Multiple Reviewers**:
TerraformFmt and TfLint run concurrently during the review phase. This is SAFE because:
- Both agents have read-only access to the CodeCommit artefact
- Neither agent modifies shared state (they create independent Review artefacts)
- Orchestrator waits for BOTH reviews before proceeding

**Sequential Claim Processing**:
Claims are processed one at a time by the orchestrator. If a second user runs `holt forage` during an active workflow:
- A second GoalDefined artefact is created
- A second claim is created
- TerraformDrafter may bid on both claims
- **Risk**: Single-instance agent can only process one claim at a time
- **Mitigation**: Document that demo is single-workflow-at-a-time. Future: Demonstrate controller-worker pattern for concurrent workflows.

**No Race Conditions**:
All agents operate on immutable artefacts (git commit hashes). There are no shared resources that require locking or synchronization.

### **6.3. Edge case handling**

**Edge Case 1: Empty or Invalid Goal**
- **Scenario**: User runs `holt forage --goal ""`
- **Expected behavior**: TerraformDrafter receives empty goal, LLM may refuse or generate generic code
- **Handling**: Validate goal is non-empty in agent script. Return Failure artefact if goal is invalid.

**Edge Case 2: LLM Generates Non-Terraform Code**
- **Scenario**: LLM misunderstands goal and generates Python code instead of HCL
- **Expected behavior**: TerraformFmt and TfLint reject the code (no `.tf` files)
- **Handling**: Current behavior is correct (validation catches this). Log clear error in Review artefact payload.

**Edge Case 3: Very Large Terraform Modules**
- **Scenario**: LLM generates 10+ files, 1000+ lines of code
- **Expected behavior**: Git commit succeeds, linters run longer, package is larger
- **Handling**: Demo should handle this gracefully. If linters timeout, document timeout configuration.

**Edge Case 4: Markdown Linting Produces No Changes**
- **Scenario**: DocGenerator produces already-perfect Markdown, MarkdownLint has nothing to fix
- **Expected behavior**: MarkdownLint still creates a new CodeCommit (even if content is identical)
- **Handling**: Current behavior is acceptable. Git detects identical content (no actual commit created), agent references original commit hash.

**Edge Case 5: Tar Command Failure**
- **Scenario**: Packager fails to create tar.gz (e.g., disk full)
- **Expected behavior**: Script exits non-zero, logs error
- **Handling**: Document disk space requirements (<1MB). Validate tar command success before outputting Terminal artefact.

## **7. Open questions & decisions**

**Question 1: LLM Provider Choice**
- **Question**: Should we use OpenAI, Anthropic Claude, or local Llama models?
- **Decision**: Start with OpenAI (GPT-4) for demo simplicity. Document that agents can be swapped to use Claude API or local models with minimal changes (just update API client in agent script).
- **Rationale**: OpenAI has the most accessible API for demos. Claude and local models can be future variants.

**Question 2: Error Recovery Strategy**
- **Question**: Should we implement feedback loops where rejected code is sent back to Drafter with linter feedback?
- **Decision**: NO for initial demo. Keep workflow linear (happy path only). Document feedback loop as Phase 4 feature (M3.6+).
- **Rationale**: Feedback loops add significant complexity (iteration count, termination conditions). Save for advanced demo.

**Question 3: Terraform Provider Choice**
- **Question**: Should demo use AWS, Azure, GCP, or provider-agnostic code?
- **Decision**: Use AWS S3 for demo (most recognizable). Include provider configuration in generated code.
- **Rationale**: AWS S3 is widely understood, free tier available for testing. No actual resources will be created (no `terraform apply`).

**Question 4: Documentation Scope**
- **Question**: How comprehensive should the generated README be?
- **Decision**: Include minimal sections: Description, Usage, Inputs, Outputs. Do NOT include Examples (would require working AWS credentials).
- **Rationale**: Keeps demo simple while demonstrating value. Advanced documentation generation can be future enhancement.

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**For AI agents implementing this demo:**

1. **Start with Static Mock Agent**: Before implementing full LLM integration, create a TerraformDrafter that outputs a HARDCODED `main.tf` file. Verify the full workflow works end-to-end with static code.

2. **Implement Comprehensive Error Handling**: Every external call (LLM API, git command, file operation) must have error handling. Log errors to stderr, exit non-zero on failure.

3. **Write Validation Before Integration**: Create test scripts that validate each agent's output:
   - `validate-terraform.sh` - runs `terraform validate` on generated code
   - `validate-markdown.sh` - checks README has required sections

4. **Use Defensive Programming**:
   ```bash
   # Bad: Assume LLM always returns valid JSON
   response=$(curl -s $OPENAI_API)
   code=$(echo "$response" | jq -r '.choices[0].message.content')

   # Good: Validate every step
   response=$(curl -s $OPENAI_API) || { echo "ERROR: API call failed"; exit 1; }
   echo "$response" | jq empty || { echo "ERROR: Invalid JSON"; exit 1; }
   code=$(echo "$response" | jq -r '.choices[0].message.content')
   [ -n "$code" ] || { echo "ERROR: Empty response"; exit 1; }
   ```

5. **TDD Approach**: Write bid logic tests BEFORE implementing agents:
   ```bash
   # test-bids.sh
   echo '{"type":"GoalDefined"}' | ./terraform-drafter/bid.sh  # expect: exclusive
   echo '{"type":"CodeCommit","payload":"abc.tf"}' | ./terraform-fmt/bid.sh  # expect: review
   ```

### **8.2. Common pitfalls to avoid**

**Pitfall 1: Forgetting to Quote File Paths in Scripts**
```bash
# Bad: Breaks if workspace has spaces
cd $WORKSPACE

# Good: Always quote
cd "$WORKSPACE"
```

**Pitfall 2: Not Checking Git Repository Status**
```bash
# Bad: Assume git is initialized
git add main.tf

# Good: Verify repository exists
[ -d .git ] || { echo "ERROR: Not a git repository"; exit 1; }
git add main.tf
```

**Pitfall 3: Hardcoding Agent Roles**
```bash
# Bad: Role name might change
if [ "$produced_by" = "MarkdownLint" ]; then

# Good: Use the role defined in holt.yml (passed via pup)
if [ "$produced_by" = "$EXPECTED_ROLE" ]; then
```

**Pitfall 4: Not Setting Git User for Commits**
```bash
# Bad: Commit might fail if git user.name not set
git commit -m "Add Terraform code"

# Good: Set git identity in agent script
git config user.name "TerraformDrafter"
git config user.email "drafter@holt.local"
git commit -m "Add Terraform code"
```

**Pitfall 5: Breaking Existing Workflows**
- This demo is ISOLATED—do not modify core Holt files
- All demo code must live in `demos/terraform-generator/`
- Test that existing Holt examples (recipe demo, echo agent) still work

### **8.3. Integration checklist**

**Pre-implementation verification:**
- [ ] All prerequisite features are complete (Phase 3 M3.7 phased execution)
- [ ] No breaking changes to existing contracts (using standard Artefact/Claim schemas)
- [ ] New data structures are backward compatible (new types are domain-specific, not structural)
- [ ] All component interfaces remain stable (no orchestrator, pup, or CLI changes)

**During implementation:**
- [ ] Each agent builds independently (`docker build` succeeds for each agent)
- [ ] Each agent's bid.sh script is tested in isolation
- [ ] Each agent's run.sh script is tested with mock input
- [ ] Git commits are verified to have proper author and message
- [ ] All file operations use quoted paths and check for errors

**Post-implementation:**
- [ ] Full workflow E2E test passes (`test-workflow.sh` succeeds)
- [ ] Watch output is readable and shows all phases clearly
- [ ] Hoard output shows complete artefact chain with proper references
- [ ] Final package extracts correctly and contains expected files
- [ ] Demo README documents all prerequisites (API keys, Docker, etc.)

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Demo Monitoring (Manual)**:

Since this is a demonstration, formal monitoring is not required. However, the demo should provide clear observability:

1. **Real-Time Progress**: `holt watch` output shows:
   - Bid submissions from all agents
   - Claim grants for each phase (review, exclusive, parallel)
   - Review approvals/rejections
   - Artefact creation events
   - Terminal artefact signaling completion

2. **Post-Execution Audit**: `holt hoard` command shows:
   - Complete artefact chain
   - Each artefact's type, producer, timestamp
   - Source artefact references (provenance)

3. **Agent Logs**: `holt logs <agent-name>` shows:
   - Bid decisions (why agent bid ignore/review/exclusive)
   - Tool execution output (terraform/tflint stdout)
   - Error messages (if any step fails)

4. **Git History**: `git log --oneline` shows:
   - Commit sequence matching artefact chain
   - Each commit message authored by agent
   - Complete code history

**Metrics to Track** (for demo evaluation):
- **Workflow duration**: Time from `holt forage` to Terminal artefact (<60s target)
- **Phase durations**: Review phase (<10s), Documentation (<15s), Packaging (<2s)
- **Success rate**: Percentage of demos that complete successfully (target: 95%+)

**No health checks needed**: Demo is short-lived and self-terminating.

### **9.2. Rollback and disaster recovery**

**Demo Rollback**:

Not applicable—demo does not modify production systems. However, for demo reset:

1. **Workspace Reset**: Delete and re-create demo workspace
   ```bash
   cd demos/terraform-generator
   rm -rf test-workspace
   mkdir test-workspace && cd test-workspace
   git init && git commit --allow-empty -m "init"
   ```

2. **Holt Instance Reset**: Stop and restart Holt
   ```bash
   holt down --name demo
   holt up --name demo
   ```

3. **Redis Cleanup**: If blackboard state is corrupted
   ```bash
   docker exec holt-demo-redis redis-cli FLUSHDB
   ```

**No data migration needed**: Demo is stateless (each run is independent).

**Recovery Time**: <2 minutes to reset and restart demo.

### **9.3. Documentation and training**

**Demo Documentation** (to be created):

1. **`demos/terraform-generator/README.md`** (Primary):
   - Purpose: "Showcase hybrid LLM-tool workflow for IaC generation"
   - Prerequisites: Docker, Git, OpenAI API key
   - Quick start: `./run-demo.sh`
   - Architecture diagram: Show 6 agents and workflow phases
   - Expected output: Screenshot of `holt watch` and final package
   - Troubleshooting: Common issues (API key missing, git not initialized)

2. **Agent-Specific READMEs**: Each agent directory has `README.md` explaining:
   - Purpose (what does this agent do?)
   - Bid logic (when does it bid review/claim/exclusive?)
   - Tool used (terraform, tflint, etc.)
   - Inputs/outputs (what artefacts does it consume/produce?)

3. **`build-all.sh`**: Script to build all Docker images with clear output
   ```bash
   echo "Building TerraformDrafter..."
   docker build -t terraform-drafter:latest agents/terraform-drafter/
   echo "✓ TerraformDrafter built"
   ```

4. **`run-demo.sh`**: One-command demo execution
   ```bash
   #!/bin/bash
   # Runs the complete Terraform Generator demo
   ./build-all.sh
   holt init
   holt up
   holt forage --goal "Create a Terraform module to provision a basic S3 bucket for static website hosting"
   holt watch  # User watches workflow execute
   ```

**Training Materials** (for demo presentations):

- **Slide deck** explaining workflow phases (review → exclusive → parallel → exclusive → terminal)
- **Video recording** of demo execution with narration
- **FAQ document** answering common questions:
  - Q: "Can this run without internet?" A: "No, requires OpenAI API (future: local LLM demo)"
  - Q: "Does this provision real AWS resources?" A: "No, generates code only"
  - Q: "How do I add more validators?" A: "Add new agents with review bids in holt.yml"

**No team training needed**: Demo is self-service for evaluation purposes.

## **10. Self-validation checklist**

### **Before starting implementation:**

- [x] I understand how this feature aligns with the current phase (Phase 3 - demonstrates multi-agent coordination with review/parallel/exclusive phases)
- [x] All success criteria (section 1.3) are measurable and testable (package exists, workflow visible, audit trail complete)
- [x] I have considered every component in section 2 explicitly (Blackboard, Orchestrator, Pup, CLI - all "no changes")
- [x] All design decisions (section 3.1) are justified and documented (third-party images, smart bidding, phased execution)

### **During implementation:**

- [ ] I am implementing the simplest solution that meets success criteria (no feedback loops, no error recovery, happy path only)
- [ ] All error scenarios (section 6) are being handled, not just happy path (API failures logged, git errors caught)
- [ ] Tests are being written before or alongside code (bid logic tests, E2E test script)
- [ ] I am validating that existing functionality is not broken (demo is isolated in demos/ directory)

### **Before submission:**

- [ ] All items in Definition of Done (section 5) are complete (all checkboxes verified)
- [ ] Feature has been tested in a clean environment from scratch (`./test-workflow.sh` passes on fresh clone)
- [ ] Documentation is updated and accurate (demo README, agent READMEs, troubleshooting guide)
- [ ] I have considered the operational impact (section 9) of this feature (monitoring via watch/hoard, reset via workspace cleanup)

---

**End of Design Document**

This specification provides a complete, implementable design for the Terraform Module Generator demo, showcasing Holt's sophisticated multi-agent orchestration capabilities while adhering to all core principles (YAGNI, auditability, single-purpose components, security). The demo uses ONLY existing Holt features (no core changes) and serves as a reference implementation for building complex, real-world workflows with hybrid LLM-and-tool agents.
