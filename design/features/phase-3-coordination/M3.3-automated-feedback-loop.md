# **Feature design: Automated Feedback Loop**

**Purpose**: Enable automated feedback loop with review-based claim reassignment
**Scope**: Phase 3 Coordination - Milestone 3
**Estimated tokens**: ~6,000 tokens
**Read when**: Implementing automated feedback, review iteration workflows

Associated phase: **Coordination (Phase 3)**
Status: **Draft**

***Template purpose:*** This document is a blueprint for M3.3, an implementable milestone that builds on M3.2's review phase execution to add automated feedback loops. It provides an unambiguous specification for implementing automatic reassignment of work to agents based on review feedback.

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Enable the orchestrator to automatically reassign work to an agent when review feedback is provided, allowing iterative refinement of work without manual intervention until reviewers approve or iteration limits are reached.

### **1.2. User story**

As a Sett user, I want the system to automatically handle review feedback loops so that when a reviewer rejects work (e.g., "needs tests", "fix bug in line 42"), the orchestrator:
1. Automatically creates a new claim assigned to the original agent (no bidding)
2. Provides the agent with both the original work and all review feedback as context
3. Agent creates a new version of the artefact (v2, v3, etc.)
4. New version goes through the review cycle again
5. Process repeats until either reviewers approve OR max iterations is reached
6. If max iterations reached, workflow terminates with clear failure reason

This eliminates manual "re-run sett forage" steps and creates a complete audit trail of the iterative refinement process.

### **1.3. Success criteria**

1. **Automatic feedback claim creation**: When review is rejected, orchestrator creates feedback claim without manual intervention
2. **Context reconstruction**: Cub correctly builds context chain including original work + all Review artefacts
3. **Agent reassignment**: Original agent (by role) is automatically assigned the rework without bidding
4. **Version increment**: Agent creates new artefact with same type/logical_id but incremented version (A(v1) → A(v2))
5. **Iteration cycle**: New version automatically gets new regular claim, goes through review again
6. **Multiple reviewer feedback**: All reviewer feedback consolidated into single feedback claim
7. **Iteration limit enforcement**: After max iterations (configurable), claim terminates with Failure artefact
8. **Audit trail completeness**: sett hoard shows complete iteration history with termination reasons
9. **Configuration validation**: sett up validates orchestrator.max_review_iterations setting
10. **Backward compatibility**: M3.2 workflows without feedback loops continue working unchanged

### **1.4. Non-goals**

- **Human intervention during feedback loop** (Phase 4): No Questions/Answers during automated iteration
- **LLM-based feedback interpretation** (Phase 4): Feedback is treated as opaque context, not semantically understood
- **Selective reviewer subset re-review** (Future): All original reviewers re-review each version
- **Parallel feedback branches** (Future): Only linear iteration supported (v1→v2→v3, not branching)
- **Dynamic iteration limits per artefact type** (Future): Single global max_review_iterations setting
- **Feedback quality scoring** (Future): No mechanism to prioritize or weight reviewer feedback

## **2. The 'what': component impact analysis**

### **2.1. Blackboard changes**

**Schema changes required**: The Claim data structure needs two new fields.

#### **2.1.1. New Claim fields**

```go
type Claim struct {
    ID                      string       `json:"id"`
    ArtefactID              string       `json:"artefact_id"`
    Status                  ClaimStatus  `json:"status"`
    GrantedReviewAgents     []string     `json:"granted_review_agents,omitempty"`
    GrantedParallelAgents   []string     `json:"granted_parallel_agents,omitempty"`
    GrantedExclusiveAgent   string       `json:"granted_exclusive_agent,omitempty"`

    // M3.3: New fields for feedback loop support
    AdditionalContextIDs    []string     `json:"additional_context_ids,omitempty"` // Review artefact IDs for feedback claims
    TerminationReason       string       `json:"termination_reason,omitempty"`      // Explicit reason when status=terminated
}
```

**Field semantics:**

- **`AdditionalContextIDs`**: Used only for feedback claims. Contains IDs of Review artefacts that provided feedback. Empty for regular claims. The Cub will fetch these artefacts and include them in the context_chain alongside the target's source_artefacts.

- **`TerminationReason`**: Populated when claim transitions to `terminated`. Provides explicit, human-readable reason for termination. Makes audit trail immediately clear without graph traversal.

**Examples:**
```
termination_reason: "Terminated due to negative review feedback. See artefacts: [uuid1, uuid2]"
termination_reason: "Terminated after reaching max review iterations (3)."
termination_reason: "Terminated due to agent failure. See Failure artefact: [uuid3]"
```

#### **2.1.2. New ClaimStatus value**

```go
const (
    ClaimStatusPendingConsensus  ClaimStatus = "pending_consensus"  // M3.1
    ClaimStatusPendingReview      ClaimStatus = "pending_review"     // M3.2
    ClaimStatusPendingParallel    ClaimStatus = "pending_parallel"   // M3.2
    ClaimStatusPendingExclusive   ClaimStatus = "pending_exclusive"  // M3.1
    ClaimStatusPendingAssignment  ClaimStatus = "pending_assignment" // M3.3: Feedback claim, bypasses bidding
    ClaimStatusComplete           ClaimStatus = "complete"
    ClaimStatusTerminated         ClaimStatus = "terminated"
)
```

**`pending_assignment` semantics:**
- Claim has already been assigned to a specific agent (in `granted_exclusive_agent`)
- No bidding occurs
- Published to standard `claim_events` channel
- Agent's existing grant-watching logic picks it up automatically
- Transitions to `complete` when agent produces artefact (same as exclusive phase)

### **2.2. Orchestrator changes**

**Major enhancement**: The orchestrator gains feedback loop detection and claim reassignment logic.

#### **2.2.1. Review rejection detection (enhanced)**

Update `CheckReviewPhaseCompletion()` in `internal/orchestrator/review_phase.go`:

```go
func (e *Engine) checkReviewPhaseCompletion(ctx context.Context, claim *blackboard.Claim, phaseState *PhaseState) {
    // Check if all granted review agents have submitted
    if len(phaseState.ReceivedArtefacts) < len(phaseState.GrantedAgents) {
        return // Still waiting for reviews
    }

    // Fetch all Review artefacts and check for feedback
    var feedbackArtefacts []*blackboard.Artefact
    for agentRole, artefactID := range phaseState.ReceivedArtefacts {
        artefact, err := e.client.GetArtefact(ctx, artefactID)
        if err != nil {
            e.logError("failed to fetch review artefact", err)
            continue
        }

        if !isApproval(artefact.Payload) {
            feedbackArtefacts = append(feedbackArtefacts, artefact)
            e.logEvent("review_rejection", map[string]interface{}{
                "claim_id":    claim.ID,
                "reviewer":    agentRole,
                "artefact_id": artefact.ID,
            })
        }
    }

    if len(feedbackArtefacts) > 0 {
        // M3.3: Create feedback claim instead of just terminating
        if err := e.CreateFeedbackClaim(ctx, claim, feedbackArtefacts); err != nil {
            e.logError("failed to create feedback claim", err)
        }

        // Terminate original claim with reason
        claim.Status = blackboard.ClaimStatusTerminated
        claim.TerminationReason = formatReviewRejectionReason(feedbackArtefacts)
        e.client.UpdateClaim(ctx, claim)
        delete(e.phaseStates, claim.ID)

        e.logEvent("claim_terminated_review_feedback", map[string]interface{}{
            "claim_id":           claim.ID,
            "feedback_artefacts": extractIDs(feedbackArtefacts),
        })
    } else {
        // All approved - transition to next phase (existing M3.2 logic)
        e.transitionToNextPhase(ctx, claim, phaseState)
    }
}
```

#### **2.2.2. Feedback claim creation logic**

Create new file `internal/orchestrator/feedback_loop.go`:

```go
// CreateFeedbackClaim creates a new claim assigned to the original producer of the rejected artefact.
// This bypasses bidding and provides the agent with feedback context for rework.
func (e *Engine) CreateFeedbackClaim(ctx context.Context, originalClaim *blackboard.Claim, feedbackArtefacts []*blackboard.Artefact) error {
    // Fetch the original artefact that was reviewed
    targetArtefact, err := e.client.GetArtefact(ctx, originalClaim.ArtefactID)
    if err != nil {
        return fmt.Errorf("failed to fetch target artefact: %w", err)
    }

    // Check iteration limit using version number
    iterationCount := targetArtefact.Version - 1
    if iterationCount >= e.config.Orchestrator.MaxReviewIterations {
        // Create Failure artefact and terminate
        return e.terminateMaxIterations(ctx, originalClaim, targetArtefact, iterationCount)
    }

    // Find the agent that produced the original artefact (reverse-lookup by role)
    producerAgent, err := e.findAgentByRole(targetArtefact.ProducedByRole)
    if err != nil {
        // Agent no longer exists in config
        return e.terminateMissingAgent(ctx, originalClaim, targetArtefact)
    }

    // Extract Review artefact IDs for additional context
    reviewIDs := make([]string, len(feedbackArtefacts))
    for i, art := range feedbackArtefacts {
        reviewIDs[i] = art.ID
    }

    // Create feedback claim
    feedbackClaim := &blackboard.Claim{
        ID:                    uuid.NewString(),
        ArtefactID:            targetArtefact.ID, // Target is the original work, not the Review
        Status:                blackboard.ClaimStatusPendingAssignment,
        GrantedExclusiveAgent: producerAgent,
        AdditionalContextIDs:  reviewIDs, // Inject Review artefacts into context
    }

    if err := e.client.CreateClaim(ctx, feedbackClaim); err != nil {
        return fmt.Errorf("failed to create feedback claim: %w", err)
    }

    e.logEvent("feedback_claim_created", map[string]interface{}{
        "feedback_claim_id": feedbackClaim.ID,
        "original_claim_id": originalClaim.ID,
        "target_artefact":   targetArtefact.ID,
        "assigned_agent":    producerAgent,
        "review_artefacts":  reviewIDs,
        "iteration":         iterationCount + 1,
    })

    // Publish to claim_events channel (standard flow)
    if err := e.publishClaimNotification(ctx, feedbackClaim.ID); err != nil {
        log.Printf("[Orchestrator] Failed to publish feedback claim notification: %v", err)
    }

    return nil
}

// findAgentByRole performs reverse-lookup in agent registry
func (e *Engine) findAgentByRole(role string) (string, error) {
    for agentName, agentRole := range e.agentRegistry {
        if agentRole == role {
            return agentName, nil
        }
    }
    return "", fmt.Errorf("no agent found with role '%s'", role)
}

// terminateMaxIterations creates Failure artefact when iteration limit is reached
func (e *Engine) terminateMaxIterations(ctx context.Context, claim *blackboard.Claim, artefact *blackboard.Artefact, iterations int) error {
    failurePayload := fmt.Sprintf("Max review iterations (%d) reached for artefact %s (version %d). Review feedback loop terminated.",
        e.config.Orchestrator.MaxReviewIterations, artefact.ID, artefact.Version)

    failure := &blackboard.Artefact{
        ID:               uuid.NewString(),
        LogicalID:        uuid.NewString(),
        Version:          1,
        StructuralType:   blackboard.StructuralTypeFailure,
        Type:             "MaxIterationsExceeded",
        Payload:          failurePayload,
        SourceArtefacts:  []string{artefact.ID},
        ProducedByRole:   "orchestrator",
    }

    if err := e.client.CreateArtefact(ctx, failure); err != nil {
        return fmt.Errorf("failed to create Failure artefact: %w", err)
    }

    claim.Status = blackboard.ClaimStatusTerminated
    claim.TerminationReason = fmt.Sprintf("Terminated after reaching max review iterations (%d).", iterations+1)

    e.logEvent("claim_terminated_max_iterations", map[string]interface{}{
        "claim_id":        claim.ID,
        "artefact_id":     artefact.ID,
        "iterations":      iterations + 1,
        "failure_id":      failure.ID,
    })

    return e.client.UpdateClaim(ctx, claim)
}

// terminateMissingAgent creates Failure artefact when original agent no longer exists
func (e *Engine) terminateMissingAgent(ctx context.Context, claim *blackboard.Claim, artefact *blackboard.Artefact) error {
    failurePayload := fmt.Sprintf("Cannot create feedback claim: agent with role '%s' no longer exists in configuration.",
        artefact.ProducedByRole)

    failure := &blackboard.Artefact{
        ID:               uuid.NewString(),
        LogicalID:        uuid.NewString(),
        Version:          1,
        StructuralType:   blackboard.StructuralTypeFailure,
        Type:             "MissingAgentConfiguration",
        Payload:          failurePayload,
        SourceArtefacts:  []string{artefact.ID},
        ProducedByRole:   "orchestrator",
    }

    if err := e.client.CreateArtefact(ctx, failure); err != nil {
        return fmt.Errorf("failed to create Failure artefact: %w", err)
    }

    claim.Status = blackboard.ClaimStatusTerminated
    claim.TerminationReason = fmt.Sprintf("Terminated due to missing agent configuration (role: %s).", artefact.ProducedByRole)

    e.logEvent("claim_terminated_missing_agent", map[string]interface{}{
        "claim_id":         claim.ID,
        "missing_role":     artefact.ProducedByRole,
        "failure_id":       failure.ID,
    })

    return e.client.UpdateClaim(ctx, claim)
}

// formatReviewRejectionReason creates human-readable termination reason
func formatReviewRejectionReason(feedbackArtefacts []*blackboard.Artefact) string {
    ids := make([]string, len(feedbackArtefacts))
    for i, art := range feedbackArtefacts {
        ids[i] = art.ID
    }
    return fmt.Sprintf("Terminated due to negative review feedback. See artefacts: %v", ids)
}
```

#### **2.2.3. Grant notification for assigned claims**

Update grant detection in agent cubs to handle `pending_assignment` status:

```go
// In internal/cub/engine.go, handleClaimEvent()

func (e *Engine) handleClaimEvent(ctx context.Context, claim *blackboard.Claim) {
    // Check if this claim is granted to us
    isGranted := false

    switch claim.Status {
    case blackboard.ClaimStatusPendingReview:
        isGranted = contains(claim.GrantedReviewAgents, e.config.AgentName)

    case blackboard.ClaimStatusPendingParallel:
        isGranted = contains(claim.GrantedParallelAgents, e.config.AgentName)

    case blackboard.ClaimStatusPendingExclusive:
        isGranted = (claim.GrantedExclusiveAgent == e.config.AgentName)

    case blackboard.ClaimStatusPendingAssignment:  // M3.3: Handle feedback claims
        isGranted = (claim.GrantedExclusiveAgent == e.config.AgentName)

    default:
        return // Not a status we handle
    }

    if !isGranted {
        return
    }

    // Execute work...
}
```

### **2.3. Agent cub changes**

**Major enhancement**: The cub's context building logic must incorporate `additional_context_ids`.

#### **2.3.1. Context chain assembly (enhanced)**

Update context building in `internal/cub/context.go`:

```go
// buildContextChain constructs the full context for tool execution.
// For regular claims: traverses target_artefact.source_artefacts
// For feedback claims: also includes claim.additional_context_ids (Review artefacts)
func (e *Engine) buildContextChain(ctx context.Context, claim *blackboard.Claim, targetArtefact *blackboard.Artefact) ([]*blackboard.Artefact, error) {
    const maxDepth = 10 // Safety limit for recursion

    visited := make(map[string]bool)
    queue := []string{}

    // Initialize queue with target's source artefacts
    queue = append(queue, targetArtefact.SourceArtefacts...)

    // M3.3: Add additional context IDs for feedback claims
    if len(claim.AdditionalContextIDs) > 0 {
        queue = append(queue, claim.AdditionalContextIDs...)
        log.Printf("[Cub] Feedback claim detected, adding %d Review artefacts to context", len(claim.AdditionalContextIDs))
    }

    var contextChain []*blackboard.Artefact
    depth := 0

    for len(queue) > 0 && depth < maxDepth {
        artefactID := queue[0]
        queue = queue[1:]

        if visited[artefactID] {
            continue
        }
        visited[artefactID] = true

        // Fetch artefact
        art, err := e.bbClient.GetArtefact(ctx, artefactID)
        if err != nil {
            log.Printf("[Cub] Warning: failed to fetch artefact %s: %v", artefactID, err)
            continue
        }

        // For each unique logical_id, fetch the latest version from thread
        latestVersion, err := e.fetchLatestVersion(ctx, art.LogicalID)
        if err != nil {
            log.Printf("[Cub] Warning: failed to fetch latest version of %s: %v", art.LogicalID, err)
            latestVersion = art // Fall back to current version
        }

        contextChain = append(contextChain, latestVersion)

        // Recursively add source artefacts to queue
        queue = append(queue, latestVersion.SourceArtefacts...)
        depth++
    }

    // Sort newest to oldest (by creation timestamp or version)
    sort.Slice(contextChain, func(i, j int) bool {
        return contextChain[i].Version > contextChain[j].Version
    })

    // Deduplicate by logical_id (keep only latest version)
    deduplicated := make([]*blackboard.Artefact, 0, len(contextChain))
    seenLogicalIDs := make(map[string]bool)

    for _, art := range contextChain {
        if !seenLogicalIDs[art.LogicalID] {
            deduplicated = append(deduplicated, art)
            seenLogicalIDs[art.LogicalID] = true
        }
    }

    return deduplicated, nil
}

// fetchLatestVersion retrieves the latest version of an artefact from its thread ZSET
func (e *Engine) fetchLatestVersion(ctx context.Context, logicalID string) (*blackboard.Artefact, error) {
    // Query thread ZSET for latest version
    threadKey := fmt.Sprintf("sett:%s:thread:%s", e.config.InstanceName, logicalID)

    // Get highest scored entry (latest version)
    results, err := e.redisClient.ZRevRange(ctx, threadKey, 0, 0).Result()
    if err != nil || len(results) == 0 {
        return nil, fmt.Errorf("no versions found for logical_id %s", logicalID)
    }

    latestArtefactID := results[0]
    return e.bbClient.GetArtefact(ctx, latestArtefactID)
}
```

#### **2.3.2. Automatic version management for feedback claims**

The Cub is responsible for automatically managing versioning when processing feedback claims. This keeps agents simple and unaware of versioning concerns.

**Detection logic:**

```go
// In internal/cub/engine.go, Work Executor

func (e *Engine) executeWork(ctx context.Context, claim *blackboard.Claim, targetArtefact *blackboard.Artefact) error {
    // ... execute agent tool, get output ...

    // M3.3: Automatic version management for feedback claims
    var newArtefact *blackboard.Artefact

    if claim.Status == blackboard.ClaimStatusPendingAssignment {
        // Feedback claim: create new version of target_artefact
        newArtefact = e.createReworkArtefact(claim, targetArtefact, toolOutput)
    } else {
        // Regular claim: create new work
        newArtefact = e.createNewArtefact(claim, targetArtefact, toolOutput)
    }

    // ... publish artefact to blackboard ...
}
```

**Rework artefact creation:**

```go
// createReworkArtefact builds a new version of the target artefact using agent output
func (e *Engine) createReworkArtefact(claim *blackboard.Claim, targetArtefact *blackboard.Artefact, output *ToolOutput) *blackboard.Artefact {
    // Combine target artefact + Review artefacts for source_artefacts
    sourceArtefacts := []string{targetArtefact.ID}
    sourceArtefacts = append(sourceArtefacts, claim.AdditionalContextIDs...)

    return &blackboard.Artefact{
        ID:              uuid.NewString(),
        LogicalID:       targetArtefact.LogicalID,                    // Same thread
        Version:         targetArtefact.Version + 1,                   // Increment version
        StructuralType:  blackboard.StructuralTypeStandard,
        Type:            targetArtefact.Type,                          // Same type
        Payload:         output.ArtefactPayload,
        SourceArtefacts: sourceArtefacts,                             // Target + Reviews
        ProducedByRole:  e.config.AgentRole,
    }
}

// createNewArtefact builds a new artefact (new logical_id, version 1)
func (e *Engine) createNewArtefact(claim *blackboard.Claim, targetArtefact *blackboard.Artefact, output *ToolOutput) *blackboard.Artefact {
    return &blackboard.Artefact{
        ID:              uuid.NewString(),
        LogicalID:       uuid.NewString(),                            // New thread
        Version:         1,                                            // First version
        StructuralType:  blackboard.StructuralTypeStandard,
        Type:            output.ArtefactType,
        Payload:         output.ArtefactPayload,
        SourceArtefacts: []string{targetArtefact.ID},
        ProducedByRole:  e.config.AgentRole,
    }
}
```

**Key points:**
- Agent tools remain simple (only output `artefact_type`, `artefact_payload`, `summary`)
- Cub detects feedback claims via `claim.status == pending_assignment`
- Version increment is automatic: `target.version + 1`
- Source artefacts automatically include target + all Review artefacts
- No agent awareness of versioning required

### **2.4. Agent tool contract changes**

**No changes required**. The agent tool contract remains unchanged from M3.2.

Agents continue to output:
```json
{
  "artefact_type": "CodeCommit",
  "artefact_payload": "abc123def",
  "summary": "Created implementation"
}
```

**Rationale**: Version management is handled entirely by the Cub (section 2.3.2). Agents remain simple and unaware of whether they're creating new work or rework. The Cub detects feedback claims via `claim.status` and automatically manages `logical_id`, `version`, `type`, and `source_artefacts` fields.

### **2.5. Configuration changes**

**New orchestrator section** in `sett.yml`:

```yaml
version: "1.0"

orchestrator:
  max_review_iterations: 3  # How many times an artefact can be rejected and reworked

agents:
  reviewer:
    role: "Reviewer"
    image: "example-reviewer-agent:latest"
    bidding_strategy: "review"

  coder:
    role: "Coder"
    image: "example-git-agent:latest"
    bidding_strategy: "exclusive"

services:
  redis:
    image: redis:7-alpine
```

**Validation** (in `internal/config/config.go`):

```go
type SettConfig struct {
    Version      string                `yaml:"version"`
    Orchestrator OrchestratorConfig    `yaml:"orchestrator,omitempty"` // M3.3
    Agents       map[string]AgentConfig `yaml:"agents"`
    Services     map[string]interface{} `yaml:"services"`
}

type OrchestratorConfig struct {
    MaxReviewIterations int `yaml:"max_review_iterations"`
}

func (c *SettConfig) Validate() error {
    // ... existing validation ...

    // M3.3: Validate orchestrator config
    if c.Orchestrator.MaxReviewIterations < 0 {
        return fmt.Errorf("orchestrator.max_review_iterations must be >= 0 (0 = unlimited)")
    }

    return nil
}
```

### **2.6. CLI changes**

**No changes required** for M3.3. The CLI's `sett up`, `sett forage`, `sett hoard`, and `sett down` commands work unchanged.

**Enhanced output** in `sett hoard`:

Claims with `termination_reason` will display the reason:

```
Claims:
  • abc-123 (terminated) - Terminated due to negative review feedback. See artefacts: [uuid1, uuid2]
  • def-456 (complete)
  • ghi-789 (terminated) - Terminated after reaching max review iterations (3).
```

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

**Critical Design Decisions:**

1. **Claim Schema Extensions (Additional Context)**:
   - **Decision**: Add `additional_context_ids` field to Claim
   - **Rationale**: Enables Orchestrator to inject Review artefacts into Cub's context without complex graph traversal
   - **Trade-off**: Slight schema expansion vs. dramatic simplification of Cub logic
   - **User impact**: None (internal implementation detail)

2. **Iteration Counting via Version Number**:
   - **Decision**: Use `artefact.version - 1` to count iterations
   - **Rationale**: Simple, efficient (single field lookup), no graph traversal needed
   - **Trade-off**: Assumes version increments correctly (agent responsibility)
   - **Risk mitigation**: Agent contract clearly specifies version increment requirement

3. **Feedback Claim Bypasses Bidding**:
   - **Decision**: New status `pending_assignment` with pre-populated `granted_exclusive_agent`
   - **Rationale**: Ensures original agent gets the rework, no race conditions
   - **Trade-off**: Agents cannot refuse feedback claims
   - **Alternative considered**: Let agents bid on feedback claims - rejected as over-complex

4. **Termination Reason Explicitness**:
   - **Decision**: Add `termination_reason` string field to Claim
   - **Rationale**: Makes audit trail immediately clear without requiring graph traversal
   - **Trade-off**: Slight redundancy (reason is derivable) vs. operational clarity
   - **User benefit**: `sett hoard` shows why claims failed at a glance

5. **Max Iterations as Global Setting**:
   - **Decision**: Single `orchestrator.max_review_iterations` applies to all artefact types
   - **Rationale**: Simple, sufficient for M3.3
   - **Trade-off**: Cannot have different limits for different artefact types
   - **Future enhancement**: Per-agent or per-artefact-type limits in M3.4+

6. **Version Control Responsibility**:
   - **Decision**: Cub automatically manages logical_id, version, and type for feedback claims
   - **Rationale**: Keeps agents simple and unaware of versioning, centralizes logic in Cub
   - **Trade-off**: Cub must detect feedback claims via claim.Status
   - **Benefit**: No agent tool contract changes required, perfect backward compatibility

**Risks:**

1. **Cub Fails to Detect Feedback Claim**:
   - **Risk**: Cub doesn't check claim.Status, treats feedback claim as new work
   - **Probability**: Low (single status check in executeWork())
   - **Impact**: High (iteration count incorrect, audit trail broken)
   - **Mitigation**:
     - Unit tests for createReworkArtefact() detection logic
     - Unit tests for version increment (target.version + 1)
     - E2E tests validate version progression (v1→v2→v3)

2. **Infinite Loop Without Max Iterations**:
   - **Risk**: Agent and reviewer disagree indefinitely, loop forever
   - **Probability**: High if max_review_iterations not configured
   - **Impact**: High (resource exhaustion, workflow stuck)
   - **Mitigation**:
     - Default max_review_iterations = 3 in config template
     - Validation warning if set to 0 (unlimited)
     - Clear documentation of risk

3. **Multiple Reviewers Create Large Context**:
   - **Risk**: 5 reviewers × 3 iterations = 15 Review artefacts in context
   - **Probability**: Low (most workflows have 1-2 reviewers)
   - **Impact**: Medium (large context passed to agent tool)
   - **Mitigation**:
     - Context depth limit (maxDepth=10) prevents unbounded growth
     - Deduplication by logical_id keeps context manageable

4. **Original Agent Removed from Config**:
   - **Risk**: Agent that produced A(v1) no longer exists when A(v1) is rejected
   - **Probability**: Low (rare to remove agents mid-workflow)
   - **Impact**: Medium (workflow terminates with Failure)
   - **Mitigation**:
     - Explicit Failure artefact with clear error message
     - Logged as configuration error
     - Operator alerted via termination_reason

5. **Race Condition: New Claim Created Before Feedback Claim Completes**:
   - **Risk**: Agent produces A(v2), new claim created while feedback claim still active
   - **Probability**: Low (feedback claim lifecycle is simple and fast)
   - **Impact**: Low (two claims exist simultaneously, but independent)
   - **Mitigation**: No mitigation needed - this is expected and safe behavior

### **3.2. Implementation steps**

**Phase 1: Blackboard Schema Changes**

1. **[Blackboard]** Add `AdditionalContextIDs []string` field to Claim struct in `pkg/blackboard/types.go`
2. **[Blackboard]** Add `TerminationReason string` field to Claim struct
3. **[Blackboard]** Add `ClaimStatusPendingAssignment` constant to ClaimStatus enum
4. **[Blackboard]** Write unit tests for new Claim fields (marshal/unmarshal)

**Phase 2: Configuration Schema Changes**

5. **[Config]** Add `OrchestratorConfig` struct to `internal/config/config.go`
6. **[Config]** Add `MaxReviewIterations int` field to OrchestratorConfig
7. **[Config]** Update Validate() to check max_review_iterations >= 0
8. **[Config]** Write unit tests for orchestrator config validation
9. **[Config]** Update example sett.yml with orchestrator section

**Phase 3: Orchestrator Feedback Loop Logic**

10. **[Orchestrator]** Create `internal/orchestrator/feedback_loop.go`:
    - CreateFeedbackClaim() function
    - findAgentByRole() reverse-lookup function
    - terminateMaxIterations() with Failure artefact
    - terminateMissingAgent() with Failure artefact
    - formatReviewRejectionReason() helper

11. **[Orchestrator]** Update `checkReviewPhaseCompletion()` in review_phase.go:
    - Collect all feedback artefacts (not just first)
    - Call CreateFeedbackClaim() instead of just terminating
    - Set termination_reason on original claim

12. **[Orchestrator]** Update `processArtefactForPhases()` in engine.go:
    - Handle artefacts created by pending_assignment claims
    - Transition feedback claims to complete (same as exclusive)

13. **[Orchestrator]** Write unit tests for feedback loop logic:
    - Test feedback claim creation
    - Test iteration counting (version - 1)
    - Test max iterations termination
    - Test missing agent termination

**Phase 4: Agent Cub Context Building**

14. **[Cub]** Update `buildContextChain()` in internal/cub/context.go:
    - Add claim.AdditionalContextIDs to queue initialization
    - Implement fetchLatestVersion() using thread ZSET
    - Add deduplication by logical_id
    - Add sorting (newest to oldest)

15. **[Cub]** Update grant detection in handleClaimEvent():
    - Add case for ClaimStatusPendingAssignment
    - Check claim.GrantedExclusiveAgent

16. **[Cub]** Update artefact creation logic in executeWork():
    - Detect feedback claims via claim.Status == ClaimStatusPendingAssignment
    - Call createReworkArtefact() for feedback claims
    - Call createNewArtefact() for regular claims

17. **[Cub]** Implement createReworkArtefact() function:
    - Use targetArtefact.LogicalID (same thread)
    - Increment version: targetArtefact.Version + 1
    - Use targetArtefact.Type (same type)
    - Build source_artefacts: [targetArtefact.ID] + claim.AdditionalContextIDs

18. **[Cub]** Write unit tests for version management:
    - Test createReworkArtefact() increments version correctly
    - Test source_artefacts includes target + Reviews
    - Test logical_id and type preservation
    - Test createNewArtefact() for regular claims

19. **[Cub]** Write unit tests for enhanced context building:
    - Test additional_context_ids injection
    - Test latest version fetching
    - Test deduplication and sorting

**Phase 5: Testing**

20. **[Tests]** Create `internal/orchestrator/feedback_loop_test.go`:
    - Unit tests for CreateFeedbackClaim()
    - Unit tests for iteration limit logic
    - Unit tests for missing agent handling

21. **[Tests]** Create `internal/cub/version_management_test.go`:
    - Unit tests for createReworkArtefact() version increment
    - Unit tests for source_artefacts construction (target + Reviews)
    - Unit tests for logical_id and type preservation

22. **[Tests]** Create `cmd/sett/commands/e2e_m3_3_test.go`:
    - E2E test: Single reviewer rejects → agent reworks → reviewer approves → completes
    - E2E test: Max iterations reached → Failure artefact created
    - E2E test: Multiple reviewers all reject → consolidated feedback claim
    - E2E test: Original agent removed → Failure artefact with clear message
    - E2E test: Version progression (v1→v2→v3) in audit trail (verify Cub auto-increments)

23. **[Tests]** Update existing M3.2 tests:
    - Ensure backward compatibility (no regression)
    - Verify termination_reason field populated correctly

**Phase 6: Documentation**

24. **[Docs]** Update `README.md` with M3.3 status
25. **[Docs]** Update Phase 3 README with M3.3 completion status
26. **[Docs]** Document that agents are unaware of versioning (Cub handles it)
27. **[Docs]** Document orchestrator.max_review_iterations configuration
28. **[Docs]** Update troubleshooting guide with iteration limit errors

### **3.3. Performance & resource considerations**

**Resource usage:**

- **Redis**: Additional claim fields add ~50 bytes per claim (AdditionalContextIDs, TerminationReason)
- **Memory**: No additional orchestrator memory (feedback claim logic is stateless)
- **CPU**: Iteration counting is O(1) (single field lookup), no performance impact
- **Context size**: Feedback claims may have larger context (original work + reviews), but bounded by maxDepth

**Scalability limits:**

- **Feedback claims per workflow**: Bounded by max_review_iterations (typically 3)
- **Review artefacts per feedback claim**: Bounded by number of reviewers (typically 1-3)
- **Context chain depth**: Bounded by maxDepth=10 (safety limit)

**Performance requirements:**

- **Feedback claim creation**: <50ms from review rejection to claim creation
- **Iteration check**: <1ms (single version field lookup)
- **Context building**: <100ms for typical context (5-10 artefacts)
- **No regression**: M3.2 workflows maintain same performance

### **3.4. Testing strategy**

**Unit tests:**

1. **Feedback loop logic**:
   - Test CreateFeedbackClaim() creates correct claim structure
   - Test iteration counting: artefact.version - 1
   - Test max iterations detection and Failure creation
   - Test missing agent detection and Failure creation
   - Test formatReviewRejectionReason() with multiple reviews

2. **Version management** (Cub-specific):
   - Test createReworkArtefact() increments version: target.version + 1
   - Test createReworkArtefact() preserves logical_id and type
   - Test createReworkArtefact() builds source_artefacts: [target] + additional_context_ids
   - Test createNewArtefact() generates new logical_id and version=1
   - Test detection of feedback claims via claim.status

3. **Context building**:
   - Test additional_context_ids injection into queue
   - Test fetchLatestVersion() from thread ZSET
   - Test deduplication by logical_id
   - Test sorting (newest to oldest)
   - Test maxDepth safety limit

4. **Configuration validation**:
   - Test max_review_iterations validation (>= 0)
   - Test default value if orchestrator section missing

**Integration tests:**

1. **Orchestrator + Blackboard**:
   - Real Redis + orchestrator
   - Simulate review rejection
   - Verify feedback claim created with correct fields
   - Verify original claim terminated with reason

2. **Cub + Blackboard**:
   - Real Redis + cub
   - Create feedback claim with additional_context_ids
   - Verify context chain includes Review artefacts
   - Verify version increment in rework artefact

**E2E tests:**

1. **Single iteration feedback loop**:
   ```
   Scenario: Reviewer rejects once, agent fixes, reviewer approves
   Steps:
     1. sett up (reviewer + coder)
     2. sett forage --goal "implement feature"
     3. Coder produces CodeCommit v1 (Cub creates with logical_id=X, version=1)
     4. Reviewer rejects with {"issue": "needs tests"}
     5. Verify original claim terminated with reason
     6. Verify feedback claim created (pending_assignment to coder)
     7. Coder executes, produces output (Cub auto-creates CodeCommit v2)
     8. Verify v2: logical_id=X (same), version=2 (incremented), type=CodeCommit (same)
     9. Verify new regular claim created for v2
     10. Reviewer approves v2
     11. Verify claim completes successfully
   ```

2. **Max iterations reached**:
   ```
   Scenario: Agent and reviewer disagree 3 times, loop terminates
   Config: max_review_iterations: 2
   Steps:
     1. v1 rejected → feedback claim → v2 created
     2. v2 rejected → feedback claim → v3 created
     3. v3 rejected → max iterations (v3.version - 1 = 2) → Failure artefact
     4. Verify claim terminated with "max iterations (2)" reason
     5. Verify Failure artefact created with clear message
   ```

3. **Multiple reviewers feedback consolidation**:
   ```
   Scenario: 2 reviewers both reject, single feedback claim created
   Steps:
     1. Reviewer-1 produces Review: {"issue": "bug in line 10"}
     2. Reviewer-2 produces Review: {"issue": "missing tests"}
     3. Verify original claim terminated
     4. Verify ONE feedback claim created
     5. Verify feedback claim has both Review IDs in additional_context_ids
     6. Verify agent receives both reviews in context
   ```

4. **Missing agent graceful failure**:
   ```
   Scenario: Original agent removed from config during workflow
   Steps:
     1. Agent "coder-a" produces CodeCommit v1
     2. Remove "coder-a" from sett.yml, restart orchestrator
     3. Reviewer rejects v1
     4. Verify Failure artefact created: "agent with role 'Coder' no longer exists"
     5. Verify claim terminated with clear reason
   ```

5. **Backward compatibility**:
   ```
   Scenario: M3.2 workflows without feedback continue working
   Steps:
     1. Use M3.2 sett.yml (no orchestrator section)
     2. Run workflow with review approval (no rejection)
     3. Verify workflow completes normally
     4. Verify no feedback claims created
   ```

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**No new third-party dependencies introduced.**

All functionality uses existing libraries:
- Go standard library for iteration counting
- Existing Redis client for claim updates
- Existing event subscription mechanisms

**Deferred features:**
- Per-agent iteration limits (future enhancement)
- LLM-based feedback interpretation (Phase 4)
- Parallel feedback branches (future)
- Feedback quality scoring (future)

**Justification**: Automated feedback loop is a core Phase 3 requirement identified during M3.2 usage. The simplest implementation (global max_review_iterations, version-based counting) meets all current needs.

### **4.2. Auditability**

**New artefacts created:**
- Failure artefacts for max iterations (structural_type: Failure)
- Failure artefacts for missing agents (structural_type: Failure)

**Immutable audit trail enhanced:**
- All feedback claims stored immutably in Redis
- Termination reasons explicitly recorded on claims
- Version progression visible in artefact thread ZSETs
- Complete iteration history traceable via source_artefacts links

**State changes captured:**
- Feedback claim creation: Logged with iteration number
- Max iterations reached: Logged with Failure ID
- Missing agent detected: Logged with role name
- Original claim termination: Logged with reason

**Audit trail example** (for 3-iteration workflow):
```
1. Claim C1 (GoalDefined) → pending_review → A(v1) created
2. Review rejected → C1 terminated (reason: "review feedback")
3. Feedback claim FC1 created → pending_assignment → A(v2) created
4. New claim C2 (A(v2)) → pending_review → Review rejected
5. C2 terminated (reason: "review feedback")
6. Feedback claim FC2 created → pending_assignment → A(v3) created
7. New claim C3 (A(v3)) → pending_review → Review rejected
8. Max iterations (v3.version-1=2 >= max=2) → Failure F1 created
9. C3 terminated (reason: "max iterations (3)")
```

### **4.3. Small, single-purpose components**

**Component responsibilities remain clear:**

- **Orchestrator**: Adds feedback loop coordination (natural extension of claim lifecycle management)
- **Agent Cub**: Enhanced context building to include additional_context_ids
- **Agents**: Responsible for version increment (maintains agent autonomy)

**No tight coupling introduced:**
- Components still communicate only via blackboard (Redis)
- Feedback loop is orchestrator-internal logic
- Cubs unaware of feedback loop mechanics (just see pending_assignment status)

**Responsibilities preserved:**
- Orchestrator remains "traffic cop" (no domain logic in feedback decisions)
- Agent cubs remain autonomous (version management is agent's choice)
- Config remains declarative validation layer

### **4.4. Security considerations**

**No new security implications:**

- All communication via existing Redis channels (no new network exposure)
- Feedback loop logic is orchestrator-internal
- Additional context IDs are validated artefact UUIDs

**Data validation:**
- Iteration count validated (>= 0)
- Agent existence validated before feedback claim creation
- Artefact IDs validated before adding to additional_context_ids

### **4.5. Backward compatibility**

**Breaking changes:**

None! M3.3 is fully backward compatible with M3.2.

**Non-breaking changes:**

- New claim fields (AdditionalContextIDs, TerminationReason) are optional
- New claim status (pending_assignment) only used for feedback claims
- Orchestrator config section is optional (defaults to max_review_iterations=3)
- Agent tool contract unchanged (no modifications required)

**Existing workflows preserved:**

- M3.2 workflows without review rejection continue working unchanged
- All agents continue working without modifications (Cub handles versioning)
- All Phase 2, M3.1, and M3.2 E2E tests must continue passing

### **4.6. Dependency impact**

**Redis usage:**
- Minimal increase: +50 bytes per claim (new fields)
- Thread ZSET queries for latest version (cached by Redis)
- **Impact**: Negligible

**Memory usage:**
- No additional orchestrator memory (stateless logic)
- Cub context may be larger for feedback claims
- **Impact**: <1KB per feedback claim (bounded by maxDepth)

**CPU usage:**
- Iteration counting is O(1) field lookup
- Context building unchanged complexity (still O(n) where n=depth)
- **Impact**: Negligible

**Build dependencies:**
- No changes to Go version or build tools
- **Impact**: None

## **5. Definition of done**

- [ ] All implementation steps from section 3.2 are complete
- [ ] All tests defined in section 3.4 are implemented and passing
- [ ] Performance requirements from section 3.3 are met and verified
- [ ] Overall test coverage has not decreased (maintain 90%+ for new packages)
- [ ] All Phase 2, M3.1, and M3.2 E2E tests continue passing (backward compatibility)
- [ ] Orchestrator config validation enforces max_review_iterations >= 0
- [ ] Feedback claim creation handles all edge cases (max iterations, missing agent)
- [ ] Context building includes additional_context_ids correctly
- [ ] Cub version management (createReworkArtefact) works correctly
- [ ] Termination reasons are clear and actionable
- [ ] Example sett.yml demonstrates orchestrator.max_review_iterations
- [ ] M3.3 limitations are documented (no per-agent limits, no LLM interpretation)
- [ ] All success criteria from section 1.3 are validated
- [ ] All failure modes identified in section 6 have been tested
- [ ] Operational readiness checklist from section 9 is satisfied

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Orchestrator failures:**

1. **Max iterations reached**:
   - **Behavior**: CreateFeedbackClaim() detects version - 1 >= max, creates Failure artefact
   - **Detection**: Automatic (iteration check before feedback claim creation)
   - **Recovery**: None (workflow terminated, manual intervention required)
   - **Logging**: "claim_terminated_max_iterations" event with iteration count
   - **Termination reason**: "Terminated after reaching max review iterations (N)."

2. **Original agent no longer in config**:
   - **Behavior**: findAgentByRole() fails, creates Failure artefact
   - **Detection**: Automatic (agent registry lookup)
   - **Recovery**: None (configuration error, requires fixing sett.yml)
   - **Logging**: "claim_terminated_missing_agent" event with role name
   - **Termination reason**: "Terminated due to missing agent configuration (role: X)."

3. **Feedback claim creation fails**:
   - **Behavior**: Redis error during CreateClaim(), logs error
   - **Detection**: Error logged, original claim may still be in pending_review
   - **Recovery**: Orchestrator retry on next event loop iteration (claim still exists)
   - **Logging**: "failed to create feedback claim: {error}"

**Cub failures:**

4. **Cub fails to detect feedback claim**:
   - **Behavior**: Cub doesn't check claim.Status, treats feedback claim as new work
   - **Detection**: New logical_id generated instead of version increment
   - **Recovery**: None (audit trail broken, new thread created)
   - **Impact**: Iteration counting fails, max iterations bypassed
   - **Prevention**: Unit tests for claim.Status detection, E2E version progression tests

5. **Cub fails to increment version**:
   - **Behavior**: Bug in createReworkArtefact(), version not incremented
   - **Detection**: Audit trail shows duplicate versions or version=1
   - **Recovery**: None (requires Cub fix and workflow restart)
   - **Impact**: Iteration counting incorrect
   - **Prevention**: Unit tests for version increment, E2E tests verify v1→v2→v3

**Agent failures:**

6. **Agent crashes during feedback claim**:
   - **Behavior**: Claim stuck in pending_assignment (M3.2 limitation still applies)
   - **Detection**: Manual monitoring (claim in pending_assignment >30min)
   - **Recovery**: Manual intervention (terminate claim or restart agent)
   - **Logging**: No automatic timeout in M3.3 (deferred to M3.4)
   - **Future**: Runtime failure detection in M3.4

**Configuration failures:**

7. **max_review_iterations set to 0 (unlimited)**:
   - **Behavior**: No iteration limit enforced
   - **Detection**: Config validation logs warning
   - **Recovery**: User decision (0 is valid, means unlimited)
   - **Impact**: Potential infinite loop if agent/reviewer disagree
   - **Logging**: Warning during sett up validation

8. **max_review_iterations missing from config**:
   - **Behavior**: Default value used (3)
   - **Detection**: None (valid configuration)
   - **Recovery**: N/A
   - **Logging**: No logging needed (intentional default)

### **6.2. Concurrency considerations**

**Race condition analysis:**

1. **Multiple reviewers reject simultaneously**:
   - **Possibility**: 2+ reviewers produce Review artefacts at same time
   - **Protection**: checkReviewPhaseCompletion() waits for ALL reviewers
   - **Impact**: Single feedback claim created with all Review IDs
   - **Safety**: Atomic claim update prevents duplicate feedback claims

2. **Feedback claim completes while new claim is being created**:
   - **Possibility**: Agent produces A(v2), both feedback claim completion and new claim creation occur
   - **Protection**: Separate claim lifecycles, no shared state
   - **Impact**: Both operations succeed independently
   - **Safety**: No issue (expected behavior)

3. **Agent removed from config during feedback claim creation**:
   - **Possibility**: Config reload occurs between review rejection and feedback claim creation
   - **Protection**: findAgentByRole() checks current agent registry
   - **Impact**: Feedback claim creation fails, Failure artefact created
   - **Safety**: Graceful failure with clear error message

### **6.3. Edge case handling**

**Edge case: All reviewers approve (no feedback)**:
- **Behavior**: checkReviewPhaseCompletion() finds no feedback artefacts
- **Detection**: feedbackArtefacts slice is empty
- **Recovery**: N/A (normal M3.2 workflow, transitions to next phase)
- **Logging**: No feedback-related logging

**Edge case: max_review_iterations = 0 (unlimited)**:
- **Behavior**: Iteration check always passes (N >= 0 is always true)
- **Detection**: Config validation warns user
- **Recovery**: Workflow can iterate indefinitely
- **Logging**: Warning during sett up

**Edge case: Reviewer provides feedback on v1, v2 is created, different reviewer rejects v2**:
- **Behavior**: Two separate feedback loops (v1→v2 and v2→v3)
- **Detection**: Separate claims with separate additional_context_ids
- **Recovery**: N/A (expected behavior)
- **Logging**: Each iteration logged with iteration number

**Edge case: Artefact version = 1, reviewer rejects**:
- **Behavior**: Iteration count = 1 - 1 = 0, feedback claim created (first iteration)
- **Detection**: Normal feedback loop start
- **Recovery**: N/A
- **Logging**: iteration=1 logged

**Edge case: Artefact version = 4, max_review_iterations = 3, reviewer rejects**:
- **Behavior**: Iteration count = 4 - 1 = 3, equals max, creates Failure
- **Detection**: iteration_count >= max_review_iterations
- **Recovery**: Workflow terminated
- **Logging**: "max iterations (3)" logged

**Edge case: Context chain depth exceeds maxDepth**:
- **Behavior**: buildContextChain() stops at depth=10, logs warning
- **Detection**: depth counter reaches limit
- **Recovery**: Context truncated at 10 artefacts (safety limit)
- **Logging**: Warning logged if maxDepth reached

## **7. Open questions & decisions**

**All questions resolved. Ready for implementation.**

The following decisions have been made:

1. ✅ **Claim status for feedback claims**: New status `pending_assignment`
2. ✅ **Claim target artefact**: Targets the original work (not the Review)
3. ✅ **Post-feedback workflow**: Feedback claim completes → new regular claim for A(v2)
4. ✅ **Iteration depth tracking**: Use `artefact.version - 1` (simple, efficient)
5. ✅ **Version management**: Agent responsibility via tool contract
6. ✅ **Context chain construction**: Cub traverses source_artefacts + additional_context_ids
7. ✅ **Multiple reviewers rejecting**: One feedback claim with all Review IDs
8. ✅ **Agent resolution**: Reverse-lookup agentRegistry by produced_by_role
9. ✅ **Max iterations reached**: Create Failure artefact, terminate with reason
10. ✅ **Original agent not found**: Create Failure artefact, terminate with reason
11. ✅ **Configuration schema**: orchestrator.max_review_iterations in sett.yml
12. ✅ **Original claim after feedback**: Status=terminated with explicit reason

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**Start with the simplest path:**
1. Begin with blackboard schema changes (Claim fields)
2. Then configuration schema (OrchestratorConfig)
3. Then orchestrator feedback loop logic (CreateFeedbackClaim)
4. Then cub context building (additional_context_ids)
5. Then cub version management (createReworkArtefact, createNewArtefact)
6. Finally comprehensive testing

**Implement comprehensive error handling from the beginning:**
- All iteration checks with clear Failure artefacts
- All agent lookups with missing agent handling
- All claim updates with Redis error handling
- Clear logging for all feedback loop events

**Write tests before implementation (TDD approach):**
- Start with unit tests for iteration counting (version - 1)
- Then unit tests for CreateFeedbackClaim()
- Then integration tests with real Redis
- Finally E2E tests with full workflows

**Use defensive programming:**
- Always validate artefact version before counting iterations
- Validate agent exists before creating feedback claim
- Handle missing additional_context_ids gracefully
- Never assume context_chain is non-empty

### **8.2. Common pitfalls to avoid**

**Feedback loop pitfalls:**

1. **Forgetting to check max iterations before creating feedback claim**:
   - Always check `(artefact.version - 1) >= max_iterations` BEFORE calling CreateFeedbackClaim()
   - Create Failure artefact if limit reached
   - Test with workflows that hit the limit

2. **Not consolidating multiple reviewer feedback**:
   - Collect ALL Review artefacts before creating feedback claim
   - Don't create separate feedback claims per reviewer
   - Test with 2+ reviewers rejecting simultaneously

3. **Forgetting to set termination_reason**:
   - Always populate termination_reason when setting status=terminated
   - Use formatReviewRejectionReason() helper
   - Include relevant artefact IDs in reason

4. **Agent registry reverse-lookup errors**:
   - Handle case where no agent has the required role
   - Create Failure artefact with clear error message
   - Test with agent removed from config

**Context building pitfalls:**

5. **Not handling empty additional_context_ids**:
   - Check len(claim.AdditionalContextIDs) > 0 before logging "feedback claim"
   - Empty array is valid for regular claims
   - Don't assume presence means feedback claim

6. **Not fetching latest version from thread**:
   - Always use fetchLatestVersion() for each logical_id
   - Don't use the version from source_artefacts (may be outdated)
   - Fall back to current version if ZSET query fails

7. **Not deduplicating by logical_id**:
   - Multiple artefacts may share logical_id (different versions)
   - Keep only the latest version of each logical_id
   - Test context building with v1, v2, v3 of same artefact

**Cub version management pitfalls:**

8. **Not detecting feedback claims correctly**:
   - Always check claim.Status == ClaimStatusPendingAssignment
   - Don't assume additional_context_ids presence means feedback claim
   - Test both feedback and regular claim paths

9. **Not incrementing version correctly**:
   - Always use targetArtefact.Version + 1 (not hardcoded values)
   - Don't forget to preserve logical_id and type
   - Test that v1→v2→v3 progression is correct in E2E tests

10. **Not building source_artefacts correctly**:
    - Must include targetArtefact.ID + claim.AdditionalContextIDs
    - Don't forget to combine both arrays
    - Test that Review artefacts appear in source_artefacts

**Configuration pitfalls:**

11. **Not providing default for max_review_iterations**:
    - Default to 3 if orchestrator section missing
    - Don't require users to specify it
    - Document the default in config validation

### **8.3. Integration checklist**

**Pre-implementation verification:**
- [x] All prerequisite features complete (M3.2)
- [x] Claim schema changes designed and approved
- [x] Configuration schema designed and approved
- [x] Iteration counting algorithm finalized (version - 1)

**During implementation:**
- [ ] Claim fields AdditionalContextIDs and TerminationReason added
- [ ] ClaimStatusPendingAssignment constant added
- [ ] OrchestratorConfig with MaxReviewIterations added
- [ ] CreateFeedbackClaim() creates correct claim structure
- [ ] Iteration counting uses artefact.version - 1
- [ ] Max iterations creates Failure artefact
- [ ] Missing agent creates Failure artefact
- [ ] checkReviewPhaseCompletion() consolidates feedback
- [ ] buildContextChain() includes additional_context_ids
- [ ] createReworkArtefact() increments version correctly
- [ ] createReworkArtefact() preserves logical_id and type
- [ ] createReworkArtefact() builds source_artefacts with target + Reviews
- [ ] Cub detects feedback claims via claim.Status
- [ ] All M3.2 tests still passing

**Post-implementation:**
- [ ] All E2E tests passing (feedback loops, max iterations, missing agent)
- [ ] Cub version management E2E tests verify v1→v2→v3 progression
- [ ] Performance benchmarks met
- [ ] Documentation updated (README, Cub version management guide, config reference)
- [ ] Documentation clarifies agents are unaware of versioning

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Metrics to track:**

1. **Feedback loop metrics**:
   - Feedback claims created per hour
   - Average iterations per workflow (mean, median, max)
   - Max iterations terminations (count and percentage)
   - Missing agent failures (should be rare)

2. **Iteration progression**:
   - Version distribution (how many v1, v2, v3, etc.)
   - Time from rejection to rework completion
   - Review approval rate by iteration (% approved on v1, v2, v3)

3. **Context building**:
   - Average context chain size for feedback claims
   - MaxDepth hits (should be zero)
   - Thread ZSET query latency

**Structured logging events:**

```json
// Feedback claim created
{"level":"info","component":"orchestrator","event":"feedback_claim_created","feedback_claim_id":"fc123","original_claim_id":"c456","target_artefact":"a789","assigned_agent":"coder","review_artefacts":["r1","r2"],"iteration":2,"timestamp":"2025-10-18T00:00:00Z"}

// Max iterations reached
{"level":"warn","component":"orchestrator","event":"claim_terminated_max_iterations","claim_id":"c456","artefact_id":"a789","iterations":3,"failure_id":"f123","timestamp":"2025-10-18T00:05:00Z"}

// Missing agent detected
{"level":"error","component":"orchestrator","event":"claim_terminated_missing_agent","claim_id":"c456","missing_role":"Coder","failure_id":"f456","timestamp":"2025-10-18T00:05:00Z"}

// Review rejection (M3.2 event, now includes iteration info)
{"level":"warn","component":"orchestrator","event":"review_rejected","claim_id":"c456","reviewer":"reviewer-1","artefact_id":"r1","current_version":2,"timestamp":"2025-10-18T00:00:00Z"}
```

**Health check modifications:**
- No changes to orchestrator /healthz endpoint
- Feedback loop metrics could be added to /metrics endpoint (future)

**Operator diagnostics:**
- Redis CLI: Check claim additional_context_ids
  ```bash
  redis-cli HGET sett:my-instance:claim:fc123 additional_context_ids
  ```
- Redis CLI: Check termination reason
  ```bash
  redis-cli HGET sett:my-instance:claim:c456 termination_reason
  ```
- Orchestrator logs: Grep for feedback events
  ```bash
  docker logs sett-orchestrator-my-instance | grep feedback_claim_created
  ```

### **9.2. Rollback and disaster recovery**

**Rollback to M3.2:**

1. **No data migration required**: Blackboard schema is backward compatible (new fields are optional)
2. **Config changes**: Remove orchestrator section from sett.yml (optional anyway)
3. **Orchestrator binary**: Rollback to M3.2 orchestrator image
4. **Behavior**: Review rejections will terminate claims (M3.2 behavior), no feedback loops
5. **In-flight feedback claims**: Will become stuck (manual termination required)

**Disaster recovery:**

1. **Feedback claim stuck in pending_assignment**:
   - Agent crashed or taking too long
   - **Recovery**: Manually terminate claim, or wait for M3.4 timeouts
   - **Detection**: Monitor claims in pending_assignment for >30 minutes

2. **Infinite loop (max_review_iterations=0)**:
   - Agent and reviewer disagree indefinitely
   - **Recovery**: Update sett.yml to set reasonable max (e.g., 3), restart orchestrator
   - **Detection**: Observe version numbers increasing without completion

3. **Agent produces incorrect version**:
   - Audit trail broken, iteration counting incorrect
   - **Recovery**: Fix agent implementation, restart workflow
   - **Detection**: Manual inspection of artefact versions in sett hoard

**Recovery time objectives:**

- Feedback claim creation failure: <1 minute (automatic retry on next event)
- Stuck feedback claim detection: <5 minutes (monitoring alerts)
- Max iterations termination: Immediate (automatic Failure creation)
- Rollback to M3.2: <5 minutes (orchestrator image swap)

### **9.3. Documentation and training**

**Documentation updates required:**

1. **README.md**:
   - Add M3.3 status badge
   - Update feature list (automated feedback loops)
   - Document orchestrator.max_review_iterations configuration
   - Document iteration limits and Failure artefacts

2. **docs/agent-development.md**:
   - Explain rework scenarios and version increment
   - Document logical_id and version tool output fields
   - Example agent implementation for rework
   - Explain context_chain in feedback claims

3. **New guide: docs/feedback-loops.md**:
   - How automated feedback loops work
   - Best practices for max_review_iterations values
   - Troubleshooting iteration limits
   - Handling agent disagreements

4. **Example sett.yml**:
   ```yaml
   # Example: M3.3 with automated feedback loop
   version: "1.0"

   orchestrator:
     max_review_iterations: 3  # Max times an artefact can be reworked

   agents:
     reviewer:
       role: "Reviewer"
       image: "example-reviewer-agent:latest"
       bidding_strategy: "review"

     coder:
       role: "Coder"
       image: "example-git-agent:latest"
       bidding_strategy: "exclusive"
   ```

**Troubleshooting guides:**

1. **Common issues**:
   - "Max iterations reached" → Increase orchestrator.max_review_iterations or fix agent/reviewer disagreement
   - "Missing agent configuration" → Add missing agent back to sett.yml
   - "Version not incrementing" → Check Cub logs, may be bug in createReworkArtefact() logic
   - "Feedback claim stuck" → Check agent logs, may need manual termination (wait for M3.4 timeouts)

2. **Debugging commands**:
   ```bash
   # Check claim termination reason
   redis-cli HGET sett:my-instance:claim:abc123 termination_reason

   # Check feedback claim additional context
   redis-cli HGET sett:my-instance:claim:fc123 additional_context_ids

   # View orchestrator feedback events
   docker logs sett-orchestrator-my-instance | grep -E "feedback_claim|max_iterations"

   # Check artefact version progression
   redis-cli ZRANGE sett:my-instance:thread:logical-id 0 -1 WITHSCORES
   ```

**Upgrade notes (M3.2 → M3.3):**

1. **No breaking changes**: M3.3 is fully backward compatible
2. **New behavior**: Review rejections now create feedback claims (automatic rework)
3. **Configuration**: Add orchestrator.max_review_iterations to control iteration limit (default: 3)
4. **Agent updates**: No agent changes required - Cub handles all versioning automatically
5. **Audit trail**: sett hoard now shows termination_reason for failed claims

## **10. Self-validation checklist**

### **Before starting implementation:**

- [x] I understand how M3.3 builds on M3.2 (adds automated feedback loops)
- [x] All success criteria (section 1.3) are measurable and testable
- [x] I have considered every component in section 2 explicitly
- [x] All design decisions (section 3.1) are justified and documented
- [x] Iteration counting mechanism is clear and efficient (version - 1)
- [x] Feedback claim creation logic is well-defined
- [x] Context building enhancement is unambiguous
- [x] Cub version management is automatic and agent-agnostic

### **During implementation:**

- [ ] I am implementing the simplest solution that meets success criteria
- [ ] All error scenarios (section 6) are being handled, not just happy path
- [ ] Tests are being written before or alongside code (TDD approach)
- [ ] I am validating that M3.2 functionality is not broken (backward compatibility)
- [ ] Logging is comprehensive (feedback events, iterations, terminations)
- [ ] Termination reasons are always populated when status=terminated
- [ ] Iteration counting is efficient (no graph traversal)
- [ ] Agent registry lookups handle missing agents gracefully

### **Before submission:**

- [ ] All items in Definition of Done (section 5) are complete
- [ ] Feature has been tested in a clean environment from scratch
- [ ] Documentation is updated and accurate (README, Cub version guide, config reference)
- [ ] I have considered the operational impact (section 9) of this feature
- [ ] All Phase 2, M3.1, and M3.2 tests pass (backward compatibility validated)
- [ ] E2E tests with feedback loops pass consistently
- [ ] E2E tests verify v1→v2→v3 version progression (Cub auto-increment)
- [ ] Max iterations tests pass (Failure artefact verified)
- [ ] Missing agent tests pass (graceful failure verified)
- [ ] Example sett.yml with orchestrator config provided and tested
- [ ] Documentation clarifies agents are unaware of versioning
