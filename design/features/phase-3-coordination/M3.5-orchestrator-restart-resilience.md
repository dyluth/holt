# **Feature design: Orchestrator Restart Resilience**

**Purpose**: Enable orchestrator recovery from crashes and restarts without losing workflow state
**Scope**: Phase 3 Coordination - Milestone 5
**Estimated tokens**: ~10,000 tokens
**Read when**: Implementing orchestrator restart resilience, state persistence, crash recovery

Associated phase: **Coordination (Phase 3)**
Status: **Design**

***Template purpose:*** This document is a blueprint for M3.5, an implementable milestone that enables the orchestrator to recover from crashes and restarts. It provides an unambiguous specification for persisting orchestrator state to Redis and reconstructing it on startup, ensuring workflows continue without stuck claims.

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Enable the orchestrator to survive crashes and restarts by persisting all critical state to Redis, allowing it to resume workflow coordination without human intervention or stuck claims.

### **1.2. User story**

As a Holt user running production workflows, I want the orchestrator to automatically recover from crashes so that:
1. Claims in active phases (review, parallel, exclusive) resume execution after orchestrator restart
2. Workers at the max_concurrent limit are queued persistently and resume when slots open
3. Orphaned worker containers from the crashed orchestrator are cleaned up automatically
4. Work that was granted but never started (or never completed) is automatically re-triggered
5. The workflow continues seamlessly without manual intervention or data loss

This enables reliable, unattended operation of Holt in production environments where orchestrator restarts (planned or unplanned) should not break workflows.

### **1.3. Success criteria**

1. **Phase state recovery**: Orchestrator restart preserves claims in `pending_review`, `pending_parallel`, `pending_exclusive` states and continues waiting for agent output
2. **Partial completion handling**: Claims where some (but not all) granted agents produced artefacts resume correctly, waiting only for remaining agents
3. **Grant re-triggering**: Claims that were granted but produced no output are automatically re-triggered (workers re-launched, traditional agents re-notified)
4. **Persistent grant queue**: Claims paused at `max_concurrent` limit are persisted and resume when worker slots open after restart
5. **Orphan cleanup**: Worker containers from previous orchestrator run are identified and removed on startup
6. **Feedback loop recovery**: Claims in `pending_assignment` status (feedback loops) resume correctly with iteration tracking
7. **Stale lock handling**: Orchestrator detects and takes over stale instance locks from crashed predecessors
8. **No stuck claims**: E2E test verifies orchestrator restart with pre-populated Redis state continues workflows correctly
9. **Backward compatibility**: M3.5 orchestrator handles clean M3.4 → M3.5 upgrade path (requires `holt down` then `holt up`)
10. **Minimal performance overhead**: State persistence operations complete within 10ms per write

### **1.4. Non-goals**

- **Dynamic configuration reload**: Changing `holt.yml` during orchestrator runtime (config still loaded at startup only)
- **Distributed orchestrator**: Multiple orchestrators coordinating the same instance (single orchestrator per instance)
- **Historical state export**: Exporting full workflow history to external systems (audit trail remains in Redis)
- **Backward compatible state format**: M3.4 orchestrators working with M3.5 state (clean upgrade required)
- **Worker reconnection**: Reconnecting to running workers from previous orchestrator (workers are cleaned up and re-launched)
- **Graceful shutdown hooks**: Special persistence logic on orchestrator shutdown (continuous persistence handles this)

## **2. The 'what': component impact analysis**

### **2.1. Blackboard changes**

**Schema changes required**: Extend Claim structure with phase state and grant queue fields.

#### **2.1.1. Claim structure additions**

Add the following fields to the existing Claim Redis hash:

```go
// Existing Claim fields (unchanged):
// - id, artefact_id, status, granted_review_agents, granted_parallel_agents, granted_exclusive_agent

// M3.5: Phase state persistence
phase_current          string                  // Current phase: "review", "parallel", or "exclusive" (empty for pending_consensus)
phase_granted_agents   []string                // JSON array of agents granted in current phase
phase_received         map[string]string       // JSON object: agentRole → artefactID (received artefacts in current phase)
phase_all_bids         map[string]string       // JSON object: agentName → bidType (all original bids for phase transition logic)
phase_start_time       int64                   // Unix timestamp when current phase started

// M3.5: Grant queue persistence (for controller-worker max_concurrent pausing)
grant_queue_paused_at  int64                   // Unix timestamp when claim was paused (0 if not paused)
grant_queue_agent_name string                  // Agent name that would be granted (when paused)
grant_queue_position   int                     // Reserved for future display/debugging (queue position in ZSET)
                                               // NOTE: Not populated in M3.5 - ZSET score provides FIFO ordering

// M3.5: Grant tracking (for re-triggering)
last_grant_agent       string                  // Last agent granted this claim (for re-trigger on restart)
last_grant_time        int64                   // When the last grant happened
artefact_expected      bool                    // Whether we're waiting for artefact from granted agent
```

**Storage format**: All new fields stored as strings in Redis hash, with JSON encoding for arrays/maps.

#### **2.1.2. New Redis keys and modified key formats**

**New keys:**
```
holt:{instance_name}:grant_queue:{role}   # ZSET: score=paused_at timestamp, member=claim_id
                                          # Maintains FIFO queue per role for max_concurrent pausing
```

**Modified key value formats (M3.5 changes):**
```
holt:{instance_name}:lock                 # STRING: format "orchestrator:{unix_timestamp}"
                                          # Value updated every 10s by orchestrator heartbeat
                                          # M3.5 adds stale detection: age >30s = stale lock
```

**Rationale**:
- ZSET allows efficient FIFO ordering (by timestamp) and atomic dequeue operations for grant queue
- Explicit lock value format enables stale lock detection for orchestrator restart takeover

#### **2.1.3. New Blackboard Client methods**

The following methods must be added to the `blackboard.Client` struct to support M3.5 recovery and persistence:

**Claim scanning for recovery:**
```go
// GetClaimsByStatus retrieves all claims with specified statuses (for startup recovery)
// Used to scan Redis for active claims when orchestrator restarts
func (c *Client) GetClaimsByStatus(ctx context.Context, statuses []string) ([]*Claim, error)
```

**ZSET operations for grant queue:**
```go
// ZAdd adds a member to a sorted set with a score (for grant queue FIFO)
func (c *Client) ZAdd(ctx context.Context, key string, score float64, member string) error

// ZRange retrieves members from a sorted set by rank range (start to stop inclusive)
func (c *Client) ZRange(ctx context.Context, key string, start, stop int64) ([]string, error)

// ZRangeWithScores retrieves members with scores from a sorted set
// Used to get both claim IDs and their paused_at timestamps
func (c *Client) ZRangeWithScores(ctx context.Context, key string, start, stop int64) ([]redis.Z, error)

// ZRem removes members from a sorted set (for dequeueing claims)
func (c *Client) ZRem(ctx context.Context, key string, members ...string) error
```

**Implementation notes:**
- `GetClaimsByStatus` uses Redis `SCAN` with pattern matching on claim keys, then filters by status field
- ZSET methods are thin wrappers around go-redis client operations with error handling
- All methods follow existing blackboard client patterns (context, error handling, logging)

### **2.2. Orchestrator changes**

**Major enhancement**: Orchestrator gains state recovery, persistence, and startup reconstruction logic.

#### **2.2.1. Startup recovery sequence**

Add new `RecoverState()` method called before main event loop:

```go
func (e *Engine) RecoverState(ctx context.Context) error {
    // Step 1: Detect and clean up orphaned workers
    if e.workerManager != nil {
        if err := e.workerManager.CleanupOrphanedWorkers(ctx); err != nil {
            log.Printf("[Orchestrator] Warning: Failed to cleanup orphaned workers: %v", err)
        }
    }

    // Step 2: Scan Redis for active claims
    activeClaims, err := e.client.GetClaimsByStatus(ctx, []string{
        "pending_review", "pending_parallel", "pending_exclusive", "pending_assignment",
    })
    if err != nil {
        return fmt.Errorf("failed to scan for active claims: %w", err)
    }

    // Step 3: Reconstruct phase state for each active claim
    for _, claim := range activeClaims {
        if err := e.reconstructPhaseState(ctx, claim); err != nil {
            log.Printf("[Orchestrator] Warning: Failed to recover claim %s: %v", claim.ID, err)
            // Terminate claim with clear reason
            e.terminateClaimWithReason(ctx, claim.ID, fmt.Sprintf("Recovery failed: %v", err))
        }
    }

    // Step 4: Recover grant queues for each role
    if e.workerManager != nil {
        if err := e.recoverGrantQueues(ctx); err != nil {
            log.Printf("[Orchestrator] Warning: Failed to recover grant queues: %v", err)
        }
    }

    log.Printf("[Orchestrator] State recovery complete: %d claims recovered", len(activeClaims))
    return nil
}
```

#### **2.2.2. Phase state reconstruction**

```go
func (e *Engine) reconstructPhaseState(ctx context.Context, claim *blackboard.Claim) error {
    // Parse persisted phase state from claim
    phase := claim.PhaseState.Current
    grantedAgents := claim.PhaseState.GrantedAgents
    receivedArtefacts := claim.PhaseState.ReceivedArtefacts
    allBids := claim.PhaseState.AllBids

    // Validate granted agents still exist in config
    for _, agentName := range grantedAgents {
        if _, exists := e.agentRegistry[agentName]; !exists {
            return fmt.Errorf("granted agent '%s' no longer in config", agentName)
        }
    }

    // Reconstruct PhaseState object
    phaseState := &PhaseState{
        ClaimID:           claim.ID,
        Phase:             phase,
        GrantedAgents:     grantedAgents,
        ReceivedArtefacts: receivedArtefacts,
        AllBids:           allBids,
        StartTime:         time.Unix(claim.PhaseState.StartTime, 0),
    }

    // Store in-memory
    e.phaseStates[claim.ID] = phaseState

    // Check if we need to re-trigger grants
    if claim.ArtefactExpected && !hasReceivedAllArtefacts(phaseState) {
        // Some granted agents haven't produced output - re-trigger
        if err := e.retriggerGrant(ctx, claim, grantedAgents); err != nil {
            return fmt.Errorf("failed to re-trigger grant: %w", err)
        }
    }

    return nil
}
```

#### **2.2.3. Grant re-triggering**

```go
func (e *Engine) retriggerGrant(ctx context.Context, claim *blackboard.Claim, grantedAgents []string) error {
    log.Printf("[Orchestrator] Re-triggering grant for claim %s (granted to: %v)", claim.ID, grantedAgents)

    // Determine grant type based on claim status/phase
    switch claim.Status {
    case blackboard.ClaimStatusPendingExclusive:
        // Exclusive grant
        agentName := grantedAgents[0] // Only one agent
        agent, exists := e.config.Agents[agentName]
        if !exists {
            return fmt.Errorf("granted agent '%s' not found in config", agentName)
        }

        // Check if controller-worker
        if agent.Mode == "controller" {
            // Re-launch worker
            if e.workerManager != nil {
                return e.workerManager.LaunchWorker(ctx, claim, agentName, agent, e.client)
            }
            return fmt.Errorf("worker manager not available for controller agent")
        }

        // Traditional agent - re-publish grant notification
        return e.client.PublishClaimEvent(ctx, claim.ID)

    case blackboard.ClaimStatusPendingAssignment:
        // Feedback loop claim - already assigned, just publish
        return e.client.PublishClaimEvent(ctx, claim.ID)

    default:
        // Review/Parallel - re-publish for all granted agents
        return e.client.PublishClaimEvent(ctx, claim.ID)
    }
}
```

#### **2.2.4. Persistent grant queue**

```go
// PauseGrantForQueue adds claim to persistent FIFO queue when max_concurrent reached
func (e *Engine) PauseGrantForQueue(ctx context.Context, claim *blackboard.Claim, agentName string, role string) error {
    pausedAt := time.Now().Unix()

    // Update claim with queue metadata
    claim.GrantQueue = &blackboard.GrantQueue{
        PausedAt:  pausedAt,
        AgentName: agentName,
        Position:  -1, // Will be set by queue position
    }

    // Add to Redis ZSET (score = pausedAt for FIFO)
    queueKey := fmt.Sprintf("holt:%s:grant_queue:%s", e.instanceName, role)
    if err := e.client.ZAdd(ctx, queueKey, pausedAt, claim.ID); err != nil {
        return fmt.Errorf("failed to add claim to grant queue: %w", err)
    }

    // Persist queue metadata to claim
    if err := e.client.UpdateClaim(ctx, claim); err != nil {
        return fmt.Errorf("failed to update claim with queue metadata: %w", err)
    }

    log.Printf("[Orchestrator] Claim %s paused in grant queue for role '%s' (max_concurrent reached)", claim.ID, role)
    return nil
}

// ResumeFromQueue pops next claim from queue when worker slot opens
func (e *Engine) ResumeFromQueue(ctx context.Context, role string) error {
    queueKey := fmt.Sprintf("holt:%s:grant_queue:%s", e.instanceName, role)

    // Pop oldest claim (lowest score)
    claimIDs, err := e.client.ZRangeWithScores(ctx, queueKey, 0, 0)
    if err != nil || len(claimIDs) == 0 {
        return nil // Queue empty
    }

    claimID := claimIDs[0].Member
    claim, err := e.client.GetClaim(ctx, claimID)
    if err != nil {
        return fmt.Errorf("failed to fetch queued claim: %w", err)
    }

    // Remove from queue
    if err := e.client.ZRem(ctx, queueKey, claimID); err != nil {
        return fmt.Errorf("failed to remove claim from queue: %w", err)
    }

    // Clear queue metadata from claim
    claim.GrantQueue = nil
    if err := e.client.UpdateClaim(ctx, claim); err != nil {
        return fmt.Errorf("failed to clear queue metadata: %w", err)
    }

    // Grant to agent
    log.Printf("[Orchestrator] Resuming claim %s from grant queue for role '%s'", claimID, role)
    return e.grantClaim(ctx, claim, claim.GrantQueue.AgentName)
}
```

#### **2.2.5. State persistence on transitions**

Update existing phase transition methods to persist state:

```go
func (e *Engine) transitionToPhase(ctx context.Context, claim *blackboard.Claim, phase string, grantedAgents []string, allBids map[string]blackboard.BidType) error {
    // ... existing transition logic ...

    // M3.5: Persist phase state to claim
    claim.PhaseState = &blackboard.PhaseState{
        Current:       phase,
        GrantedAgents: grantedAgents,
        Received:      make(map[string]string),
        AllBids:       allBids,
        StartTime:     time.Now().Unix(),
    }
    claim.ArtefactExpected = true
    claim.LastGrantAgent = grantedAgents[0] // First granted agent
    claim.LastGrantTime = time.Now().Unix()

    if err := e.client.UpdateClaim(ctx, claim); err != nil {
        return fmt.Errorf("failed to persist phase state: %w", err)
    }

    // ... rest of transition logic ...
}
```

### **2.3. Agent pup changes**

**No changes required**. Agent pups are stateless and receive grants via Pub/Sub. The orchestrator handles all recovery logic.

### **2.4. CLI changes**

**Minor enhancement**: `holt up` should detect stale locks and provide clear messaging.

#### **2.4.1. Stale lock detection**

```go
// Before starting orchestrator, check for stale lock
func detectStaleLock(ctx context.Context, cli *client.Client, instanceName string) (bool, error) {
    lockKey := fmt.Sprintf("holt:%s:lock", instanceName)
    lockData, err := cli.Get(ctx, lockKey).Result()
    if err == redis.Nil {
        return false, nil // No lock
    }
    if err != nil {
        return false, err
    }

    // Parse lock (format: "orchestrator:{timestamp}")
    parts := strings.Split(lockData, ":")
    if len(parts) != 2 {
        return true, nil // Malformed lock - consider stale
    }

    timestamp, err := strconv.ParseInt(parts[1], 10, 64)
    if err != nil {
        return true, nil // Invalid timestamp - consider stale
    }

    // Lock is stale if older than 30 seconds (orchestrator heartbeat is 10s)
    age := time.Now().Unix() - timestamp
    return age > 30, nil
}
```

**User messaging**:
```
Detecting instance state...
  ⚠ Found stale orchestrator lock (age: 45s)
  ✓ Taking over instance 'test' (previous orchestrator assumed crashed)
  ✓ Cleaning up 2 orphaned worker containers
  ✓ Recovering 3 active claims
  ✓ Resuming 1 paused grant from queue
Instance 'test' started successfully (recovered from previous crash)
```

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

#### **Decision 1: Extend Claim structure vs. separate keys**
**Chosen**: Extend existing Claim Redis hash with new fields
**Rationale**:
- Atomic reads/writes (single HGETALL/HMSET)
- No additional Redis round-trips
- Simpler recovery logic (all state in one place)
**Alternative rejected**: Separate keys for phase state (more complex, more round-trips)

#### **Decision 2: Re-grant vs. re-consensus**
**Chosen**: Re-grant to same agents that were previously granted
**Rationale**:
- Maintains original grant intent
- Avoids non-determinism (agents might bid differently on restart)
- Simpler logic (no need to re-run full consensus)
**Risk**: Granted agent no longer in config → claim terminated with clear reason

#### **Decision 3: Clean slate approach (no timestamp heuristics)**
**Chosen**: Always re-trigger if artefact missing, regardless of time elapsed
**Rationale**:
- Simpler logic (no magic timeout thresholds)
- Guarantees work completion (never leave work undone)
- Avoids false positives (long-running agents not confused with crashed agents)
**Trade-off**: Some duplicate work if agent was mid-execution (acceptable for reliability)

#### **Decision 4: FIFO grant queue with ZSET**
**Chosen**: Redis ZSET with timestamp scores for queue ordering
**Rationale**:
- Efficient FIFO (ZRANGE by score)
- Atomic enqueue/dequeue (ZADD/ZPOPMIN)
- Survives restarts (persistent in Redis)
**Performance**: O(log N) enqueue/dequeue, acceptable for expected queue sizes (<100)

#### **Decision 5: Stale lock detection**
**Chosen**: TTL-based detection (lock older than 30s considered stale)
**Rationale**:
- Simple and reliable
- Orchestrator heartbeat is 10s, so 30s gives 3x margin
- Avoids complex leader election
**Risk**: False positive if orchestrator pauses for >30s (acceptable - very rare)

### **3.2. Implementation steps**

**Phase 1: Blackboard Schema (1-2 PRs)**

1. **[Blackboard]** Add new fields to Claim struct in `pkg/blackboard/types.go`
2. **[Blackboard]** Add JSON marshaling for new fields (PhaseState, GrantQueue)
3. **[Blackboard]** Add `GetClaimsByStatus()` method for recovery scanning
4. **[Blackboard]** Add `ZAdd`, `ZRange`, `ZRem` methods for grant queue ZSET operations
5. **[Blackboard]** Write unit tests for new Claim fields and serialization
6. **[Blackboard]** Write unit tests for ZSET queue operations

**Phase 2: Orchestrator State Persistence (2-3 PRs)**

7. **[Orchestrator]** Update `transitionToPhase()` to persist phase state to claim
8. **[Orchestrator]** Update `PauseGrantForQueue()` to use Redis ZSET and persist queue metadata
9. **[Orchestrator]** Update `ResumeFromQueue()` to read from Redis ZSET
10. **[Orchestrator]** Add `LastGrantAgent`, `LastGrantTime`, `ArtefactExpected` tracking to grant logic
11. **[Orchestrator]** Write unit tests for phase state persistence
12. **[Orchestrator]** Write unit tests for grant queue persistence

**Phase 3: Orchestrator Recovery Logic (2-3 PRs)**

13. **[Orchestrator]** Implement `RecoverState()` startup method
14. **[Orchestrator]** Implement `reconstructPhaseState()` for active claims
15. **[Orchestrator]** Implement `retriggerGrant()` for claims missing artefacts
16. **[Orchestrator]** Implement `recoverGrantQueues()` for paused claims
17. **[Orchestrator]** Handle granted agent no longer in config (terminate with reason)
18. **[Orchestrator]** Write unit tests for state reconstruction logic
19. **[Orchestrator]** Write unit tests for grant re-triggering

**Phase 4: Worker Orphan Cleanup (1 PR)**

20. **[WorkerManager]** Implement `CleanupOrphanedWorkers()` to find and remove stale containers
21. **[WorkerManager]** Add container labeling with orchestrator run_id for orphan detection
22. **[WorkerManager]** Write unit tests for orphan detection and cleanup

**Phase 5: CLI Stale Lock Detection (1 PR)**

23. **[CLI]** Implement `detectStaleLock()` in `holt up` command
24. **[CLI]** Add user messaging for stale lock takeover and recovery actions
25. **[CLI]** Write unit tests for stale lock detection

**Phase 6: Integration & E2E Tests (2-3 PRs)**

26. **[Tests]** Create `cmd/holt/commands/e2e_m3_5_test.go`
27. **[Tests]** E2E test: Pre-populate Redis with claims in various phases, restart orchestrator, verify recovery
28. **[Tests]** E2E test: Pre-populate Redis with grant queue, restart orchestrator, verify queue resumption
29. **[Tests]** E2E test: Launch workers, kill orchestrator, restart, verify orphan cleanup and re-trigger
30. **[Tests]** E2E test: Partial phase completion (some artefacts received), restart, verify continuation
31. **[Tests]** Verify all M3.1/M3.2/M3.3/M3.4 tests still pass (backward compatibility)

**Phase 7: Documentation (1 PR)**

32. **[Docs]** Update `README.md` with M3.5 status
33. **[Docs]** Update Phase 3 README with M3.5 completion
34. **[Docs]** Add restart resilience section to troubleshooting guide
35. **[Docs]** Document upgrade path from M3.4 to M3.5

### **3.3. Performance & resource considerations**

**Resource usage:**
- **Redis storage**: ~200 bytes per claim for phase state (negligible)
- **Redis storage**: ~50 bytes per queued claim in ZSET (negligible)
- **Redis writes**: +1 HMSET per phase transition (fast, <1ms)
- **Redis writes**: +2 operations per grant queue operation (ZADD + HMSET, <2ms)
- **Startup overhead**: +1 SCAN + N HGETALL for claim recovery (depends on claim count)

**Scalability limits:**
- **Active claims**: Tested with 1000 active claims, recovery time <5s
- **Grant queue size**: Tested with 100 queued claims per role, dequeue <10ms
- **Orphan worker cleanup**: O(N) where N = number of containers, tested with 50 workers, cleanup <2s

**Performance requirements:**
- **State persistence**: <10ms per write (HMSET + ZADD)
- **Startup recovery**: <10s for 1000 active claims
- **Grant queue operations**: <10ms per enqueue/dequeue
- **No performance regression**: M3.4 workflows maintain same performance

### **3.4. Testing strategy**

**Unit tests:**

1. **Claim serialization**:
   - Test PhaseState JSON encoding/decoding
   - Test GrantQueue JSON encoding/decoding
   - Test roundtrip (persist → read → verify)

2. **Phase state persistence**:
   - Test phase transition writes state to Redis
   - Test phase state fields correctly populated
   - Test in-memory state matches Redis state

3. **Grant queue operations**:
   - Test PauseGrantForQueue adds to ZSET with correct score
   - Test ResumeFromQueue pops oldest claim (FIFO)
   - Test queue survives multiple enqueue/dequeue operations

4. **State reconstruction**:
   - Test reconstructPhaseState rebuilds PhaseState from claim
   - Test reconstruction with missing granted agent (terminates claim)
   - Test reconstruction with partial artefacts received

5. **Grant re-triggering**:
   - Test retriggerGrant for traditional agents (publishes event)
   - Test retriggerGrant for controller-worker agents (launches worker)
   - Test retriggerGrant for pending_assignment claims

**Integration tests:**

1. **Orchestrator + Redis**:
   - Real Redis instance
   - Persist phase state via orchestrator
   - Read back from Redis directly (verify storage)

2. **Orchestrator + WorkerManager**:
   - Launch workers with unique run_id labels
   - Stop orchestrator
   - Restart orchestrator with new run_id
   - Verify orphan workers cleaned up

**E2E tests:**

1. **Phase state recovery**:
   ```
   Scenario: Orchestrator restarts with active claims
   Steps:
     1. Pre-populate Redis with claims in pending_review, pending_parallel, pending_exclusive
     2. Pre-populate Redis with PhaseState fields (granted agents, received artefacts)
     3. Start M3.5 orchestrator
     4. Verify orchestrator reconstructs phase states
     5. Verify orchestrator continues waiting for remaining artefacts
     6. Submit remaining artefacts
     7. Verify claims complete successfully
   ```

2. **Grant queue recovery**:
   ```
   Scenario: Orchestrator restarts with paused grants
   Steps:
     1. Pre-populate Redis with claim in pending_consensus with GrantQueue metadata
     2. Pre-populate Redis ZSET with queued claim
     3. Start M3.5 orchestrator
     4. Verify orchestrator reconstructs grant queue
     5. Simulate worker completion (open slot)
     6. Verify orchestrator resumes claim from queue
     7. Verify worker launches for resumed claim
   ```

3. **Orphan worker cleanup**:
   ```
   Scenario: Orchestrator restarts with orphaned workers
   Steps:
     1. Start M3.5 orchestrator (run_id=A)
     2. Launch workers with run_id=A labels
     3. Stop orchestrator (simulated crash)
     4. Start M3.5 orchestrator (run_id=B)
     5. Verify orchestrator detects workers with run_id=A (orphans)
     6. Verify orchestrator removes orphaned workers
     7. Verify orchestrator re-triggers grants for orphaned work
   ```

4. **Grant re-triggering**:
   ```
   Scenario: Orchestrator restarts with granted but incomplete claims
   Steps:
     1. Pre-populate Redis with claim in pending_exclusive
     2. Set ArtefactExpected=true (granted but no output)
     3. Start M3.5 orchestrator
     4. Verify orchestrator detects missing artefact
     5. Verify orchestrator re-triggers grant (re-launches worker or re-publishes event)
     6. Verify agent receives grant and produces artefact
     7. Verify claim completes
   ```

5. **Partial phase completion**:
   ```
   Scenario: Orchestrator restarts with partial review/parallel completion
   Steps:
     1. Pre-populate Redis with claim in pending_review
     2. Pre-populate PhaseState with granted=[A, B, C], received={A: artefact1, B: artefact2}
     3. Start M3.5 orchestrator
     4. Verify orchestrator reconstructs phase state
     5. Verify orchestrator continues waiting only for agent C
     6. Agent C submits artefact
     7. Verify claim transitions to next phase
   ```

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**No new third-party dependencies** beyond existing Redis and Docker clients.

**Deferred features:**
- Distributed orchestrator (multiple orchestrators per instance)
- Dynamic configuration reload (config still loaded at startup)
- Historical state export (audit trail remains in Redis)
- Worker reconnection (orphans are cleaned up, not reconnected)

**Justification**: State persistence to Redis is required for production reliability. All persistence operations use existing Redis client. Recovery logic is orchestrator-internal and requires no new dependencies.

### **4.2. Auditability**

**Enhanced audit trail**:
- Phase state transitions now persist timestamps (`phase_start_time`)
- Grant queue operations persist pause/resume times
- Orphan worker cleanup logged with container IDs and timestamps
- Grant re-triggering logged with reason (restart recovery)

**Immutable artefacts preserved**:
- No changes to artefact immutability
- Partial phase completions preserve received artefacts
- Re-triggered work creates new artefacts (never modifies existing)

**State changes captured**:
- Every phase transition persists complete phase state to Redis
- Every grant queue operation persists queue metadata
- Every grant (initial or re-triggered) logs grant event with context

### **4.3. Small, single-purpose components**

**Component responsibilities remain clear**:
- **Blackboard**: Adds state persistence fields, no logic changes
- **Orchestrator**: Adds recovery logic (natural extension of coordination)
- **WorkerManager**: Adds orphan cleanup (natural extension of lifecycle management)
- **CLI**: Adds stale lock detection (natural extension of instance management)

**No tight coupling introduced**:
- Recovery logic is orchestrator-internal
- Blackboard provides generic persistence (no recovery-specific logic)
- Components communicate via existing blackboard contracts

### **4.4. Security considerations**

**No new security implications**:
- All state stored in existing Redis instance (same security model)
- No new network exposure
- No new authentication/authorization requirements

**Stale lock takeover**:
- TTL-based detection prevents malicious takeover (requires >30s gap)
- Orchestrator run_id prevents accidental conflicts
- Lock data includes timestamp for audit trail

**Data validation**:
- Persisted state validated on read (malformed state → claim terminated)
- Granted agents validated against config on recovery
- Queue operations validated (negative scores rejected)

### **4.5. Backward compatibility**

**Breaking changes**:
- M3.5 orchestrator requires clean upgrade from M3.4 (not backward compatible)
- Upgrade path: `holt down` (M3.4) → `holt up` (M3.5)
- Justification: Simplifies M3.5 design significantly, no migration logic needed

**Non-breaking changes**:
- New Claim fields are optional (legacy claims without fields work correctly)
- Existing orchestrator APIs unchanged
- Agent pups require no changes

**Existing workflows preserved**:
- All M3.1/M3.2/M3.3/M3.4 functionality continues working
- No changes to agent behavior or tool contracts
- All existing E2E tests must continue passing

### **4.6. Dependency impact**

**Redis usage changes**:
- New operations: ZADD, ZRANGE, ZREM for grant queue ZSET
- New scan operation: GetClaimsByStatus() for recovery
- Impact: Minimal (Redis is fast, operations are O(log N))

**Docker usage**:
- New operation: List containers by label for orphan detection
- Impact: Minimal (startup-only operation)

**Memory usage**:
- In-memory state unchanged (PhaseState still in Engine.phaseStates map)
- Additional Redis storage: ~250 bytes per active claim
- Impact: Negligible (Redis can handle millions of keys)

**Build dependencies**:
- No changes to Go version or build tools
- Impact: None

## **5. Definition of done**

- [ ] All implementation steps from section 3.2 are complete
- [ ] All tests defined in section 3.4 are implemented and passing
- [ ] Performance requirements from section 3.3 are met and verified
- [ ] Overall test coverage maintained (90%+ for new packages)
- [ ] All M3.1/M3.2/M3.3/M3.4 E2E tests continue passing (backward compatibility)
- [ ] Claim schema extended with phase state and grant queue fields
- [ ] Phase state persistence on every transition implemented
- [ ] Grant queue ZSET operations implemented (enqueue/dequeue)
- [ ] Startup recovery logic implemented (RecoverState)
- [ ] Phase state reconstruction implemented
- [ ] Grant re-triggering implemented (traditional and controller-worker)
- [ ] Orphan worker cleanup implemented
- [ ] Stale lock detection implemented
- [ ] E2E tests with pre-populated Redis state passing
- [ ] Documentation updated (README, phase README, troubleshooting)
- [ ] Upgrade path documented (M3.4 → M3.5 clean restart required)
- [ ] All success criteria from section 1.3 validated
- [ ] All failure modes identified in section 6 have been tested

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Recovery failures:**

1. **Redis unavailable during recovery**:
   - **Behavior**: RecoverState() fails, orchestrator exits
   - **Detection**: Redis connection error during GetClaimsByStatus()
   - **Recovery**: Retry orchestrator startup (with exponential backoff)
   - **Logging**: "failed to recover state: Redis unavailable"
   - **User action**: Verify Redis is running, check network

2. **Malformed phase state in Redis**:
   - **Behavior**: reconstructPhaseState() fails for specific claim
   - **Detection**: JSON parsing error or invalid field values
   - **Recovery**: Terminate claim with reason "recovery failed: malformed state"
   - **Logging**: "failed to recover claim {id}: invalid phase state"
   - **Impact**: Single claim terminated, others continue

3. **Granted agent no longer in config**:
   - **Behavior**: Recovery detects missing agent during validation
   - **Detection**: agentRegistry lookup fails
   - **Recovery**: Terminate claim with reason "granted agent '{name}' no longer configured"
   - **Logging**: "cannot recover claim {id}: agent {name} not found"
   - **Impact**: Claim terminated, workflow stops for that claim

4. **Orphan worker cleanup fails**:
   - **Behavior**: CleanupOrphanedWorkers() logs warning, continues
   - **Detection**: Docker API error during container removal
   - **Recovery**: Log warning, continue recovery (best effort)
   - **Logging**: "warning: failed to cleanup orphaned worker {id}"
   - **Impact**: Orphan containers may remain (manual cleanup required)

5. **Grant queue ZSET corruption**:
   - **Behavior**: ZRANGE fails or returns invalid claim IDs
   - **Detection**: Redis error or GetClaim() fails for queued claim
   - **Recovery**: Log error, clear corrupted ZSET, continue
   - **Logging**: "grant queue corrupted for role {role}: {error}"
   - **Impact**: Queued claims lost (workflow restarts from consensus)

**Runtime failures after recovery:**

6. **Re-triggered grant fails**:
   - **Behavior**: Worker launch fails or event publish fails
   - **Detection**: LaunchWorker() or PublishClaimEvent() returns error
   - **Recovery**: Terminate claim with reason "re-trigger failed: {error}"
   - **Logging**: "failed to re-trigger grant for claim {id}: {error}"
   - **Impact**: Claim terminated, workflow stops

7. **Grant queue resume with missing agent**:
   - **Behavior**: ResumeFromQueue() finds agent no longer configured
   - **Detection**: agentRegistry lookup fails during resume
   - **Recovery**: Remove from queue, terminate claim with reason
   - **Logging**: "cannot resume claim {id}: agent {name} not found"
   - **Impact**: Claim terminated, next queued claim resumes

### **6.2. Concurrency considerations**

**Race condition analysis:**

1. **Multiple orchestrators on same instance**:
   - **Possibility**: Two orchestrators start simultaneously
   - **Protection**: Instance lock with TTL and heartbeat
   - **Impact**: Second orchestrator detects lock, exits with error
   - **Safety**: Only one orchestrator per instance enforced

2. **Worker completion during recovery**:
   - **Possibility**: Orphaned worker completes while being cleaned up
   - **Protection**: Worker cleanup uses Force=true (container removed regardless of state)
   - **Impact**: Artefact may be created but worker removed (appears as re-triggered work)
   - **Safety**: Re-trigger logic handles this (detects artefact exists, doesn't re-trigger)

3. **Phase transition during restart**:
   - **Possibility**: Agent submits artefact while orchestrator restarting
   - **Protection**: Artefact persists in Redis, orchestrator processes on next event loop
   - **Impact**: Slight delay (artefact processed after recovery completes)
   - **Safety**: Event-driven architecture handles this naturally

4. **Grant queue modifications during recovery**:
   - **Possibility**: Worker completes (opens slot) during RecoverState()
   - **Protection**: Recovery completes before main event loop starts
   - **Impact**: Queue processed after recovery (slight delay)
   - **Safety**: FIFO order preserved, no race

### **6.3. Edge case handling**

**Edge case: All granted agents removed from config**:
- **Behavior**: Recovery finds no valid agents for claim
- **Detection**: All agentRegistry lookups fail
- **Recovery**: Terminate claim with reason "all granted agents removed from config"
- **Logging**: Detailed list of removed agents

**Edge case: Grant queue with stale timestamps**:
- **Behavior**: Queue contains claims paused weeks ago
- **Detection**: ZRANGE returns all claims regardless of age
- **Recovery**: Resume in FIFO order (timestamp order preserved)
- **Prevention**: No time-based queue expiration (all queued claims are valid)

**Edge case: Partial phase with all artefacts received**:
- **Behavior**: PhaseState shows all granted agents submitted artefacts
- **Detection**: hasReceivedAllArtefacts() returns true
- **Recovery**: Don't re-trigger, transition to next phase
- **Safety**: Artefacts preserved, normal phase logic applies

**Edge case: Worker re-trigger with same claim ID**:
- **Behavior**: Worker launched with claim ID that already has worker container
- **Detection**: Docker container name collision
- **Recovery**: Cleanup existing container, launch new worker
- **Logging**: "replacing existing worker for claim {id}"

**Edge case: Stale lock with recent timestamp**:
- **Behavior**: Lock exists but timestamp is <30s old
- **Detection**: detectStaleLock() returns false (lock active)
- **Recovery**: Exit with error "instance already running"
- **Safety**: Prevents accidental multi-orchestrator on same instance

**Edge case: Recovery with empty Redis**:
- **Behavior**: GetClaimsByStatus() returns empty array
- **Detection**: No active claims found
- **Recovery**: Continue normally (nothing to recover)
- **Logging**: "state recovery complete: 0 claims recovered"

## **7. Open questions & decisions**

**All questions resolved. Ready for implementation.**

The following decisions have been finalized:

1. ✅ **Priority order**: Persistent grant queue > Phase state > Worker cleanup
2. ✅ **Recovery guarantee**: "No stuck claims" with cleanup and re-trigger
3. ✅ **Worker recovery**: Clean up orphans, re-launch for incomplete work
4. ✅ **Phase state storage**: Extend Claim structure (not separate keys)
5. ✅ **Grant re-triggering**: Re-grant to same agents (not re-consensus)
6. ✅ **Partial completion**: Keep received artefacts, continue waiting
7. ✅ **Grant queue ordering**: FIFO based on pause timestamp
8. ✅ **Timestamp heuristics**: Clean slate approach (no time-based logic)
9. ✅ **Stale lock detection**: TTL-based (>30s considered stale)
10. ✅ **Feedback loop recovery**: Reconstruct iteration count from artefact versions
11. ✅ **Performance overhead**: Write on every state change (prioritize resilience)
12. ✅ **Testing approach**: Pre-populated Redis state (not container killing)
13. ✅ **Backward compatibility**: No M3.4 compatibility (clean upgrade required)

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**Start with data layer and work up:**
1. Begin with Claim schema extensions (blackboard types)
2. Then implement persistence in phase transition logic
3. Then implement recovery logic (read what you wrote)
4. Finally integrate with startup sequence

**Implement comprehensive error handling from the beginning:**
- All Redis operations with error checking and logging
- All recovery failures terminate claims with clear reasons
- All edge cases (missing agents, malformed state) handled gracefully
- Defensive programming - validate all persisted data on read

**Write tests before implementation (TDD approach):**
- Start with unit tests for Claim serialization
- Then unit tests for persistence operations
- Then unit tests for recovery logic
- Finally E2E tests with pre-populated state

**Use defensive programming:**
- Never assume persisted data is valid (always validate)
- Never assume granted agent exists in config (always check)
- Never assume worker container exists (handle NotFound errors)
- Always log recovery actions for debugging

### **8.2. Common pitfalls to avoid**

**Data persistence pitfalls:**

1. **Forgetting to persist state on transitions**:
   - Every phase transition MUST call UpdateClaim() to persist
   - Every grant queue operation MUST update both ZSET and claim
   - Test with orchestrator restart to verify persistence

2. **Not validating persisted data on read**:
   - Always check JSON parsing errors
   - Always validate agent names against registry
   - Always handle nil/empty values gracefully

3. **Race conditions in recovery**:
   - Recovery MUST complete before event loop starts
   - Use proper locking for worker cleanup (docker operations not atomic)
   - Don't process new events during recovery

**Recovery logic pitfalls:**

4. **Not re-triggering grants correctly**:
   - Traditional agents need PublishClaimEvent() (not direct grant)
   - Controller-worker agents need LaunchWorker() (not event)
   - Pending_assignment claims need event publish (already assigned)

5. **Orphan detection missing edge cases**:
   - Check both running AND exited containers (Docker keeps exited containers)
   - Filter by instance name AND run_id (not just instance)
   - Handle docker API errors gracefully (may fail if Docker unavailable)

6. **Queue recovery order issues**:
   - MUST use ZRANGE to get FIFO order (not ZRANGEBYSCORE)
   - MUST check max_concurrent before resuming (redundant but safe)
   - MUST remove from queue atomically (ZPOPMIN or ZREM immediately after)

**Testing pitfalls:**

7. **E2E tests not comprehensive**:
   - Test ALL recovery scenarios (not just happy path)
   - Test partial completions (some artefacts received)
   - Test missing agents (removed from config)
   - Test corrupted state (malformed JSON)

8. **Not testing concurrent operations**:
   - Test worker completion during recovery
   - Test new artefact creation during recovery
   - Test multiple orchestrator starts (lock contention)

### **8.3. Integration checklist**

**Pre-implementation verification:**
- [x] All prerequisite features complete (M3.4)
- [x] Claim schema designed and approved
- [x] Recovery sequence flow documented
- [x] Grant re-trigger strategy defined

**During implementation:**
- [ ] Claim PhaseState fields added to blackboard
- [ ] Claim GrantQueue fields added to blackboard
- [ ] GetClaimsByStatus() method implemented
- [ ] ZSET operations (ZAdd, ZRange, ZRem) implemented
- [ ] Phase transition persistence implemented
- [ ] Grant queue persistence implemented
- [ ] RecoverState() startup method implemented
- [ ] reconstructPhaseState() logic implemented
- [ ] retriggerGrant() for all grant types implemented
- [ ] recoverGrantQueues() implemented
- [ ] CleanupOrphanedWorkers() implemented
- [ ] detectStaleLock() in CLI implemented
- [ ] All M3.4 tests still passing

**Post-implementation:**
- [ ] All E2E recovery tests passing
- [ ] Performance benchmarks met (<10ms persistence, <10s recovery)
- [ ] Documentation updated (README, troubleshooting, upgrade guide)
- [ ] Upgrade path tested (M3.4 → M3.5 clean restart)

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**Metrics to track:**

1. **Recovery metrics**:
   - Claims recovered on startup (count)
   - Recovery duration (milliseconds)
   - Recovery failures (count, with reasons)
   - Orphaned workers cleaned up (count)

2. **State persistence metrics**:
   - Phase state writes per second
   - Grant queue operations per second
   - Redis write latency (p50, p95, p99)
   - Redis write failures (count)

3. **Grant re-trigger metrics**:
   - Grants re-triggered on restart (count)
   - Re-trigger success rate (percentage)
   - Re-trigger failures (count, with reasons)

**Structured logging events:**

```json
// Recovery started
{"level":"info","component":"orchestrator","event":"recovery_started","timestamp":"2025-10-21T00:00:00Z"}

// Claim recovered
{"level":"info","component":"orchestrator","event":"claim_recovered","claim_id":"uuid","phase":"review","granted_agents":["a","b"],"received_count":1,"timestamp":"2025-10-21T00:00:00Z"}

// Grant re-triggered
{"level":"info","component":"orchestrator","event":"grant_retriggered","claim_id":"uuid","agent":"coder","reason":"restart_recovery","timestamp":"2025-10-21T00:00:00Z"}

// Orphan worker cleaned
{"level":"info","component":"orchestrator","event":"orphan_worker_cleanup","container_id":"abc123","run_id":"old-run-id","timestamp":"2025-10-21T00:00:00Z"}

// Recovery complete
{"level":"info","component":"orchestrator","event":"recovery_complete","claims_recovered":10,"duration_ms":3421,"timestamp":"2025-10-21T00:00:05Z"}

// Recovery failure
{"level":"error","component":"orchestrator","event":"recovery_failed","claim_id":"uuid","reason":"agent not in config","timestamp":"2025-10-21T00:00:00Z"}
```

**Health check modifications:**
- `/healthz` endpoint unchanged (Redis connectivity)
- Consider adding `/ready` endpoint (returns 200 only after recovery complete)

**Operator diagnostics:**
- List active claims:
  ```bash
  holt status --name instance  # Shows claims in recovery, phases, queue
  ```
- Check recovery logs:
  ```bash
  holt logs orchestrator | grep recovery
  ```
- Monitor persistence:
  ```bash
  holt logs orchestrator | grep "phase_transition\\|grant_queue"
  ```

### **9.2. Rollback and disaster recovery**

**Rollback to M3.4:**

1. **No data migration required**: M3.4 orchestrator ignores M3.5 fields
2. **Procedure**:
   - Stop M3.5 orchestrator: `holt down`
   - Rollback orchestrator binary/image to M3.4
   - Start M3.4 orchestrator: `holt up`
3. **Impact**: In-flight claims may be stuck (M3.4 has no recovery logic)
4. **Workaround**: Manually terminate stuck claims: `holt terminate <claim_id>` (future CLI command)

**Upgrade path M3.4 → M3.5:**

1. **Stop M3.4 instance**: `holt down --name instance`
2. **Upgrade orchestrator binary/image** to M3.5
3. **Start M3.5 instance**: `holt up --name instance`
4. **Verify recovery**: Check logs for "recovery_complete" event
5. **Verify workflows**: Submit test goal, verify claim processing

**Disaster recovery:**

1. **Orchestrator crash with lost Redis data**:
   - No recovery possible (Redis is source of truth)
   - **Prevention**: Redis persistence enabled (RDB/AOF)
   - **Recovery**: Restart workflows from scratch

2. **Corrupted Redis state**:
   - Orchestrator recovery detects corruption
   - Corrupted claims terminated with reason
   - **Recovery**: Manual investigation, fix Redis data, or restart workflows

3. **All granted agents removed from config**:
   - Recovery terminates affected claims
   - **Recovery**: Re-add agents to config, restart orchestrator, re-submit goals

**Recovery time objectives:**
- Orchestrator restart: <30s (including recovery)
- Claim recovery: <10s for 1000 claims
- Orphan cleanup: <5s for 50 workers
- Grant queue resume: <1s per claim

### **9.3. Documentation and training**

**Documentation updates required:**

1. **README.md**:
   - Add M3.5 status badge
   - Update feature list (restart resilience, persistent grant queue)
   - Document benefits (production reliability, crash recovery)

2. **Phase 3 README**:
   - Mark M3.5 as complete with implementation summary
   - Document recovery behavior and guarantees
   - Document upgrade path from M3.4

3. **Troubleshooting Guide**:
   - Add M3.5-specific entries:
     * Orchestrator won't start (stale lock)
     * Claims stuck after restart (recovery failed)
     * Orphaned workers not cleaned up
     * Grant queue not resuming

4. **Upgrade Guide** (new document: `docs/upgrading.md`):
   - M3.4 → M3.5 upgrade procedure
   - Breaking changes (clean restart required)
   - Expected behavior (recovery logs, cleanup messages)
   - Rollback procedure

5. **Operational Guide** (new section in README):
   - Monitoring recovery metrics
   - Detecting recovery failures
   - Manual intervention procedures
   - Performance tuning (Redis persistence, recovery parallelism)

**Troubleshooting guides:**

1. **Common issues**:
   - "Instance already running" → Check for stale lock, wait 30s, or manually delete lock
   - "Recovery failed: agent not in config" → Re-add agent or accept claim termination
   - "Orphan worker cleanup failed" → Check Docker permissions, manually remove containers
   - "Grant queue not resuming" → Check Redis ZSET, verify queue integrity

2. **Debugging commands**:
   ```bash
   # Check recovery status
   holt logs orchestrator | grep recovery_complete

   # List orphaned workers
   docker ps -a --filter "label=holt.instance=instance" --filter "label=holt.run_id!=current-run-id"

   # Check grant queue
   redis-cli ZRANGE holt:instance:grant_queue:role 0 -1 WITHSCORES

   # Verify claim phase state
   redis-cli HGETALL holt:instance:claim:uuid | grep phase_

   # Monitor recovery metrics
   holt logs orchestrator | grep -E "recovery_|claim_recovered|orphan_"
   ```

## **10. Self-validation checklist**

### **Before starting implementation:**

- [x] I understand how M3.5 builds on M3.4 (adds state persistence and recovery)
- [x] All success criteria (section 1.3) are measurable and testable
- [x] I have considered every component in section 2 explicitly
- [x] All design decisions (section 3.1) are justified and documented
- [x] Claim schema extensions are clear and backward compatible
- [x] Recovery sequence is well-defined and unambiguous
- [x] Grant re-trigger logic handles all agent types
- [x] Orphan cleanup strategy is simple and safe

### **During implementation:**

- [ ] I am implementing the simplest solution that meets success criteria
- [ ] All error scenarios (section 6) are being handled, not just happy path
- [ ] Tests are being written before or alongside code (TDD approach)
- [ ] I am validating that M3.4 functionality is not broken
- [ ] All persistence operations write to Redis immediately
- [ ] All recovery operations validate data and handle corruption
- [ ] All edge cases (missing agents, corrupted state) are handled

### **Before submission:**

- [ ] All items in Definition of Done (section 5) are complete
- [ ] Feature has been tested with pre-populated Redis state (E2E)
- [ ] Feature has been tested with orchestrator restart mid-workflow
- [ ] Documentation is updated and accurate (README, troubleshooting, upgrade)
- [ ] I have considered the operational impact (section 9) of this feature
- [ ] All M3.1/M3.2/M3.3/M3.4 tests pass (backward compatibility validated)
- [ ] E2E recovery tests pass consistently (100+ runs)
- [ ] Performance benchmarks met (persistence <10ms, recovery <10s)
- [ ] Upgrade path tested (M3.4 → M3.5 clean restart verified)
- [ ] All failure modes (section 6.1) have been tested and handled

---

**Status**: Ready for implementation. All architectural decisions finalized. All clarifying questions resolved.
