# **Feature design: Governance & Reliability Enhancements (M3.9)**

**Purpose**: Hardening features for production-grade auditability, observability, and reliability
**Scope**: Three critical governance features delivered as a single cohesive milestone
**Estimated tokens**: ~12,000 tokens
**Read when**: Implementing audit trails, health checks, or timestamp precision requirements

Associated phase: **Phase 3 (Coordination)** - Production hardening for multi-agent systems
Status: **Draft**

***Milestone purpose:*** This document specifies three critical governance and reliability features that transform Holt from a functional prototype into a production-grade, auditable orchestration platform suitable for regulated industries and mission-critical workflows.

## **1. The 'why': goal and success criteria**

### **1.1. Goal statement**

Enhance Holt's governance, auditability, and operational reliability by:
1. **Agent Version Auditing**: Linking every granted claim to the exact container image version that executed it
2. **High-Precision Timestamps**: Providing millisecond-precision timestamps for accurate event sequencing
3. **Configurable Health Checks**: Enabling agents to define custom health criteria beyond simple Redis connectivity

These features collectively ensure Holt meets the stringent requirements of regulated industries (finance, healthcare, government) where complete audit trails and operational transparency are mandatory.

### **1.2. User story**

**As a compliance officer** in a regulated financial institution, I need to:
- Know exactly which version of which agent executed a specific claim (for reproducibility and security audits)
- See the precise order of events with millisecond accuracy (for regulatory reporting)
- Configure health checks that verify agent-specific readiness (beyond generic infrastructure checks)

**So that** I can confidently use Holt for workflows that must pass external audits and meet regulatory requirements like SOC 2, HIPAA, or PCI-DSS.

### **1.3. Success criteria**

**Feature 1: Agent Version Auditing**
1. Every granted claim in Redis contains `granted_agent_image_id` field with full sha256 digest
2. `holt watch` output displays agent image IDs in format: `agent=Writer@sha256:f2a3b4c5...`
3. `claim_granted` workflow events include `agent_image_id` field
4. For controller-worker agents, the worker's image ID (not controller's) is recorded
5. `holt up` fails gracefully if docker inspect cannot resolve image IDs

**Feature 2: High-Precision Timestamps**
1. All timestamps in Redis use millisecond precision (stored as Unix milliseconds)
2. Artefacts have new `created_at_ms` field
3. `holt watch` displays timestamps as "HH:MM:SS.mmm" format
4. Clean break from old data - no backward compatibility required

**Feature 3: Configurable Health Checks**
1. Agents can define custom `health_check` in holt.yml with command, interval, timeout
2. Pup executes health checks in agent's environment (workspace, env vars, user)
3. `/healthz` endpoint reflects custom health check results (200 or 503)
4. Defaults to Redis PING if `health_check` not configured (backward compatible)
5. Concurrent checks are skipped, failures are logged

### **1.4. Non-goals**

**Feature 1 Non-goals:**
- **No runtime image change detection**: If a container's image changes mid-flight (e.g., manual intervention), behavior is undefined. This is out of scope.
- **No historical image lookup**: We only track the image ID at grant time, not the full history of an agent's image changes across restarts.

**Feature 2 Non-goals:**
- **No backward compatibility**: Old instances with second-precision timestamps will be incompatible. This is intentional.
- **No migration tooling**: Users must `holt down` before upgrading to M3.9.

**Feature 3 Non-goals:**
- **No automatic remediation**: Health check failures only log warnings and return 503. No automatic container restarts or agent removal.
- **No centralized health monitoring**: Health status is exposed via `/healthz` but not aggregated or published to a monitoring channel.

## **2. The 'what': component impact analysis**

**Critical validation questions for this entire section:**
* Have I explicitly considered EVERY component (Blackboard, Orchestrator, Pup, CLI)? **YES**
* For components marked "No changes" - am I absolutely certain this feature doesn't affect them? **YES - validated below**
* Do my changes maintain the contracts and interfaces defined in the design documents? **YES - extends, does not break**
* Will this feature work correctly with both single-instance and scaled agents (controller-worker pattern)? **YES - explicit handling for workers**

### **2.1. Blackboard changes**

**Schema Changes**

**New Redis Key:**
```
holt:{instance_name}:agent_images (HASH)
  - Key: agent role (e.g., "Writer")
  - Value: Docker ImageID (e.g., "sha256:f2a3b4c5d6e7...")
  - Populated by: CLI (holt up) for traditional/controller agents
  - Read by: Orchestrator when granting claims
```

**Updated Artefact Schema:**
```go
type Artefact struct {
    ID                string   `json:"id"`
    LogicalID         string   `json:"logical_id"`
    Version           int      `json:"version"`
    StructuralType    string   `json:"structural_type"`
    Type              string   `json:"type"`
    Payload           string   `json:"payload"`
    SourceArtefacts   []string `json:"source_artefacts"`
    ProducedByRole    string   `json:"produced_by_role"`
    CreatedAtMs       int64    `json:"created_at_ms"`  // NEW: millisecond timestamp
}
```

**Updated Claim Schema:**
```go
type Claim struct {
    ID                     string   `json:"id"`
    ArtefactID             string   `json:"artefact_id"`
    Status                 string   `json:"status"`
    GrantedReviewAgents    []string `json:"granted_review_agents"`
    GrantedParallelAgents  []string `json:"granted_parallel_agents"`
    GrantedExclusiveAgent  string   `json:"granted_exclusive_agent"`
    GrantedAgentImageID    string   `json:"granted_agent_image_id"`  // NEW: image digest
    PhaseStartTimeMs       int64    `json:"phase_start_time_ms"`     // CHANGED: from phase_start_time
    // ... other fields ...
}
```

**Data Migration:**
- **Decision**: NO migration support. Old claims with `phase_start_time` (seconds) are incompatible.
- **Enforcement**: Orchestrator will fail to parse old claims, forcing clean restart.

**Backward Compatibility:**
- **Breaking change**: Instances running pre-M3.9 cannot upgrade in-place.
- **Mitigation**: Documented in upgrade guide - must run `holt down` before upgrading.

### **2.2. Orchestrator changes**

**Changes Required**

**1. Image ID Resolution (Feature 1)**

**For Traditional/Controller Agents:**
- Read from `holt:{instance}:agent_images` hash when granting claim
- Populate `granted_agent_image_id` field in Claim

**For Worker Agents:**
- At worker launch time, resolve image tag to digest:
  ```go
  // In WorkerManager.LaunchWorker()
  imageDigest, err := resolveImageDigest(workerConfig.Image)
  if err != nil {
      return fmt.Errorf("failed to resolve worker image: %w", err)
  }

  // Update claim with worker's image ID
  claim.GrantedAgentImageID = imageDigest
  ```

**Image Digest Resolution:**
```go
func resolveImageDigest(imageTag string) (string, error) {
    // Use Docker client to inspect image
    imageInfo, _, err := dockerClient.ImageInspectWithRaw(ctx, imageTag)
    if err != nil {
        return "", err
    }

    // Return RepoDigests[0] or ID as fallback
    if len(imageInfo.RepoDigests) > 0 {
        return imageInfo.RepoDigests[0], nil
    }
    return imageInfo.ID, nil
}
```

**2. Timestamp Precision (Feature 2)**

**All timestamp writes:**
- Change `time.Now().Unix()` to `time.Now().UnixMilli()`
- Update struct field names: `phase_start_time` â†’ `phase_start_time_ms`

**Examples:**
```go
// Before
claim.PhaseStartTime = time.Now().Unix()

// After
claim.PhaseStartTimeMs = time.Now().UnixMilli()
```

**3. Workflow Event Enrichment (Feature 1)**

**Claim Granted Event:**
```go
type ClaimGrantedEvent struct {
    ClaimID       string `json:"claim_id"`
    ArtefactID    string `json:"artefact_id"`
    GrantedTo     string `json:"granted_to"`
    GrantType     string `json:"grant_type"`
    AgentImageID  string `json:"agent_image_id"`  // NEW
    TimestampMs   int64  `json:"timestamp_ms"`    // CHANGED: ms precision
}
```

**No changes to:**
- Bidding logic
- Claim lifecycle state machine
- Phase transition logic (review â†’ parallel â†’ exclusive)

### **2.3. Agent pup changes**

**Changes Required**

**1. Health Check System (Feature 3)**

**Configuration Loading:**
```go
type HealthCheckConfig struct {
    Command  []string `yaml:"command"`
    Interval string   `yaml:"interval"`  // Default: "30s"
    Timeout  string   `yaml:"timeout"`   // Default: "5s"
}

type AgentConfig struct {
    // ... existing fields ...
    HealthCheck *HealthCheckConfig `yaml:"health_check"`
}
```

**Health Check Execution:**
```go
type HealthChecker struct {
    config       *HealthCheckConfig
    lastResult   atomic.Value  // stores bool: true = healthy, false = unhealthy
    checkRunning atomic.Bool
    workspace    string
    env          []string
}

func (hc *HealthChecker) Start() {
    interval, _ := time.ParseDuration(hc.config.Interval)
    ticker := time.NewTicker(interval)

    go func() {
        for range ticker.C {
            hc.runCheck()
        }
    }()
}

func (hc *HealthChecker) runCheck() {
    // Skip if previous check still running
    if !hc.checkRunning.CompareAndSwap(false, true) {
        log.Warn("Skipping health check - previous check still running")
        return
    }
    defer hc.checkRunning.Store(false)

    timeout, _ := time.ParseDuration(hc.config.Timeout)
    ctx, cancel := context.WithTimeout(context.Background(), timeout)
    defer cancel()

    cmd := exec.CommandContext(ctx, hc.config.Command[0], hc.config.Command[1:]...)
    cmd.Dir = hc.workspace
    cmd.Env = hc.env

    err := cmd.Run()
    healthy := err == nil

    hc.lastResult.Store(healthy)

    if !healthy {
        log.Warnf("Health check failed: %v", err)
    }
}

func (hc *HealthChecker) IsHealthy() bool {
    result := hc.lastResult.Load()
    if result == nil {
        return true  // No checks run yet, assume healthy
    }
    return result.(bool)
}
```

**Updated /healthz Endpoint:**
```go
func (p *Pup) healthzHandler(w http.ResponseWriter, r *http.Request) {
    var healthy bool

    if p.healthChecker != nil {
        // Use custom health check result
        healthy = p.healthChecker.IsHealthy()
    } else {
        // Backward compatible: Redis PING
        healthy = p.blackboard.Ping()
    }

    if healthy {
        w.WriteHeader(http.StatusOK)
        w.Write([]byte("OK"))
    } else {
        w.WriteHeader(http.StatusServiceUnavailable)
        w.Write([]byte("Unhealthy"))
    }
}
```

**2. Artefact Creation Timestamps (Feature 2)**

**When creating artefacts:**
```go
// In artefact creation logic
artefact := &blackboard.Artefact{
    ID:             uuid.New().String(),
    // ... other fields ...
    CreatedAtMs:    time.Now().UnixMilli(),  // NEW
}
```

**No changes to:**
- Bidding logic (bid.sh execution)
- Work execution logic (run.sh execution)
- Context assembly

### **2.4. CLI changes**

**Changes Required**

**1. Image ID Population (Feature 1)**

**In `holt up` command:**

```go
func (cmd *UpCommand) Run() error {
    // ... existing container startup logic ...

    // NEW: Populate agent_images hash
    if err := cmd.populateAgentImages(instanceName, agents); err != nil {
        return fmt.Errorf("failed to populate agent images: %w", err)
    }

    return nil
}

func (cmd *UpCommand) populateAgentImages(instance string, agents map[string]*config.Agent) error {
    redisKey := fmt.Sprintf("holt:%s:agent_images", instance)

    for role, agentConfig := range agents {
        // Skip workers - they're not running yet
        if agentConfig.Mode == "controller" {
            containerName := fmt.Sprintf("holt-%s-%s", instance, role)
        } else {
            containerName := fmt.Sprintf("holt-%s-%s", instance, role)
        }

        imageID, err := cmd.getContainerImageID(containerName)
        if err != nil {
            return fmt.Errorf("failed to get image ID for %s: %w", role, err)
        }

        // Store in Redis
        if err := cmd.redis.HSet(ctx, redisKey, role, imageID).Err(); err != nil {
            return fmt.Errorf("failed to store image ID for %s: %w", role, err)
        }

        log.Infof("Registered agent %s with image %s", role, imageID)
    }

    return nil
}

func (cmd *UpCommand) getContainerImageID(containerName string) (string, error) {
    containerInfo, err := cmd.dockerClient.ContainerInspect(ctx, containerName)
    if err != nil {
        return "", err
    }

    // Get image digest
    imageInfo, _, err := cmd.dockerClient.ImageInspectWithRaw(ctx, containerInfo.Image)
    if err != nil {
        return "", err
    }

    // Prefer RepoDigests (full sha256:...), fallback to ID
    if len(imageInfo.RepoDigests) > 0 {
        return imageInfo.RepoDigests[0], nil
    }
    return imageInfo.ID, nil
}
```

**Error Handling:**
- If `docker inspect` fails for any agent, `holt up` **fails completely**
- Clear error message: `"Failed to resolve image ID for agent 'Writer': permission denied. Cannot start instance without complete audit trail."`

**2. Watch Command Formatting (Features 1 & 2)**

**In `internal/watch/watch.go`:**

```go
// Update defaultFormatter

// Feature 1: Display agent@imageID
func formatClaimGranted(event ClaimGrantedEvent) string {
    // Truncate image ID to first 12 chars of digest
    imageShort := truncateImageID(event.AgentImageID)

    return fmt.Sprintf("ðŸ† Claim granted: agent=%s@%s, claim=%s, type=%s",
        event.GrantedTo,
        imageShort,
        event.ClaimID[:8],
        event.GrantType)
}

func truncateImageID(imageID string) string {
    // Extract sha256 hash and take first 12 chars
    // "sha256:f2a3b4c5d6e7..." -> "f2a3b4c5d6e7"
    if strings.HasPrefix(imageID, "sha256:") {
        hash := strings.TrimPrefix(imageID, "sha256:")
        if len(hash) >= 12 {
            return hash[:12]
        }
    }
    return imageID
}

// Feature 2: Millisecond timestamp display
func formatTimestamp(timestampMs int64) string {
    t := time.UnixMilli(timestampMs)
    return t.Format("15:04:05.000")  // Changed from "15:04:05"
}
```

**No changes to:**
- `holt down` command
- `holt forage` command
- `holt hoard` command (though output will show `created_at_ms` field)

---

## **3. The 'how': implementation & testing plan**

### **3.1. Key design decisions & risks**

#### **Decision 1: Image ID Source of Truth**

**Decision**: For traditional agents, CLI populates `agent_images` hash at startup. For workers, Orchestrator resolves on-demand at launch.

**Rationale**:
- CLI has full container context at `holt up` time
- Orchestrator has flexibility to resolve worker images dynamically
- Clear separation of concerns

**Risk**: Worker image resolution adds latency to claim grants.
**Mitigation**: Image resolution is fast (~10-50ms). Acceptable for audit benefit.

#### **Decision 2: No Backward Compatibility for Timestamps**

**Decision**: Clean break - old instances incompatible with M3.9.

**Rationale**:
- Attempting dual-format support adds significant complexity
- Governance features require clean slate for trust
- Forces users to think about upgrade implications

**Risk**: Users may lose in-flight work during upgrade.
**Mitigation**: Clearly documented in upgrade guide. Recommend finishing workflows before upgrading.

#### **Decision 3: Health Check Execution Environment**

**Decision**: Run health checks in exact same environment as agent work (workspace, env vars, user).

**Rationale**:
- Most realistic health assessment
- Catches environment-specific failures (permissions, missing files, env vars)
- Consistent with agent execution model

**Risk**: Health check command could have side effects in workspace.
**Mitigation**: Documented best practice - health checks should be read-only (e.g., `test -f /workspace/config.yml`).

#### **Decision 4: Skip Concurrent Health Checks**

**Decision**: If a health check is still running when interval triggers, skip the new check.

**Rationale**:
- Simpler than queueing or killing previous check
- Indicates agent is unhealthy (slow to respond)
- Prevents check accumulation under load

**Risk**: Long-running checks could cause extended unhealthy periods.
**Mitigation**: Documented guideline - health checks should complete in <50% of interval (e.g., 5s timeout for 30s interval).

### **3.2. Implementation steps**

**Phase 1: Blackboard Schema Updates**

1. Update `pkg/blackboard/types.go`:
   - Add `CreatedAtMs int64` to Artefact struct
   - Add `GrantedAgentImageID string` to Claim struct
   - Rename `PhaseStartTime` to `PhaseStartTimeMs` in Claim struct
2. Update JSON marshal/unmarshal logic
3. Write unit tests for new fields

**Phase 2: CLI Changes (Feature 1)**

1. Add Docker client to `holt up` command
2. Implement `populateAgentImages()` function
3. Implement `getContainerImageID()` with error handling
4. Add integration test for `agent_images` hash population
5. Add error handling test (docker inspect failure)

**Phase 3: Orchestrator Changes (Features 1 & 2)**

1. Update claim grant logic to read `agent_images` hash
2. Implement worker image resolution in `WorkerManager`
3. Update `claim_granted` event schema
4. Change all `time.Now().Unix()` to `time.Now().UnixMilli()`
5. Update struct field references (`PhaseStartTime` â†’ `PhaseStartTimeMs`)
6. Write unit tests for image ID resolution
7. Write integration tests for timestamp precision

**Phase 4: Pup Changes (Features 2 & 3)**

1. Add `HealthCheckConfig` to config structs
2. Implement `HealthChecker` with periodic execution
3. Update `/healthz` endpoint to use `HealthChecker`
4. Update artefact creation to use `CreatedAtMs`
5. Write unit tests for health checker (skip logic, timeout, env)
6. Write integration tests for `/healthz` endpoint

**Phase 5: Watch Formatting (Features 1 & 2)**

1. Update `defaultFormatter` timestamp format
2. Implement `formatClaimGranted` with image ID display
3. Implement `truncateImageID` helper
4. Write unit tests for formatting functions

**Phase 6: Configuration Schema**

1. Update `holt.yml` schema documentation
2. Add example `health_check` blocks to docs
3. Add validation for `health_check` fields

**Phase 7: Demo Updates**

1. Add `health_check` to one agent in recipe-generator demo
2. Add `health_check` to one agent in terraform-generator demo
3. Update E2E tests to assert millisecond timestamps in watch output
4. Update E2E tests to assert image IDs in watch output

**Phase 8: Documentation & Testing**

1. Write upgrade guide (M3.8 â†’ M3.9)
2. Update system specification with new schemas
3. Run full test suite
4. Performance testing (health check overhead)

### **3.3. Performance & resource considerations**

**Resource usage:**

**Feature 1: Image ID Resolution**
- **One-time cost** at `holt up`: Docker inspect for each agent (~10-50ms per agent)
- **Per-grant cost** for workers: Image resolve (~10-50ms)
- **Storage**: ~100 bytes per agent in `agent_images` hash
- **Network**: Minimal (local Docker socket)

**Feature 2: Millisecond Timestamps**
- **Storage overhead**: +4 bytes per timestamp (int64 vs int32)
- **Compute overhead**: Negligible (same time.Now() call)
- **Display overhead**: String formatting ~5% slower (3 extra digits)

**Feature 3: Health Checks**
- **CPU**: Depends on health check command (e.g., `exit 0` is negligible, `curl` is ~1-5ms)
- **Memory**: ~1KB per HealthChecker goroutine
- **I/O**: Depends on health check command
- **Default interval (30s)**: Minimal overhead for typical agents

**Scalability limits:**
- Image ID resolution scales linearly with agent count (O(n))
- Health checks run independently per agent (no cross-agent coordination)
- 100 agents Ã— 30s interval = ~3.3 health checks/second (negligible load)

**Performance requirements:**
- Image ID resolution: <100ms per agent (acceptable during startup)
- Health check execution: <50% of interval (e.g., 5s for 30s interval)
- Timestamp formatting: No user-perceivable impact (<1ms)

### **3.4. Testing strategy**

**Unit tests:**

**Feature 1: Agent Version Auditing**
```go
func TestPopulateAgentImages(t *testing.T) {
    // Test successful image ID population
    // Test docker inspect failure
    // Test worker vs traditional agent handling
}

func TestResolveWorkerImageID(t *testing.T) {
    // Test RepoDigests extraction
    // Test fallback to Image ID
    // Test image not found error
}

func TestTruncateImageID(t *testing.T) {
    // Test sha256:... format
    // Test short IDs
    // Test non-standard formats
}
```

**Feature 2: High-Precision Timestamps**
```go
func TestMillisecondPersistence(t *testing.T) {
    // Test UnixMilli() storage
    // Test UnixMilli() retrieval
    // Test timestamp ordering with millisecond precision
}

func TestTimestampFormatting(t *testing.T) {
    // Test "15:04:05.000" format
    // Test millisecond display
}
```

**Feature 3: Configurable Health Checks**
```go
func TestHealthChecker(t *testing.T) {
    // Test successful health check (exit 0)
    // Test failed health check (exit 1)
    // Test timeout handling
    // Test skip logic (concurrent check)
    // Test default values
}

func TestHealthzEndpoint(t *testing.T) {
    // Test 200 OK on healthy
    // Test 503 on unhealthy
    // Test backward compat (no health_check defined)
}
```

**Integration tests:**

**Feature 1: E2E Image Auditing**
```bash
#!/bin/bash
# test-image-auditing.sh

holt up
holt forage --goal "test"

# Wait for claim grant
sleep 5

# Assert agent_images hash exists
redis-cli HGETALL "holt:default-1:agent_images" | grep -q "sha256"

# Assert claim has granted_agent_image_id
redis-cli HGET "holt:default-1:claim:..." granted_agent_image_id | grep -q "sha256"

# Assert watch output shows image ID
holt watch --tail 10 | grep -q "@sha256"
```

**Feature 2: E2E Timestamp Precision**
```bash
#!/bin/bash
# test-timestamp-precision.sh

holt up
holt forage --goal "test"

# Assert watch shows milliseconds
holt watch --tail 5 | grep -E "[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]{3}"

# Assert artefact has created_at_ms
redis-cli HGET "holt:default-1:artefact:..." created_at_ms | grep -q "[0-9]\{13\}"
```

**Feature 3: E2E Custom Health Checks**
```bash
#!/bin/bash
# test-custom-health-check.sh

# Create agent with custom health check
cat > holt.yml <<EOF
agents:
  TestAgent:
    image: "test-agent:latest"
    health_check:
      command: ["test", "-f", "/workspace/healthy.txt"]
      interval: "5s"
      timeout: "2s"
EOF

holt up

# Create healthy marker file
touch /workspace/healthy.txt

# Assert /healthz returns 200
curl -f http://localhost:8080/healthz

# Remove healthy marker
rm /workspace/healthy.txt

# Wait for health check interval
sleep 6

# Assert /healthz returns 503
! curl -f http://localhost:8080/healthz
```

**Performance tests:**

```go
func BenchmarkImageIDResolution(b *testing.B) {
    // Benchmark docker inspect performance
}

func BenchmarkHealthCheckExecution(b *testing.B) {
    // Benchmark health check overhead
    // Test with various commands (exit 0, curl, test -f)
}

func BenchmarkTimestampFormatting(b *testing.B) {
    // Benchmark millisecond timestamp display
}
```

**Demo integration tests:**

```bash
# Updated recipe-generator test
./demos/recipe-generator/test_workflow.sh

# Assert millisecond timestamps in output
# Assert image IDs in watch output

# Updated terraform-generator test
./demos/terraform-generator/test-workflow.sh

# Assert millisecond timestamps in output
# Assert image IDs in watch output
```

## **4. Principle compliance check**

### **4.1. YAGNI (You Ain't Gonna Need It)**

**No speculative features added.**

These three features address concrete, real-world requirements:
- **Image auditing**: Required for security compliance, reproducibility, and incident response
- **Millisecond timestamps**: Required for accurate audit trails in high-frequency workflows
- **Custom health checks**: Required for agent-specific readiness validation (database connections, config file presence, etc.)

**What we're NOT building:**
- Image change detection (out of scope - environment assumed stable)
- Backward compatibility (intentionally breaking for clean governance model)
- Centralized health monitoring (agents expose `/healthz`, orchestrator consumes it - separation of concerns)

**New dependencies:**
- Docker client in CLI (already used for container management)
- Atomic types for health checker (standard library)
- **Zero** external dependencies added

### **4.2. Auditability**

**This milestone dramatically enhances auditability.**

**Before M3.9:**
- Claim grants show agent role, but not which version executed
- Timestamps have 1-second precision (insufficient for fast workflows)
- Health status is binary (Redis ping only)

**After M3.9:**
- **Complete provenance**: Every claim â†’ exact container image version
- **Precise timeline**: Millisecond-precision timestamps enable accurate event ordering
- **Operational transparency**: Custom health checks expose agent-specific readiness

**Audit trail example (post-M3.9):**
```
2025-10-28 14:32:18.234 ðŸ† Claim granted: agent=Writer@f2a3b4c5, claim=abc123, type=exclusive
2025-10-28 14:32:18.456 ðŸ“ Artefact created: id=def456, type=RecipeYAML, by=Writer@f2a3b4c5
2025-10-28 14:32:18.789 ðŸ” Review started: agent=Validator@9e8d7c6b
```

**Compliance value:**
- **SOC 2**: Complete audit trail with precise timestamps
- **HIPAA**: Immutable record of which software version processed data
- **PCI-DSS**: Verifiable agent versions for security assessments

### **4.3. Small, single-purpose components**

**Each feature is independently valuable:**

| Feature | Purpose | Component Scope |
|---------|---------|----------------|
| Agent Version Auditing | Link claims to exact image versions | CLI + Orchestrator + Blackboard |
| High-Precision Timestamps | Accurate event sequencing | All components (cross-cutting) |
| Configurable Health Checks | Agent-specific readiness | Pup only |

**No tight coupling:**
- Features can be tested independently
- Image auditing doesn't depend on health checks
- Timestamp precision applies uniformly across features
- Health checks are backward compatible (optional)

**Separation of concerns:**
- CLI: Populates agent images (knows about containers)
- Orchestrator: Uses agent images (doesn't know about Docker)
- Pup: Executes health checks (doesn't know about orchestrator)

### **4.4. Security considerations**

**Security implications and mitigations:**

**Feature 1: Agent Version Auditing**

**Risk**: Malicious actor replaces agent image but audit trail shows old version
**Mitigation**: Image IDs are resolved from running containers (not user input). Docker's content-addressable storage ensures integrity.

**Risk**: Docker socket access required for image inspection
**Mitigation**:
- CLI already requires Docker socket for container management
- Use read-only Docker operations (inspect, not modify)
- Document required Docker permissions in deployment guide

**Feature 2: High-Precision Timestamps**

**Risk**: Timestamp precision leak enables timing attacks
**Mitigation**: Timestamps are already publicly visible in watch output. Millisecond precision doesn't introduce new attack surface.

**Risk**: Clock skew between components
**Mitigation**: All timestamps use same source (host system clock). Document NTP requirement for distributed deployments.

**Feature 3: Configurable Health Checks**

**Risk**: Malicious health check command (e.g., `rm -rf /workspace`)
**Mitigation**:
- Health check runs as same non-root user (UID 1000)
- Health check defined in holt.yml (configuration-as-code, reviewed)
- Documented best practice: Health checks should be read-only

**Risk**: Health check command execution timing leak
**Mitigation**: Health check results are logged, not hidden. Timing is operational data, not sensitive.

**Risk**: Health check command accesses secrets in environment
**Mitigation**: This is by design - health check validates agent can operate in its environment. If secrets are in env vars, agent work also has access.

**No new attack surfaces** introduced - features leverage existing security model.

### **4.5. Backward compatibility**

**INTENTIONAL BREAKING CHANGES**

This milestone introduces breaking changes to enable clean governance model:

**Breaking Change 1: Timestamp Precision**
- **Old format**: `phase_start_time` (Unix seconds, int64)
- **New format**: `phase_start_time_ms` (Unix milliseconds, int64)
- **Impact**: Old instances cannot parse new claims, new instances cannot parse old claims
- **Migration**: Users must run `holt down` before upgrading

**Breaking Change 2: Artefact Schema**
- **New field**: `created_at_ms` (required)
- **Impact**: Old artefacts missing this field may cause issues in new code
- **Migration**: None - old artefacts remain in Redis but new code expects new field

**Backward Compatible Changes:**

**Health Checks (Feature 3)**:
- **Fully backward compatible**
- If `health_check` not defined in holt.yml, pup uses legacy Redis PING
- Existing agents continue working without modification

**Deprecation:** None - this is a hard break, not gradual deprecation.

**Upgrade Path:**
1. Finish all in-flight workflows
2. Run `holt down` to stop current instance
3. Upgrade Holt binaries (CLI, orchestrator, pup images)
4. Clear Redis (optional but recommended): `redis-cli FLUSHDB`
5. Run `holt up` to start new M3.9 instance

**Documented in:** `docs/upgrade-guides/M3.8-to-M3.9.md` (to be created)

### **4.6. Dependency impact**

**System dependencies:**

**No changes to:**
- Redis: Uses existing data structures (HASH, field additions)
- Docker: Uses existing API (inspect, no new operations)
- Git: Not affected
- Go: Standard library only (atomic, time, exec)

**New Docker API usage:**
```go
// CLI: docker inspect (already used for health checks)
containerInfo, err := dockerClient.ContainerInspect(ctx, containerName)
imageInfo, _, err := dockerClient.ImageInspectWithRaw(ctx, imageID)

// Orchestrator: docker inspect for workers
imageInfo, _, err := dockerClient.ImageInspectWithRaw(ctx, workerImage)
```

**No new external dependencies.**

**Build impact:**
- No Makefile changes
- No new build flags
- No new compiler requirements

**Development environment impact:**
- Developers must have Docker API v1.41+ (Docker 20.10+)
- Same requirement as existing Holt development

**CI/CD impact:**
- Existing Docker-in-Docker setup sufficient
- No new test infrastructure needed

## **5. Definition of done**

*This checklist must be fully satisfied for the milestone to be considered complete.*

### **Feature 1: Agent Version Auditing**
- [ ] CLI (`holt up`) populates `holt:{instance}:agent_images` Redis hash
- [ ] CLI (`holt up`) fails gracefully if docker inspect fails
- [ ] Orchestrator reads `agent_images` hash when granting claims to traditional agents
- [ ] Orchestrator resolves worker image IDs at launch time
- [ ] Claim schema includes `granted_agent_image_id` field
- [ ] `claim_granted` workflow event includes `agent_image_id` field
- [ ] `holt watch` displays agent image IDs as `agent=Role@sha256:abc123...`
- [ ] Unit tests for image ID resolution (traditional and worker agents)
- [ ] Integration test verifying end-to-end image ID auditing

### **Feature 2: High-Precision Timestamps**
- [ ] All Redis timestamp writes use `.UnixMilli()` instead of `.Unix()`
- [ ] Artefact struct has `created_at_ms` field
- [ ] Claim struct has `phase_start_time_ms` field (renamed from `phase_start_time`)
- [ ] `holt watch` displays timestamps in "HH:MM:SS.mmm" format
- [ ] Unit tests for millisecond persistence and retrieval
- [ ] Integration test verifying millisecond precision in watch output

### **Feature 3: Configurable Health Checks**
- [ ] `holt.yml` schema supports optional `health_check` block
- [ ] `health_check` block has `command`, `interval`, `timeout` fields
- [ ] Default values applied: `interval="30s"`, `timeout="5s"`
- [ ] Pup executes health checks in agent's environment (workspace, env, user)
- [ ] Concurrent health checks are skipped
- [ ] Failed health checks are logged as warnings
- [ ] `/healthz` endpoint returns 200 or 503 based on last check result
- [ ] Backward compatibility: `/healthz` uses Redis PING if `health_check` not defined
- [ ] Unit tests for HealthChecker (success, failure, timeout, skip)
- [ ] Integration test for custom health check execution

### **Cross-Feature Requirements**
- [ ] Recipe-generator demo updated with `health_check` example
- [ ] Terraform-generator demo updated with `health_check` example
- [ ] Demo E2E tests assert millisecond timestamps in watch output
- [ ] Demo E2E tests assert image IDs in watch output
- [ ] Upgrade guide created: `docs/upgrade-guides/M3.8-to-M3.9.md`
- [ ] System specification updated with new schemas
- [ ] Performance benchmarks run (image resolution, health checks, timestamps)
- [ ] All existing tests still pass (backward compat for health checks verified)

### **Documentation & Polish**
- [ ] All new config fields documented with examples
- [ ] Breaking changes clearly documented in upgrade guide
- [ ] Security considerations documented (Docker permissions, health check best practices)
- [ ] Architecture diagrams updated (if applicable)

## **6. Error scenarios & edge cases**

### **6.1. Failure modes**

**Failure Mode 1: Docker Inspect Fails During `holt up`**
- **Trigger**: Docker daemon unreachable, permission denied, container not found
- **Component behavior**: CLI aborts `holt up` before starting agents
- **User impact**: Instance fails to start, clear error message
- **Error message**: `"Failed to resolve image ID for agent 'Writer': Get http://docker/containers/holt-default-1-Writer/json: dial unix /var/run/docker.sock: connect: permission denied. Cannot start instance without complete audit trail."`
- **Mitigation**: User must fix Docker access before retrying

**Failure Mode 2: Worker Image Resolution Fails**
- **Trigger**: Image tag doesn't exist, registry unreachable
- **Component behavior**: Orchestrator logs error, creates Failure artefact
- **User impact**: Claim terminates, workflow halts
- **Error message**: `"Failed to resolve worker image 'my-agent:v2.0': image not found"`
- **Mitigation**: User must fix image tag in holt.yml or pull image manually

**Failure Mode 3: Health Check Timeout**
- **Trigger**: Health check command runs longer than configured timeout
- **Component behavior**: Pup kills health check process, marks as unhealthy
- **User impact**: Agent reports 503 Unhealthy until next successful check
- **Logged message**: `"[WARN] Health check timed out after 5s for agent Writer"`
- **Mitigation**: User increases timeout or optimizes health check command

**Failure Mode 4: Health Check Command Not Found**
- **Trigger**: Health check command doesn't exist in container
- **Component behavior**: Pup logs error, marks as unhealthy
- **User impact**: Agent reports 503 Unhealthy permanently
- **Logged message**: `"[ERROR] Health check failed: exec: 'nonexistent': executable file not found in $PATH"`
- **Mitigation**: User fixes health check command in holt.yml

**Failure Mode 5: Millisecond Timestamp Overflow**
- **Trigger**: Year 2262 (Unix milliseconds exceed int64 max)
- **Component behavior**: Timestamp wraps to negative value
- **User impact**: Display shows incorrect time
- **Mitigation**: Not applicable (Holt unlikely to run in year 2262). Document as known limitation.

**Failure Mode 6: Concurrent Health Check Accumulation**
- **Trigger**: Health check interval < timeout, checks pile up
- **Component behavior**: Pup skips new checks, logs warnings
- **User impact**: Agent may appear unhealthy longer than expected
- **Logged message**: `"[WARN] Skipping health check - previous check still running"`
- **Mitigation**: User configures timeout < interval (documented guideline)

### **6.2. Concurrency considerations**

**Image ID Hash Access**
- **Scenario**: Multiple orchestrator instances read `agent_images` hash
- **Safety**: Redis HGET is atomic, read-only access, no conflicts
- **No locking required**

**Worker Image Resolution**
- **Scenario**: Multiple workers launched simultaneously for same agent type
- **Safety**: Each worker resolves its own image ID independently
- **No locking required** - image resolution is idempotent

**Health Check Execution**
- **Scenario**: Health check interval triggers while previous check running
- **Safety**: `atomic.Bool` prevents concurrent execution
- **Lock-free design** using atomic CAS (Compare-And-Swap)

**Timestamp Creation**
- **Scenario**: Multiple goroutines create timestamps simultaneously
- **Safety**: `time.Now().UnixMilli()` is thread-safe
- **No locking required**

**No race conditions** introduced by these features.

### **6.3. Edge case handling**

**Edge Case 1: Image ID Format Variations**
- **Scenario**: Image ID is short ID, not full digest
- **Handling**: Store whatever Docker returns (ID or RepoDigests[0])
- **Display**: Truncate to 12 chars regardless of format

**Edge Case 2: Health Check Exits Before First Interval**
- **Scenario**: Health check never runs (pup crashes immediately)
- **Handling**: `lastResult.Load()` returns nil, default to healthy
- **Rationale**: Fail-open for agent startup (pessimistic would prevent all agents from starting)

**Edge Case 3: Health Check Command Has Exit Code 2+**
- **Scenario**: Command exits with 2, 127, 255, etc.
- **Handling**: Any non-zero exit code = unhealthy
- **Display**: Log actual exit code for debugging

**Edge Case 4: Zero-Length Image Digest**
- **Scenario**: Docker returns empty RepoDigests array and empty ID
- **Handling**: `holt up` fails with error "empty image ID for agent X"
- **Mitigation**: User must fix Docker installation

**Edge Case 5: Health Check Command Modifies Workspace**
- **Scenario**: Health check runs `echo "test" > /workspace/health.log`
- **Handling**: Command succeeds, file created
- **Mitigation**: Documented best practice - health checks should be read-only

**Edge Case 6: Millisecond Precision on Low-Resolution Clocks**
- **Scenario**: System clock has 10ms resolution (some VMs)
- **Handling**: Multiple timestamps may have identical values
- **Impact**: Events within same 10ms window have same timestamp (acceptable)

## **7. Open questions & decisions**

All questions have been resolved through user clarifications. No open questions remain.

**Resolved Questions:**

1. âœ… **Controller-Worker Image IDs**: Workers' image IDs recorded (not controllers')
2. âœ… **Image Change Detection**: Out of scope
3. âœ… **Docker Inspect Failures**: `holt up` fails completely
4. âœ… **Timestamp Migration**: No backward compatibility, clean break
5. âœ… **Artefact Timestamps**: Yes, add `created_at_ms`
6. âœ… **Health Check Environment**: Same as agent work (workspace, env, user)
7. âœ… **Health Check Defaults**: interval=30s, timeout=5s
8. âœ… **Health Check Failures**: Log warnings, return 503
9. âœ… **Concurrent Health Checks**: Skip new check
10. âœ… **Implementation Order**: Single atomic milestone (M3.9)
11. âœ… **Demo Updates**: Yes, both demos updated with health checks and E2E assertions

## **8. AI agent implementation guidance**

### **8.1. Development approach**

**For AI agents implementing this feature:**

1. **Start with Schema Changes**: Update `pkg/blackboard/types.go` first. All components depend on this.

2. **Implement Features in Order**:
   - **First**: Feature 2 (timestamps) - simple, cross-cutting
   - **Second**: Feature 1 (image auditing) - CLI then orchestrator
   - **Third**: Feature 3 (health checks) - pup only, most complex

3. **Test as You Go**: Don't wait until end to test. Each feature should have passing unit tests before moving to next.

4. **Use Type System**: Go's type system will catch timestamp field renames (`PhaseStartTime` â†’ `PhaseStartTimeMs`). Use compiler errors as a checklist.

5. **Docker Client Best Practices**:
   ```go
   // Good: Always check errors
   imageInfo, _, err := dockerClient.ImageInspectWithRaw(ctx, imageID)
   if err != nil {
       return fmt.Errorf("failed to inspect image: %w", err)
   }

   // Good: Prefer RepoDigests, fallback to ID
   if len(imageInfo.RepoDigests) > 0 {
       return imageInfo.RepoDigests[0], nil
   }
   return imageInfo.ID, nil

   // Bad: Assume RepoDigests always exists
   return imageInfo.RepoDigests[0], nil  // Panic if empty!
   ```

6. **Health Check Testing**:
   ```go
   // Good: Test with real command execution
   hc := NewHealthChecker(&HealthCheckConfig{
       Command: []string{"sh", "-c", "exit 0"},
       Interval: "1s",
       Timeout: "500ms",
   })
   hc.Start()
   time.Sleep(1200 * time.Millisecond)
   assert.True(t, hc.IsHealthy())

   // Good: Test timeout with slow command
   hc := NewHealthChecker(&HealthCheckConfig{
       Command: []string{"sh", "-c", "sleep 10"},
       Timeout: "100ms",
   })
   hc.runCheck()
   assert.False(t, hc.IsHealthy())
   ```

### **8.2. Common pitfalls to avoid**

**Pitfall 1: Using Wrong Time Function**
```go
// Bad: Still using Unix() (seconds)
timestamp := time.Now().Unix()

// Good: Use UnixMilli() (milliseconds)
timestamp := time.Now().UnixMilli()
```

**Pitfall 2: Not Handling Empty RepoDigests**
```go
// Bad: Index without checking length
imageID := imageInfo.RepoDigests[0]  // Panic if empty!

// Good: Check length first
if len(imageInfo.RepoDigests) > 0 {
    imageID = imageInfo.RepoDigests[0]
}
```

**Pitfall 3: Blocking Health Check Execution**
```go
// Bad: Health check blocks main goroutine
func (p *Pup) Start() {
    p.healthChecker.Start()  // Blocks forever!
    p.startWorkExecutor()    // Never reached
}

// Good: Health check runs in background
func (p *Pup) Start() {
    go p.healthChecker.Start()  // Background goroutine
    p.startWorkExecutor()
}
```

**Pitfall 4: Forgetting to Set Working Directory**
```go
// Bad: Health check runs in arbitrary directory
cmd := exec.Command(hc.config.Command[0], hc.config.Command[1:]...)

// Good: Health check runs in workspace
cmd := exec.Command(hc.config.Command[0], hc.config.Command[1:]...)
cmd.Dir = "/workspace"
cmd.Env = os.Environ()
```

**Pitfall 5: Not Applying Default Values**
```go
// Bad: Assumes interval and timeout always set
interval, _ := time.ParseDuration(config.Interval)

// Good: Apply defaults
interval := "30s"
if config.Interval != "" {
    interval = config.Interval
}
intervalDuration, _ := time.ParseDuration(interval)
```

**Pitfall 6: Comparing Old and New Timestamp Fields**
```go
// Bad: Mixing old and new field names
if claim.PhaseStartTime > otherTime {  // Wrong field!
    // ...
}

// Good: Use new field name consistently
if claim.PhaseStartTimeMs > otherTimeMs {
    // ...
}
```

### **8.3. Integration checklist**

**Pre-implementation verification:**
- [ ] All prerequisite features complete (Phase 3 M3.1-M3.8)
- [ ] Docker client available in CLI (already used for container management)
- [ ] Redis client available in all components
- [ ] Standard library `atomic`, `time`, `exec` packages available

**During implementation:**
- [ ] Schema changes backward-incompatible (intentional)
- [ ] All timestamp writes use `.UnixMilli()`
- [ ] All timestamp displays use "15:04:05.000" format
- [ ] CLI populates `agent_images` hash on success only (fail-fast on error)
- [ ] Orchestrator reads `agent_images` for traditional agents
- [ ] Orchestrator resolves worker images dynamically
- [ ] Health checker uses atomic operations (no mutexes)
- [ ] Health checks run in agent's environment
- [ ] Default values applied for `interval` and `timeout`

**Post-implementation:**
- [ ] Unit tests pass for all three features
- [ ] Integration tests pass for all three features
- [ ] Demo E2E tests updated and passing
- [ ] Watch output shows millisecond timestamps
- [ ] Watch output shows image IDs
- [ ] Health checks work with custom commands
- [ ] Upgrade guide written and reviewed

## **9. Operational readiness**

### **9.1. Monitoring and observability**

**New Observability Features:**

**1. Agent Image Auditing (Feature 1)**

**What to monitor:**
- Image ID resolution failures during `holt up`
- Worker image resolution failures during claim grants
- Image ID presence in all granted claims

**How to monitor:**
```bash
# Check agent_images hash populated
redis-cli HGETALL "holt:default-1:agent_images"

# Verify all claims have image IDs
redis-cli SCAN 0 MATCH "holt:default-1:claim:*" | \
  xargs -I {} redis-cli HGET {} granted_agent_image_id | \
  grep -c "sha256"

# Watch for image ID in real-time
holt watch | grep "@sha256"
```

**Alerts to configure:**
- `holt up` failures with "Failed to resolve image ID"
- Claims missing `granted_agent_image_id` field (data integrity issue)

**2. High-Precision Timestamps (Feature 2)**

**What to monitor:**
- Timestamp precision in audit logs
- Event ordering accuracy (millisecond granularity)
- Clock skew between components (if distributed)

**How to monitor:**
```bash
# Verify millisecond precision in watch output
holt watch | grep -E "[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]{3}"

# Check artefact timestamps
redis-cli HGET "holt:default-1:artefact:abc123" created_at_ms
```

**Alerts to configure:**
- Clock skew >100ms between orchestrator and agents (NTP issue)
- Timestamp parsing errors (format mismatch)

**3. Configurable Health Checks (Feature 3)**

**What to monitor:**
- Health check success/failure rates
- Health check execution duration
- Consecutive failure counts

**How to monitor:**
```bash
# Check agent health status
curl http://localhost:8080/healthz

# View health check logs
docker logs holt-default-1-Writer | grep "Health check"

# Monitor for consecutive failures
docker logs holt-default-1-Writer | \
  grep "Health check has failed" | \
  tail -20
```

**Alerts to configure:**
- 3+ consecutive health check failures for any agent
- Health check execution time >50% of interval
- `/healthz` returning 503 for >5 minutes

**Logging Enhancements:**

```go
// Feature 1: Log image registration
log.Infof("Registered agent %s with image %s", role, imageID)

// Feature 3: Log health check failures
log.Warnf("Health check failed for agent %s: %v", role, err)
log.Warnf("Health check has failed %d consecutive times for agent %s", count, role)
```

### **9.2. Rollback and disaster recovery**

**Rollback Scenarios:**

**Scenario 1: M3.9 Instance Fails to Start**
- **Symptom**: `holt up` fails with "Failed to resolve image ID"
- **Cause**: Docker permissions, missing images, daemon unreachable
- **Recovery**:
  1. Fix Docker issue (permissions, pull images, restart daemon)
  2. Retry `holt up`
- **Data loss**: None (instance never started)

**Scenario 2: Need to Downgrade to M3.8**
- **Symptom**: M3.9 has critical bug, must revert
- **Cause**: Regression in new code
- **Recovery**:
  1. `holt down` (stop M3.9 instance)
  2. Clear Redis: `redis-cli FLUSHDB` (required - incompatible schemas)
  3. Downgrade Holt binaries to M3.8
  4. Rebuild agent images with M3.8 pup
  5. `holt up` with M3.8
- **Data loss**: All in-flight workflows lost (acceptable - documented)

**Scenario 3: Health Check Command Causes Agent Crash**
- **Symptom**: Agent repeatedly restarts, 503 Unhealthy
- **Cause**: Health check command has side effect (writes to critical file, OOM)
- **Recovery**:
  1. `holt down`
  2. Fix `health_check.command` in holt.yml (or remove block)
  3. `holt up`
- **Data loss**: Workflow progress since last successful artefact

**Disaster Recovery:**

**Redis Data Loss:**
- **Impact**: All agent images, claims, artefacts lost
- **Recovery**: Restart from `holt up` (re-populates `agent_images`)
- **Workflow impact**: All in-flight workflows lost

**Docker Daemon Failure:**
- **Impact**: Cannot resolve image IDs, cannot start agents
- **Recovery**: Restart Docker daemon, retry `holt up`
- **Workflow impact**: None if Redis data intact

**NTP Desync (Distributed Deployment):**
- **Impact**: Timestamps inaccurate, event ordering wrong
- **Recovery**: Fix NTP configuration, restart affected components
- **Workflow impact**: Audit trail timestamps may be skewed

**Recovery Time Objectives (RTO):**
- Docker permission fix: <5 minutes
- Downgrade to M3.8: <15 minutes (includes Redis flush, binary swap)
- Health check fix: <5 minutes (config change only)

### **9.3. Documentation and training**

**Documentation to Create:**

**1. Upgrade Guide: `docs/upgrade-guides/M3.8-to-M3.9.md`**
```markdown
# Upgrading from M3.8 to M3.9

## Breaking Changes

- **Timestamp Precision**: All timestamps now use millisecond precision
- **Schema Changes**: Claims and artefacts have new/renamed fields
- **No In-Place Upgrade**: Must stop instance before upgrading

## Upgrade Steps

1. Finish all in-flight workflows
2. Run `holt down` to stop instance
3. Upgrade Holt CLI: `go install github.com/yourusername/holt/cmd/holt@v3.9.0`
4. Rebuild agent images with new pup: `make docker-pup && make build-demo-agents`
5. Clear Redis (optional): `redis-cli FLUSHDB`
6. Run `holt up` to start M3.9 instance

## Verification

- Watch output shows millisecond timestamps: `HH:MM:SS.mmm`
- Watch output shows image IDs: `agent=Role@sha256:...`
- Health checks work: `curl http://localhost:8080/healthz`

## Rollback

If issues occur, downgrade to M3.8:
1. `holt down`
2. `redis-cli FLUSHDB` (required)
3. Install M3.8 binaries
4. `holt up`
```

**2. Feature Documentation: `docs/features/agent-version-auditing.md`**
- Purpose and compliance value
- How image IDs are resolved
- Watch output format
- Troubleshooting image resolution failures

**3. Feature Documentation: `docs/features/high-precision-timestamps.md`**
- Why millisecond precision matters
- Display format
- NTP requirements for distributed deployments

**4. Feature Documentation: `docs/features/configurable-health-checks.md`**
- Configuration schema
- Best practices (read-only checks, timeout < interval)
- Examples (file existence, database ping, API health)
- Troubleshooting health check failures

**5. Configuration Reference: Update `docs/configuration-reference.md`**
```yaml
agents:
  MyAgent:
    # ... existing fields ...

    # Optional: Custom health check (M3.9+)
    health_check:
      command: ["sh", "-c", "test -f /workspace/ready"]
      interval: "30s"  # Default: 30s
      timeout: "5s"    # Default: 5s
```

**Training Materials:**

**For Operators:**
- How to interpret watch output with image IDs and millisecond timestamps
- How to configure custom health checks
- How to troubleshoot image resolution failures
- How to monitor health check failures

**For Developers:**
- How to write effective health check commands
- Health check best practices (read-only, fast execution)
- How to use image IDs for reproducibility

**For Compliance Teams:**
- How to audit agent versions from claim records
- How to use millisecond timestamps for regulatory reporting
- How to trace workflow execution with image IDs

**No Formal Training Required:**
- Features are transparent (watch output shows everything)
- Configuration is self-documenting (YAML schema)
- Upgrade guide provides step-by-step instructions

## **10. Self-validation checklist**

### **Before starting implementation:**

- [x] I understand how this feature aligns with the current phase (Phase 3 - production hardening)
- [x] All success criteria (section 1.3) are measurable and testable
- [x] I have considered every component in section 2 explicitly (Blackboard, Orchestrator, Pup, CLI)
- [x] All design decisions (section 3.1) are justified and documented
- [x] Breaking changes are intentional and well-justified (governance clean slate)

### **During implementation:**

- [ ] I am implementing the simplest solution that meets success criteria
- [ ] All error scenarios (section 6) are being handled, not just happy path
- [ ] Tests are being written before or alongside code (TDD approach)
- [ ] I am validating that health checks work for both success and failure cases
- [ ] I am verifying timestamp precision in both storage and display

### **Before submission:**

- [ ] All items in Definition of Done (section 5) are complete
- [ ] Feature has been tested in a clean environment from scratch
- [ ] Documentation is updated and accurate (upgrade guide, feature docs)
- [ ] I have considered the operational impact (section 9) of this feature
- [ ] Breaking changes are clearly called out in all documentation
- [ ] Demo updates validate all three features end-to-end

---

**End of Design Document**

This specification provides a complete, implementable design for M3.9 Governance & Reliability Enhancements, transforming Holt into a production-grade, auditable orchestration platform suitable for regulated industries and mission-critical workflows.
